{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_feature = pickle.load(open(\"pickles/location_feature.pkl\",\"rb\"))\n",
    "title_overlap = pickle.load(open(\"pickles/title_overlap.pkl\",\"rb\"))\n",
    "context_count = pickle.load(open(\"pickles/context_count.pkl\",\"rb\"))\n",
    "cue_words = pickle.load(open(\"pickles/cue_words_reduced.pkl\", \"rb\"))\n",
    "tags = pickle.load(open(\"pickles/baseline_tags.pkl\", \"rb\"))\n",
    "contexts = pickle.load(open(\"pickles/reduced_context.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_dist = pickle.load(open(\"pickles/contexts_dist.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paper_name': 'joint morphological and syntactic analysis for richly inflected languages.', 'context': ['ut', 'half', 'size', 'use', 'unlabel', 'sentenc', 'chemic', 'domain', 'share', 'ratio', 'train', 'set', 'size', 'unlabel', 'data', 'set', 'size', 'compar', 'web', 'domain', 'data', 'set', 'keep', 'scale', 'train', 'unlabel', 'set', 'allow', 'easili', 'adapt', 'best', 'set', 'web', 'domain', 'experi', '<number>', 'depend', 'parser', 'use', 'mate', 'transitionbas', 'depend', 'parser', 'default', 'set', 'experi', 'cf', '<this_citation>', 'tag', 'use', 'predict', 'po', 'tag', 'carri', 'experi', 'believ', 'realist', 'scenario', 'parser', 'intern', 'tagger', 'use', 'suppli', 'po', 'tag', 'unlabel', 'set', 'test', 'dataset', 'order', 'compar', 'previou', 'work', 'evalu', 'approach', 'addit', 'gold', 'po', 'tag', 'text', 'chemic', 'domain', 'gold', 'tag', 'use', 'previou', 'work', 'baselin', 'gener', 'train', 'p']}\n"
     ]
    }
   ],
   "source": [
    "print(contexts['W15-2201'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_context = pickle.load(open(\"pickles/fixed_context.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389\n"
     ]
    }
   ],
   "source": [
    "print(len(context_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_table = pickle.load(open(\"pickles/num_table.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "cit_titles = pickle.load(open(\"pickles/citation_titles.pkl\",\"rb\"))\n",
    "ids = tags.keys()\n",
    "ids = list(set(ids).intersection(set(num_table.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389\n"
     ]
    }
   ],
   "source": [
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K15-1005', 'P11-1062', 'N13-1123', 'N03-1026', 'W12-4205', 'D10-1083', 'W15-4911', 'D14-1032', 'P00-1039', 'P03-1032', 'W06-1641', 'D13-1179', 'P12-1069', 'P96-1006', 'D10-1004', 'D15-1004', 'W15-2138', 'E12-1051', 'D12-1037', 'D12-1042', 'D10-1079', 'W13-3512', 'S15-1002', 'N10-1128', 'P08-1106', 'P10-1103', 'E09-1057', 'P98-2159', 'E09-1005', 'P13-1078', 'P13-1107', 'P12-1060', 'P14-1070', 'N06-1006', 'N07-1026', 'D11-1138', 'P11-1054', 'N10-1085', 'J00-4001', 'D09-1159', 'D12-1069', 'D12-1085', 'P13-1042', 'S14-1003', 'D13-1134', 'N09-1050', 'J08-1003', 'W11-0319', 'E93-1007', 'J08-1001', 'S14-1002', 'J05-1003', 'D12-1064', 'D10-1075', 'D15-1015', 'W06-1606', 'Q15-1011', 'D12-1056', 'D15-1102', 'D15-1038', 'P14-1006', 'P14-1087', 'E12-1006', 'P08-1088', 'D09-1037', 'P06-1085', 'D13-1033', 'D12-1120', 'E09-1088', 'D11-1083', 'D10-1081', 'E14-1073', 'N12-1031', 'P10-1020', 'D13-1202', 'D14-1133', 'P15-1043', 'P10-1138', 'D08-1023', 'P04-1048', 'P15-1071', 'P10-1009', 'D13-1016', 'P14-1033', 'D12-1032', 'D14-1175', 'D15-1084', 'W15-4627', 'E12-1072', 'P11-1061', 'D14-1090', 'Q14-1005', 'P89-1032', 'E12-1073', 'D15-1082', 'P13-1109', 'D10-1117', 'P14-1032', 'N10-1081', 'J04-1004', 'P11-1109', 'N06-1015', 'D08-1099', 'E99-1017', 'W03-0416', 'W11-0124', 'N07-1017', 'P99-1048', 'P12-1052', 'W97-0312', 'P08-1064', 'K15-1027', 'Q14-1030', 'W15-4903', 'P13-1123', 'P10-1004', 'W11-0301', 'P05-1077', 'D15-1039', 'W13-0119', 'P13-1144', 'P06-1095', 'P09-1016', 'D08-1031', 'W15-1824', 'P09-1008', 'P13-1087', 'D12-1012', 'W04-2402', 'J05-4005', 'D14-1080', 'N07-1072', 'J80-2003', 'P12-1046', 'W06-2914', 'W13-3504', 'P09-1022', 'P03-1012', 'P11-1087', 'P15-1079', 'W04-3248', 'D12-1122', 'W06-1649', 'J08-2004', 'P08-1010', 'D11-1058', 'J01-2004', 'W10-2926', 'D09-1105', 'P12-1072', 'N12-1052', 'P04-1028', 'P15-1053', 'P10-1005', 'P02-1059', 'D11-1129', 'D15-1081', 'N13-1060', 'N13-1071', 'W97-0301', 'P12-1107', 'D14-1008', 'N04-1023', 'N03-1025', 'P07-1003', 'P05-1020', 'P10-1080', 'P10-1105', 'P03-1064', 'N03-1027', 'W03-1012', 'P09-1107', 'P04-1008', 'P12-1057', 'N13-1119', 'P09-1036', 'P13-1013', 'D14-1119', 'D15-1017', 'D14-1150', 'P06-1102', 'K15-1024', 'E14-1003', 'W13-2516', 'W03-0417', 'P08-1093', 'P09-1001', 'P04-1020', 'P14-1114', 'D09-1122', 'D10-1125', 'P08-1099', 'P08-1048', 'J14-4004', 'P96-1010', 'P09-1068', 'E14-1066', 'P15-1154', 'D14-1044', 'D14-1003', 'D15-1221', 'W02-1022', 'D12-1133', 'P14-1107', 'D13-1001', 'D11-1131', 'P08-1040', 'D14-1017', 'P90-1036', 'D14-1204', 'D15-1012', 'W04-3230', 'W03-1015', 'P15-1147', 'P14-1065', 'P15-1172', 'D09-1010', 'D12-1026', 'N13-1011', 'P13-1104', 'D10-1021', 'D15-1238', 'N12-1023', 'P13-1122', 'P10-1140', 'N13-1106', 'D12-1028', 'E12-1025', 'A00-2038', 'W09-1106', 'N12-1093', 'P13-1118', 'P13-1150', 'E14-1001', 'P13-1113', 'P15-1167', 'N09-1064', 'P03-1036', 'D15-1035', 'Q15-1005', 'W06-1603', 'P15-1138', 'D11-1064', 'J88-1003', 'E97-1041', 'P13-1111', 'P05-1035', 'D15-1050', 'D12-1134', 'P11-1065', 'D11-1141', 'P98-1090', 'N09-1009', 'N09-1010', 'E14-1021', 'P14-1090', 'P00-1019', 'N09-1036', 'W04-3206', 'K15-1026', 'P14-1066', 'D11-1051', 'P96-1043', 'W10-2915', 'D14-1093', 'D12-1125', 'P12-1014', 'W14-1609', 'N06-1057', 'D15-1005', 'N10-1021', 'P15-1135', 'W15-5618', 'N13-1111', 'P06-1043', 'W04-2403', 'P94-1025', 'E03-1053', 'D15-1286', 'Q15-1010', 'D14-1002', 'S15-1003', 'D15-1154', 'K15-1021', 'E09-1046', 'W11-4501', 'W12-3101', 'P11-1116', 'D14-1028', 'P92-1016', 'Q14-1034', 'P08-1097', 'D13-1025', 'N13-1137', 'N10-1014', 'D10-1041', 'W08-2104', 'N07-1011', 'W11-0125', 'D13-1111', 'J14-2003', 'D09-1036', 'D14-1062', 'P06-1004', 'P15-1102', 'P99-1016', 'P05-1010', 'W11-0307', 'D09-1070', 'D11-1026', 'E12-1013', 'N01-1011', 'P01-1041', 'D09-1137', 'N13-1090', 'P14-1061', 'D11-1014', 'P14-1083', 'P06-1009', 'E09-1044', 'W15-2203', 'D14-1216', 'E14-1014', 'W09-1116', 'D14-1085', 'P15-1026', 'K15-1006', 'P13-1007', 'E14-1002', 'P14-1129', 'P00-1006', 'D13-1002', 'N13-1007', 'D15-1164', 'W14-1619', 'P03-1046', 'D13-1159', 'D15-1058', 'D09-1108', 'D12-1022', 'P08-1050', 'D14-1009', 'Q14-1019', 'N10-1129', 'D13-1007', 'Q14-1020', 'P14-1062', 'P11-1021', 'D14-1107', 'K15-1004', 'P10-1107', 'D11-1110', 'D15-1279', 'E12-1024', 'D13-1040', 'D15-1020', 'D08-1087', 'D12-1024', 'P10-1130', 'W15-4608', 'D15-1030', 'D15-1166', 'N12-1096', 'D12-1081', 'E09-1073', 'P02-1062', 'D09-1127', 'D14-1043', 'D10-1123', 'P00-1022', 'N07-1047', 'P15-1130', 'N09-1062', 'W11-0328', 'P13-1097', 'W10-2924', 'N13-1003', 'W10-4216', 'P09-1039', 'D10-1045', 'P15-1083', 'P06-1113', 'P15-1031', 'P15-1108', 'N03-1008', 'P03-1035', 'P13-1059', 'D15-1061', 'D14-1153', 'P14-1012', 'P11-1053', 'P00-1011', 'P09-1109', 'W04-2408', 'E14-1008', 'D14-1219', 'K15-1008', 'E03-1002', 'D14-1024', 'W14-4404', 'P09-1066', 'W05-0610', 'W10-2905', 'E06-1005', 'D08-1034', 'D15-1037', 'P96-1032', 'P14-1085', 'D12-1017', 'N12-1009', 'D15-1136', 'P14-1013', 'D14-1004', 'N09-1073', 'D13-1065', 'D09-1012', 'N09-1045', 'W14-1202', 'N13-1086', 'D12-1095', 'E14-1051', 'D09-1015', 'P13-1089', 'D09-1013', 'D11-1063', 'P13-1160', 'P15-1089', 'P00-1072', 'P14-1020', 'D14-1176', 'D12-1039', 'P04-1015', 'P02-1025', 'P04-1007', 'W03-0402', 'D14-1218', 'D12-1080', 'E14-1042', 'D15-1087', 'K15-1003', 'Q14-1009', 'D11-1030', 'P08-1074', 'D14-1022', 'P11-1063', 'S14-1001', 'D13-1103', 'P94-1022', 'E14-1064', 'E06-1033', 'J98-4002', 'N13-1033', 'P15-1050', 'N07-1022', 'P14-1116', 'N07-1031', 'P14-1015', 'P06-1059', 'E06-1026', 'N03-1032', 'P11-1103', 'E12-1008', 'P11-1090', 'W02-1028', 'N07-1044', 'P15-1074', 'P13-1161', 'P05-1025', 'N06-1052', 'D12-1058', 'E09-1097', 'D11-1122', 'W13-1722', 'D12-1127', 'J11-3002', 'D11-1082', 'P14-1128', 'P00-1063', 'W09-3722', 'P08-1047', 'D15-1028', 'D15-1227', 'Q14-1027', 'D13-1028', 'P15-1136', 'J95-4002', 'D08-1048', 'N12-1004', 'W14-4338', 'D15-1128', 'D11-1080', 'D10-1111', 'N07-1046', 'D13-1062', 'P03-1025', 'P09-1070', 'P96-1041', 'D15-1108', 'W12-1643', 'D15-1162', 'D08-1090', 'N01-1006', 'P98-1051', 'D11-1124', 'W14-4343', 'P01-1010', 'E14-1032', 'S14-1018', 'E09-1054', 'P08-1030', 'P14-1140', 'P15-1005', 'D13-1117', 'W15-2205', 'P15-1110', 'D10-1099', 'D12-1117', 'P10-1046', 'D15-1122', 'P12-1035', 'E99-1023', 'J14-1003', 'D15-1083', 'P12-1090', 'D11-1013', 'D12-1047', 'D12-1016', 'P10-1048', 'E09-1038', 'P11-1042', 'D12-1046', 'W13-4008', 'D13-1083', 'P15-1010', 'E12-1038', 'D12-1073', 'P12-1084', 'N10-1017', 'D13-1191', 'D14-1037', 'P04-1088', 'P12-1081', 'D13-1043', 'P05-1036', 'J10-3009', 'J05-4002', 'D15-1148', 'P97-1013', 'D12-1007', 'D08-1095', 'P12-1063', 'D15-1189', 'P95-1036', 'P13-1022', 'P98-2152', 'D14-1168', 'D11-1120', 'D14-1100', 'P14-1036', 'W01-0517', 'D14-1225', 'P93-1035', 'P10-1159', 'E14-1050', 'P09-1037', 'P06-1047', 'P10-1060', 'D14-1190', 'D12-1023', 'P06-1144', 'D09-1151', 'P10-1040', 'N13-1109', 'Q13-1004', 'P11-1038', 'E09-1049', 'D14-1205', 'N06-1033', 'E85-1006', 'D15-1241', 'P92-1050', 'P15-1113', 'P14-1126', 'P10-1090', 'P11-1124', 'D13-1136', 'Q13-1009', 'P11-1075', 'P02-1003', 'W03-1002', 'D13-1130', 'P02-1054', 'W11-0303', 'P02-1065', 'D14-1039', 'S15-1028', 'W13-4002', 'D13-1173', 'D14-1045', 'K15-1020', 'P01-1065', 'D14-1162', 'J01-4003', 'J00-4003', 'N12-1047', 'N13-1009', 'E97-1055', 'P08-1078', 'Q14-1028', 'N13-1080', 'D09-1099', 'P10-1153', 'P05-1004', 'E12-1007', 'P14-1073', 'W97-0311', 'D12-1063', 'E06-1021', 'E14-1075', 'E03-1026', 'D10-1048', 'D14-1087', 'D15-1068', 'W97-0317', 'P12-1098', 'J03-3005', 'P11-1117', 'D15-1177', 'D14-1196', 'W13-4034', 'P15-1040', 'E09-1087', 'D15-1065', 'P02-1014', 'N04-1033', 'N06-1024', 'D11-1017', 'P11-1152', 'D08-1066', 'P13-1029', 'P05-1053', 'D15-1047', 'A00-1040', 'P15-1065', 'P13-1086', 'D14-1075', 'N12-1018', 'D14-1123', 'D08-1005', 'A92-1018', 'P06-1081', 'J01-2002', 'W11-0311', 'P10-1083', 'E12-1034', 'P00-1061', 'D10-1050', 'Q13-1022', 'P11-1048', 'D13-1172', 'P14-1044', 'W14-4322', 'J10-4006', 'W06-1607', 'W15-4924', 'P06-1028', 'P95-1016', 'D13-1034', 'P13-1052', 'D15-1139', 'N07-1045', 'D14-1124', 'D11-1031', 'P10-1031', 'S15-1017', 'N03-1024', 'W04-3212', 'P09-1028', 'D13-1047', 'D09-1094', 'W97-0309', 'P14-1011', 'D15-1165', 'A00-2040', 'N09-1012', 'E12-1076', 'D13-1046', 'D10-1113', 'P11-1150', 'P13-1157', 'J09-1005', 'P14-1069', 'J07-1003', 'P10-1129', 'D12-1090', 'P08-1043', 'E09-1048', 'D14-1096', 'W03-1006', 'D14-1105', 'D14-1076', 'N13-1122', 'P06-1079', 'J90-3003', 'P14-1108', 'P15-1075', 'W14-4328', 'W15-4612', 'P15-1041', 'E09-1027', 'N03-1029', 'D15-1310', 'D14-1108', 'E06-1031', 'E14-1016', 'P14-1048', 'P96-1025', 'P15-1009', 'W14-1613', 'P14-1136', 'P13-1130', 'D14-1021', 'D08-1069', 'P08-1115', 'P96-1018', 'P06-1054', 'W11-0131', 'P07-1001', 'E12-1067', 'D10-1120', 'P11-1074', 'N12-1013', 'P12-1045', 'A00-1019', 'D14-1023', 'D15-1045', 'D15-1149', 'D15-1296', 'P12-1051', 'D11-1018', 'P04-1030', 'P15-1015', 'P10-1073', 'D13-1143', 'E14-1013', 'E14-1035', 'N06-1007', 'P08-1023', 'E12-1062', 'D10-1102', 'P13-1137', 'E09-1096', 'P15-1145', 'N03-1002', 'W15-4902', 'D11-1049', 'N10-1115', 'D09-1133', 'W00-1317', 'D14-1082', 'S14-1010', 'N10-1069', 'W13-4030', 'N12-1090', 'E99-1026', 'P15-1002', 'P05-1049', 'P15-1001', 'W97-0315', 'W06-1657', 'P12-1054', 'Q14-1003', 'P06-1109', 'P00-1070', 'E12-1033', 'E14-1061', 'P14-1091', 'P14-1022', 'P14-1084', 'P14-2033', 'Q14-1031', 'D10-1054', 'P00-1007', 'P00-1032', 'D10-1026', 'E14-1044', 'P15-1115', 'W14-4327', 'W14-4325', 'E06-1038', 'W11-3203', 'P14-1004', 'D15-1205', 'D08-1022', 'P03-1057', 'D11-1102', 'D13-1131', 'E12-1022', 'P13-1069', 'D11-1089', 'P13-1017', 'D13-1026', 'Q14-1023', 'N12-1017', 'E14-1062', 'P08-1027', 'Q15-1007', 'P05-1023', 'P09-1050', 'A94-1026', 'J04-1001', 'P15-1143', 'P08-1004', 'Q13-1025', 'D09-1021', 'D09-1090', 'J06-4003', 'D15-1042', 'N10-1122', 'P98-1103', 'D11-1081', 'P14-1003', 'N13-1138', 'J94-4001', 'W06-1669', 'P03-1013', 'P15-1078', 'P12-1075', 'P91-1041', 'W15-1818', 'P09-1054', 'W06-2912', 'W11-2024', 'P14-1057', 'W13-2702', 'P05-1002', 'N09-1029', 'P09-1021', 'N13-1132', 'N13-1075', 'N13-1001', 'P09-1047', 'D11-1142', 'D13-1156', 'P03-1001', 'D13-1015', 'J02-3001', 'W11-0324', 'D14-1166', 'P02-1011', 'N13-1015', 'W03-1010', 'P15-1021', 'D12-1002', 'P03-1042', 'D14-1137', 'D13-1041', 'P10-1011', 'N07-1055', 'P10-1137', 'D15-1159', 'W02-1006', 'N13-1044', 'D13-1029', 'D12-1006', 'D15-1089', 'P08-1028', 'P14-1130', 'W14-6803', 'D14-1139', 'D12-1075', 'D09-1115', 'E14-1046', 'P05-1024', 'W06-1659', 'P04-1078', 'W12-5502', 'D10-1002', 'J15-3005', 'P13-1088', 'P06-1116', 'P05-1015', 'J01-1002', 'D13-1164', 'D10-1003', 'W12-1639', 'N12-1045', 'P14-1005', 'D12-1111', 'D10-1043', 'P05-1059', 'D15-1040', 'P04-1076', 'D14-1012', 'P06-1136', 'W06-1671', 'P15-1028', 'D12-1051', 'P13-1171', 'Q13-1001', 'P08-1113', 'N12-1011', 'D09-1071', 'P12-1055', 'P14-1138', 'D12-1029', 'D12-1089', 'N13-1006', 'D14-1011', 'D14-1193', 'P15-1007', 'A00-3007', 'D13-1169', 'P09-1006', 'D10-1019', 'N10-1076', 'P14-1121', 'P10-1044', 'P14-1041', 'P09-1105', 'D13-1197', 'W12-1901', 'P06-1001', 'P11-1144', 'J10-4007', 'P15-1064', 'D08-1109', 'P15-1127', 'D12-1060', 'D11-1038', 'P08-1087', 'E14-1078', 'D13-1050', 'D15-1142', 'E97-1063', 'W00-1305', 'P15-1116', 'D09-1074', 'P11-1028', 'E12-1029', 'E12-1014', 'P10-1042', 'P15-1042', 'P04-1042', 'E93-1036', 'D08-1062', 'E09-1076', 'P98-2237', 'P14-1095', 'W06-1631', 'W09-3910', 'Q14-1008', 'D08-1096', 'P99-1067', 'D15-1218', 'W15-4914', 'W00-1302', 'P13-1034', 'S15-1034', 'P12-1061', 'N06-1031', 'P12-1078', 'P10-1017', 'N10-1015', 'D13-1004', 'D12-1131', 'P05-1067', 'K15-1002', 'D15-1027', 'D13-1185', 'P10-1067', 'D13-1110', 'P11-1070', 'P04-1017', 'P02-1017', 'K15-1022', 'P04-1027', 'D08-1021', 'E12-1023', 'D09-1140', 'S15-1005', 'D11-1108', 'D08-1070', 'N10-1125', 'N12-1051', 'P09-1005', 'P03-1011', 'P03-1003', 'D15-1137', 'P03-1044', 'P06-1141', 'E09-1068', 'D14-1092', 'E06-1015', 'D13-1023', 'P05-1034', 'W15-4907', 'P14-1008', 'D15-1011', 'P12-1011', 'N13-1030', 'E09-1035', 'P11-1092', 'P15-1104', 'P06-1027', 'N10-1016', 'P11-1163', 'P06-1098', 'J98-2002', 'P14-1019', 'D09-1088', 'P96-1030', 'D13-1013', 'J10-3005', 'P14-1071', 'D15-1025', 'D10-1049', 'D09-1092', 'Q13-1023', 'D14-1035', 'D14-1029', 'P88-1005', 'P03-1026', 'P14-1099', 'D12-1102', 'P12-1022', 'P06-1073', 'P11-1157', 'P09-1049', 'D13-1105', 'P08-1112', 'E06-1046', 'P15-1084', 'N12-1029', 'P10-1125', 'D11-1023', 'D15-1010', 'S15-1010', 'P10-1065', 'P13-1021', 'P15-1058', 'W12-1905', 'P12-1013', 'Q14-1014', 'N07-1061', 'P15-1171', 'D12-1114', 'E12-1032', 'Q14-1021', 'J14-2007', 'P91-1004', 'D12-1055', 'A00-2025', 'D15-1183', 'P13-1014', 'P98-2204', 'D14-1014', 'D12-1088', 'P10-1134', 'E06-1039', 'N13-1038', 'W13-3514', 'P09-1013', 'W00-1327', 'W06-1640', 'N07-1006', 'P13-1147', 'D13-1054', 'E91-1012', 'D11-1091', 'P13-1062', 'P15-1061', 'D11-1077', 'D14-1054', 'D13-1155', 'P11-1016', 'J99-1003', 'D08-1037', 'D13-1120', 'D14-1019', 'P13-1049', 'P15-1034', 'D12-1015', 'W05-0604', 'N13-1107', 'P08-1049', 'D15-1016', 'D14-1088', 'D09-1003', 'P11-1008', 'D15-1289', 'P15-1017', 'D13-1123', 'D10-1031', 'W15-4626', 'P10-1038', 'W13-4005', 'D12-1013', 'D14-1033', 'D12-1100', 'E06-1044', 'D11-1019', 'W10-2903', 'N09-1023', 'D13-1089', 'P11-1089', 'D11-1033', 'D13-1116', 'D14-1192', 'D11-1099', 'D12-1138', 'W06-1668', 'D09-1042', 'A97-1013', 'D09-1075', 'D14-1111', 'P06-1072', 'P08-1084', 'P14-1103', 'D14-1016', 'S07-1075', 'P02-1057', 'N10-1024', 'P15-1144', 'N13-1023', 'W11-0315', 'D14-1200', 'D15-1110', 'P12-1106', 'D14-1116', 'W15-4602', 'D13-1170', 'N06-1021', 'D12-1019', 'N07-1023', 'P15-1004', 'D13-1022', 'P13-1026', 'D14-1220', 'P15-1173', 'P08-1108', 'N09-1002', 'E14-1005', 'P15-1152', 'E12-1036', 'P14-1007', 'D13-1193', 'W03-1026', 'N07-1005', 'N10-1005', 'D15-1101', 'D15-1156', 'P06-1092', 'N10-1011', 'P04-1014', 'J15-1005', 'E03-1035', 'P06-1012', 'P08-1053', 'D09-1043', 'N06-1047', 'P14-1035', 'S15-1020', 'P15-1132', 'D14-1171', 'W06-2906', 'P11-1082', 'D09-1087', 'P11-1149', 'D14-1157', 'W01-0511', 'P99-1046', 'N13-1063', 'W04-3226', 'D13-1135', 'P10-1116', 'D11-1005', 'D15-1146', 'D12-1107', 'P06-1087', 'P09-1019', 'P13-1065', 'Q15-1002', 'N12-1008', 'D13-1018', 'D13-1030', 'D12-1077', 'D14-1038', 'D10-1061', 'N13-1055', 'P14-1115', 'J12-1003', 'D10-1017', 'D12-1084', 'D12-1033', 'P14-1072', 'W02-1013', 'W12-1506', 'D13-1032', 'P10-1013', 'P14-1034', 'W14-4407', 'N13-1135', 'P12-1007', 'D11-1035', 'D13-1180', 'P10-1111', 'P10-1018', 'P13-1057', 'P06-1017', 'P14-1093', 'Q14-1041', 'N07-1051', 'P06-1011', 'P08-1077', 'D08-1092', 'D14-1194', 'P03-1062', 'P12-1038', 'D13-1044', 'P13-1068', 'K15-1009', 'D12-1034', 'D09-1125', 'Q13-1030', 'P11-1130', 'W14-0610', 'D11-1105', 'P11-1007', 'D11-1060', 'P06-1104', 'P14-1002', 'N13-1117', 'D13-1183', 'D15-1053', 'D15-1138', 'P10-1074', 'P06-1101', 'W15-4906', 'D10-1025', 'D14-1207', 'D13-1081', 'P13-1128', 'J07-4002', 'D15-1085', 'J15-2004', 'D14-1135', 'D14-1142', 'P13-1016', 'W06-1653', 'N07-1059', 'J13-4004', 'D15-1200', 'W14-1611', 'P10-1001', 'P10-1034', 'P12-1032', 'D14-1052', 'P13-1010', 'P85-1014', 'P06-1051', 'N06-1014', 'J81-3002', 'N09-1022', 'W12-1607', 'E06-1014', 'P14-1052', 'E14-1039', 'P04-1013', 'D14-1222', 'D09-1111', 'P06-1046', 'N09-1019', 'P14-1046', 'J94-4002', 'E14-1070', 'P10-1027', 'D13-1069', 'P08-1085', 'P13-1173', 'D13-1014', 'E14-1009', 'N10-1131', 'J11-4008', 'E09-1013', 'D13-1144', 'E14-1037', 'P10-1033', 'W06-1615', 'W13-3510', 'D15-1135', 'J12-4002', 'D10-1091', 'D14-1063', 'P05-1073', 'P11-2012', 'J11-1006', 'P12-1101', 'D15-1272', 'E12-1060', 'P05-1066', 'D14-1189', 'D10-1028', 'N13-1074', 'W99-0614', 'P14-1050', 'D09-1009', 'P06-1088', 'P13-1105', 'D13-1077', 'W06-1634', 'W01-0506', 'P06-1105', 'W14-4305', 'D12-1092', 'D11-1022', 'E12-1016', 'D15-1041', 'W09-1107', 'Q13-1020', 'D14-1163', 'D09-1001', 'W06-1665', 'D15-1019', 'J08-2002', 'P92-1006', 'W10-2921', 'P15-1082', 'N13-1102', 'P04-1016', 'P10-1026', 'E09-1084', 'P98-2234', 'D12-1044', 'N10-1119', 'D12-1001', 'W08-1107', 'D13-1201', 'D14-1169', 'P12-1044', 'D12-1110', 'P10-1145', 'P03-1066', 'W09-3928', 'D13-1056', 'N04-1029', 'W99-0605', 'P11-1084', 'P12-1049', 'J96-4004', 'D13-1101', 'J01-3002', 'P13-1125', 'J09-4009', 'Q13-1024', 'D13-1067', 'E14-1006', 'P13-1001', 'P11-1110', 'E12-1075', 'P13-1002', 'W15-2201', 'D08-1113', 'D09-1052', 'P11-1051', 'P06-1038', 'J06-2003', 'Q15-1004', 'P02-1002', 'P14-1137', 'P04-1056', 'D13-1009', 'W13-3508', 'Q13-1010', 'N12-1048', 'N06-1036', 'N10-1134', 'D14-1213', 'D09-1124', 'D11-1012', 'E12-1009', 'P15-1037', 'P14-1009', 'D15-1031', 'W02-1012', 'A97-1043', 'P11-1123', 'A00-2009', 'N03-1030']\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_old = []\n",
    "for file in os.listdir(\"xmls/\"):\n",
    "    ids_old.append(file[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(set(ids).intersection(set(ids_old)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1181"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "c = 0\n",
    "for key in ids :\n",
    "    papers = cit_titles[key]\n",
    "    for paper in papers :\n",
    "        data = {}\n",
    "        data['paper_id'] = key\n",
    "        data['citation_title'] = paper\n",
    "        for pap in location_feature[key]:\n",
    "            if(pap['paper_name']==paper) :\n",
    "                data['location_feature'] = pap['location_feature']\n",
    "                break\n",
    "                \n",
    "        for pap in num_table[key]:\n",
    "            if(pap['paper_name']==paper) :\n",
    "                data['num_table'] = pap['num_table']\n",
    "                break\n",
    "                \n",
    "        for pap in title_overlap[key]:\n",
    "            if(pap['paper_name']==paper) :\n",
    "                data['title_overlap'] = pap['overlap']\n",
    "                break\n",
    "                \n",
    "        for pap in context_count[key]:\n",
    "            if(pap['paper_name']==paper) :\n",
    "                data['context_count'] = pap['context_count']\n",
    "                break\n",
    "                \n",
    "        for pap in contexts_dist[key] :\n",
    "            if(pap['paper_name']==paper) :\n",
    "#                 data['context'] = pap['context']\n",
    "                data['distances'] = pap['distances']\n",
    "                break\n",
    "        \n",
    "        for pap in contexts[key] :\n",
    "            if(pap['paper_name']==paper) :\n",
    "                data['context'] = pap['context']\n",
    "                break\n",
    "                \n",
    "        for pap in cue_words[key]:\n",
    "            if(pap['paper_name']==paper) :\n",
    "                data['cue_count'] = pap['cue_count']\n",
    "                break\n",
    "                \n",
    "        for pap in tags[key]:\n",
    "            if(pap['paper_name']==paper) :\n",
    "                if(pap['tag']==1):\n",
    "                    data['label'] = 'baseline'\n",
    "                else :\n",
    "                    data['label'] = 'non_baseline'\n",
    "                break\n",
    "         \n",
    "        for pap in fixed_context[key]:\n",
    "            if(pap['paper_name']==paper) :\n",
    "                data['fixed_context'] = pap['fixed_context']                   \n",
    "        dataset.append(data)   \n",
    "        \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31943\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27636\n"
     ]
    }
   ],
   "source": [
    "final_dataset = []\n",
    "for data in dataset :\n",
    "    if(data['context_count']!=0):\n",
    "        final_dataset.append(data)\n",
    "print(len(final_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = dataset[:int(0.8*len(dataset))]\n",
    "testset = dataset[int(0.8*len(dataset)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(probs):\n",
    "\n",
    "\tsum = 0\n",
    "\tfor key, value in probs.items():\n",
    "\t\tsum+=value\n",
    "\tsum = round(sum, 7)\n",
    "\treturn sum==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_model(trainset, testset):\n",
    "\n",
    "    word_baseline_frequency = {}\n",
    "    word_non_baseline_frequency = {}\n",
    "    baseline_length = 0\n",
    "    non_baseline_length = 0\n",
    "\n",
    "    for i in trainset:\n",
    "        # if len(i['fixed_context'])>1 and i['label']=='baseline':\n",
    "        # \tcontinue\n",
    "        for context in i['fixed_context']:\n",
    "            if i['label']=='baseline':\n",
    "                baseline_length+=len(context)\n",
    "            else:\n",
    "                non_baseline_length+=len(context)\n",
    "            for word in context:\n",
    "                if word not in word_baseline_frequency:\n",
    "                    word_baseline_frequency[word] = 0\n",
    "                    word_non_baseline_frequency[word] = 0\n",
    "                if i['label']=='baseline':\n",
    "                    word_baseline_frequency[word]+=1\n",
    "                else:\n",
    "                    word_non_baseline_frequency[word]+=1\n",
    "\n",
    "    final_word_baseline_frequency = {}\n",
    "    final_word_baseline_frequency['<unk>'] = 0\n",
    "    word_non_baseline_frequency['<unk>'] = 0\n",
    "\n",
    "    for word in word_baseline_frequency:\n",
    "        if word_baseline_frequency[word]>5:\t\n",
    "            final_word_baseline_frequency[word] = word_baseline_frequency[word]\n",
    "        else:\n",
    "            final_word_baseline_frequency['<unk>'] += word_baseline_frequency[word]\n",
    "            word_non_baseline_frequency['<unk>'] += word_non_baseline_frequency[word]\n",
    "            del word_non_baseline_frequency[word]\n",
    "\n",
    "    word_baseline_frequency = final_word_baseline_frequency\n",
    "\n",
    "    word_baseline_probability = {}\n",
    "    word_non_baseline_probability = {}\n",
    "\n",
    "    for word in word_baseline_frequency:\n",
    "        word_baseline_probability[word] = (word_baseline_frequency[word]+1)/(baseline_length+len(word_baseline_frequency))\n",
    "        word_non_baseline_probability[word] = (word_non_baseline_frequency[word]+1)/(non_baseline_length+len(word_non_baseline_frequency))\n",
    "\n",
    "    # print(word_baseline_probability['<unk>'])\n",
    "    # print(word_non_baseline_probability['<unk>'])\n",
    "\n",
    "    assert sanity_check(word_baseline_probability) and sanity_check(word_non_baseline_probability)\n",
    "\n",
    "    conf_mat1 = np.zeros((2,2))\n",
    "    conf_mat2 = np.zeros((2,2))\n",
    "\n",
    "    min_p = 1\n",
    "\n",
    "    for i in trainset+testset:\n",
    "        P_b = 0\n",
    "        P_nb = 0\n",
    "        maxP_b = 0\n",
    "        minP_nb = 1\n",
    "        n_better = 0\n",
    "        for context in i['fixed_context']:\n",
    "            \n",
    "            P_b_con = 1\n",
    "            P_nb_con = 1\n",
    "            for word in context:\n",
    "                if word not in word_baseline_probability:\n",
    "                    P_b_con*=word_baseline_probability['<unk>']\n",
    "                    P_nb_con*=word_non_baseline_probability['<unk>']\n",
    "                else:\n",
    "                    P_b_con*=word_baseline_probability[word]\n",
    "                    P_nb_con*=word_non_baseline_probability[word]\n",
    "            if P_b_con>P_nb_con:\n",
    "                n_better+=1\n",
    "            P_b+=P_b_con\n",
    "            P_nb+=P_nb_con\n",
    "            maxP_b = max(P_b, P_b_con)\n",
    "            minP_nb = min(P_nb, P_nb_con)\n",
    "            P_b/=len(i['fixed_context'])\n",
    "            P_nb/=len(i['fixed_context'])\n",
    "\n",
    "        # if P_b>P_nb:\n",
    "        # \tprint('yes')\n",
    "\n",
    "        # i['P_b'] = P_b\n",
    "        # i['P_nb'] = P_nb\n",
    "        min_p = min(min_p, P_b, P_nb)\n",
    "        i['lmp'] = [P_b, P_nb, maxP_b, minP_nb, P_b/P_nb, maxP_b/minP_nb, n_better]\n",
    "\n",
    "    for i in trainset:\t\n",
    "        # if len(i['fixed_context'])>1 and i['label']=='baseline':\n",
    "        # \ttestset.append(i)\n",
    "        # \tcontinue\n",
    "        i['lmp'][0]/=min_p\n",
    "        i['lmp'][1]/=min_p\n",
    "        i['lmp'][2]/=min_p\n",
    "        i['lmp'][3]/=min_p\n",
    "        if i['lmp'][0]>i['lmp'][1]:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat1[0, 0]+=1\n",
    "            else:\n",
    "                conf_mat1[0, 1]+=1\n",
    "        else:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat1[1, 0]+=1\n",
    "            else:\n",
    "                conf_mat1[1, 1]+=1\n",
    "        if i['lmp'][2]>i['lmp'][3]:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat2[0, 0]+=1\n",
    "            else:\n",
    "                conf_mat2[0, 1]+=1\n",
    "        else:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat2[1, 0]+=1\n",
    "            else:\n",
    "                conf_mat2[1, 1]+=1\n",
    "\n",
    "    # print(conf_mat1)\n",
    "    prec = conf_mat1[0,0]/(conf_mat1[0,0]+conf_mat1[0,1])\n",
    "    rec = conf_mat1[0,0]/(conf_mat1[0,0]+conf_mat1[1,0])\n",
    "    print(prec, rec, 2*prec*rec/(prec+rec))\n",
    "    conf_mat1 = np.zeros((2,2))\n",
    "\n",
    "    # print(conf_mat2)\n",
    "    prec = conf_mat2[0,0]/(conf_mat2[0,0]+conf_mat2[0,1])\n",
    "    rec = conf_mat2[0,0]/(conf_mat2[0,0]+conf_mat2[1,0])\n",
    "    print(prec, rec, 2*prec*rec/(prec+rec))\n",
    "#     conf_mat2 = np.zeros((2,2))\n",
    "\n",
    "    for i in testset:\n",
    "        i['lmp'][0]/=min_p\n",
    "        i['lmp'][1]/=min_p\n",
    "        i['lmp'][2]/=min_p\n",
    "        i['lmp'][3]/=min_p\n",
    "        if i['lmp'][0]>i['lmp'][1]:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat1[0, 0]+=1\n",
    "            else:\n",
    "                conf_mat1[0, 1]+=1\n",
    "        else:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat1[1, 0]+=1\n",
    "            else:\n",
    "                conf_mat1[1, 1]+=1\n",
    "        if i['lmp'][2]>i['lmp'][3]:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat2[0, 0]+=1\n",
    "            else:\n",
    "                conf_mat2[0, 1]+=1\n",
    "        else:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat2[1, 0]+=1\n",
    "            else:\n",
    "                conf_mat2[1, 1]+=1\n",
    "\n",
    "    # print(conf_mat1)\n",
    "    prec = conf_mat1[0,0]/(conf_mat1[0,0]+conf_mat1[0,1])\n",
    "    rec = conf_mat1[0,0]/(conf_mat1[0,0]+conf_mat1[1,0])\n",
    "    print(prec, rec, 2*prec*rec/(prec+rec))\n",
    "    conf_mat1 = np.zeros((2,2))\n",
    "\n",
    "    # print(conf_mat2)\n",
    "    prec = conf_mat2[0,0]/(conf_mat2[0,0]+conf_mat2[0,1])\n",
    "    rec = conf_mat2[0,0]/(conf_mat2[0,0]+conf_mat2[1,0])\n",
    "    print(prec, rec, 2*prec*rec/(prec+rec))\n",
    "    conf_mat2 = np.zeros((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2964485981308411 0.7232102143182855 0.4205223385920721\n",
      "0.25894213506421765 0.8549931600547196 0.39749841000636\n",
      "0.23736263736263735 0.5955882352941176 0.33944473546359355\n",
      "0.2523804312137463 0.8425283156740957 0.38841165571837627\n"
     ]
    }
   ],
   "source": [
    "lm_model(trainset,testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_feature(trainset, testset) :\n",
    "    total_counts = {}\n",
    "    citation_frequency = {}\n",
    "    paper_frequency = {}\n",
    "    curr_doc = trainset[0]['paper_id']\n",
    "    paper_words = set([])\n",
    "\n",
    "    for i in trainset:\n",
    "        if i['paper_id']!=curr_doc:\n",
    "            for word in paper_words:\n",
    "                if word not in paper_frequency:\n",
    "                    paper_frequency[word] = 0\n",
    "                paper_frequency[word]+=1\n",
    "            paper_words = set([])\n",
    "            curr_doc = i['paper_id']\n",
    "        words = set(i['context'])\n",
    "        paper_words = paper_words.union(words)\n",
    "        for word in words:\n",
    "            if word not in total_counts:\n",
    "                total_counts[word] = 0\n",
    "                citation_frequency[word] = 0\n",
    "            citation_frequency[word]+=1\n",
    "        counts = {word:0 for word in words}\n",
    "        for word in i['context']:\n",
    "            counts[word]+=1\n",
    "            total_counts[word]+=1\n",
    "\n",
    "    for word in paper_words:\n",
    "        if word not in paper_frequency:\n",
    "            paper_frequency[word] = 0\n",
    "        paper_frequency[word]+=1\n",
    "\n",
    "    final_citation_frequency = {word:count for word, count in citation_frequency.items()}\n",
    "    for i in citation_frequency:\n",
    "        if paper_frequency[i]<=20:\n",
    "            del total_counts[i]\n",
    "            del final_citation_frequency[i]\n",
    "\n",
    "    citation_frequency = final_citation_frequency\n",
    "    \n",
    "    dataset = trainset+testset\n",
    "    \n",
    "    idfs = {}\n",
    "\n",
    "    for word in citation_frequency:\n",
    "        idfs[word] = math.log10(len(dataset)*0.8/citation_frequency[word])\n",
    "    \n",
    "    total_counts['<unk>'] = 0\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        words = set(dataset[i]['context']) & set(total_counts.keys())\n",
    "        unigram_counts = {word:0 for word in words}\n",
    "        try:\t\n",
    "            del unigram_counts['<SOS>']\n",
    "            del unigram_counts['<EOS>']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for j in range(len(dataset[i]['context'])):\n",
    "            word = dataset[i]['context'][j]\n",
    "            if word not in unigram_counts:\n",
    "                continue\n",
    "            else:\n",
    "                unigram_counts[word]+=1-dataset[i]['distances'][j]\n",
    "\n",
    "\n",
    "        dataset[i]['unigrams'] = unigram_counts\n",
    "\n",
    "\n",
    "    total_unigram_counts = total_counts\n",
    "    features = list(total_unigram_counts.keys())\n",
    "    ngram_to_idx = {features[i]:i for i in range(len(features))}\n",
    "    \n",
    "    pickle.dump(total_counts, open('pickles/total_counts.pkl', 'wb'))\n",
    "    pickle.dump(dataset, open('pickles/data_ngram.pkl', 'wb'))\n",
    "    pickle.dump(ngram_to_idx, open('pickles/ngram_to_idx.pkl', 'wb'))\n",
    "    pickle.dump(idfs, open('pickles/idfs.pkl', 'wb'))\n",
    "    \n",
    "    data_mat = np.zeros((len(dataset), len(ngram_to_idx)+35+1+5+1))\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        context_counts = {}\n",
    "        for word in dataset[i]['unigrams']:\n",
    "            if word not in context_counts:\n",
    "                context_counts[word] = 0\n",
    "            context_counts[word]+=dataset[i]['unigrams'][word]\n",
    "            if word in idfs:\n",
    "                data_mat[i, ngram_to_idx[word]] = dataset[i]['unigrams'][word]\n",
    "\n",
    "        dataset[i]['context_feature'] = data_mat[i, :len(ngram_to_idx)]\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = context_feature(trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "output = []\n",
    "for data in dataset :\n",
    "    ar = []\n",
    "    ar.append(data['context_count'])\n",
    "    ar.append(data['title_overlap'])\n",
    "    ar.append(data['cue_count'])\n",
    "#     ar.append(sum(data['location_feature']))\n",
    "    ar.append(data['num_table'])\n",
    "    ar.extend(data['location_feature'])\n",
    "    ar.extend(data['lmp'])\n",
    "#     ar.extend(data['context_feature'])\n",
    "    values.append(ar)\n",
    "    if(data['label']=='baseline'):\n",
    "        output.append(1)\n",
    "    else :\n",
    "        output.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.array(values)\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27636, 2744)\n"
     ]
    }
   ],
   "source": [
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std_scale = MinMaxScaler((0,1))\n",
    "# values[:,0:-6] = std_scale.fit_transform(values[:,0:-6])\n",
    "scaler = MinMaxScaler((0,1))\n",
    "values = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, output):\n",
    "    n = len(data)\n",
    "    last = int(0.8*n)\n",
    "    train_data = data[:last]\n",
    "    train_output = output[:last]\n",
    "    test_data = data[last:]\n",
    "    test_output = output[last:]\n",
    "    return train_data, test_data, train_output, test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_output, test_output = split(values, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(train_data, train_output) :\n",
    "    baselines = []\n",
    "    non_baselines = []\n",
    "    for i in range(len(train_output)) :\n",
    "        if(train_output[i]==1) :\n",
    "            baselines.append(train_data[i])\n",
    "        else :\n",
    "            non_baselines.append(train_data[i])\n",
    "    \n",
    "    n = len(baselines)\n",
    "    ar = np.random.choice(len(non_baselines), len(baselines))\n",
    "    nb_ar = []\n",
    "    for x in ar :\n",
    "        nb_ar.append(non_baselines[x])\n",
    "        \n",
    "    data = []\n",
    "    data.extend(nb_ar)\n",
    "    output = []\n",
    "    for i in range(len(data)):\n",
    "        output.append(0)\n",
    "    \n",
    "    data.extend(baselines)\n",
    "    for i in range(n) :\n",
    "        output.append(1)\n",
    "        \n",
    "    total_data = []\n",
    "    for i in range(len(data)):\n",
    "        ar = []\n",
    "        ar.append(data[i])\n",
    "        ar.append(output[i])\n",
    "        total_data.append(ar)\n",
    "        \n",
    "    total_data = np.array(total_data)\n",
    "    np.random.shuffle(total_data)\n",
    "    \n",
    "    data = []\n",
    "    output = []\n",
    "    for ar in total_data :\n",
    "        data.append(ar[0])\n",
    "        output.append(ar[1])\n",
    "        \n",
    "    data = np.array(data)\n",
    "    output = np.array(output)\n",
    "    \n",
    "    return data, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_output = shuffle(train_data, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92      4984\n",
      "           1       0.40      0.73      0.51       544\n",
      "\n",
      "    accuracy                           0.87      5528\n",
      "   macro avg       0.68      0.80      0.72      5528\n",
      "weighted avg       0.91      0.87      0.88      5528\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93      2193\n",
      "           1       0.96      0.89      0.92      2193\n",
      "\n",
      "    accuracy                           0.92      4386\n",
      "   macro avg       0.93      0.92      0.92      4386\n",
      "weighted avg       0.93      0.92      0.92      4386\n",
      "\n",
      "{'C': 2, 'fit_intercept': True}\n"
     ]
    }
   ],
   "source": [
    "params = {'C':[0.1, 0.5, 1, 2], 'fit_intercept':[True, False]}\n",
    "modelin = LogisticRegression(solver='lbfgs', max_iter=1e4, n_jobs=5, random_state=1, warm_start=False, C=1)\n",
    "model = GridSearchCV(modelin, params, cv=2, n_jobs=5)\n",
    "#model = LogisticRegression(solver='lbfgs',max_iter=1e4, n_jobs=5, random_state=1, warm_start=False, C=1, fit_intercept=True)\n",
    "clf = model.fit(train_data, train_output)\n",
    "predict_test = clf.predict(test_data)\n",
    "print(classification_report(test_output, predict_test))\n",
    "predictions = clf.predict(train_data)\n",
    "print(classification_report(train_output, predictions))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
