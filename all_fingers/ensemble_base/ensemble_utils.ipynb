{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import math\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Section :\n",
    "Introduction \n",
    "Related work\n",
    "Method\n",
    "Experiment\n",
    "Evaluation\n",
    "Conclusion\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Existence of any cue word\n",
    "Number of cue words\n",
    "subject cue in neighbouring sentences\n",
    "weight of subject cue \n",
    "Location\n",
    "Popularity\n",
    "Density\n",
    "Average Density\n",
    "Synctatic feature\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting treetaggerwrapper\n",
      "  Downloading https://files.pythonhosted.org/packages/0e/9d/a48c990ca015a80ff8f19061aebba41509851b01c1b90bc7d0346af13ee0/treetaggerwrapper-2.3.tar.gz (43kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 141kB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: treetaggerwrapper\n",
      "  Running setup.py bdist_wheel for treetaggerwrapper ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/tanisha17116/.cache/pip/wheels/46/64/50/b1944d17fbf5b2882438a50bb37b3d9549443ef29e121b9a38\n",
      "Successfully built treetaggerwrapper\n",
      "Installing collected packages: treetaggerwrapper\n",
      "Successfully installed treetaggerwrapper-2.3\n"
     ]
    }
   ],
   "source": [
    "! pip3 install treetaggerwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_sents = pickle.load(open(\"../pickles/citation_sentences.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cit_strings = pickle.load(open(\"../pickles/all_cit_strings.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popularity(cit_sents, cit_strings):\n",
    "    dataset = {}\n",
    "    for key in cit_sents :\n",
    "        papers = cit_sents[key]\n",
    "        pap_list = []\n",
    "        for pap in papers :\n",
    "            dict1 = {}\n",
    "            dict1['paper_name'] = pap['paper_name']\n",
    "            sents = pap['sents']\n",
    "            pop = 0\n",
    "            for sent in sents :\n",
    "                sent = sent.lower()\n",
    "                for citstr in cit_strings :\n",
    "                    if(citstr in sent) :\n",
    "                        pop+=1\n",
    "            if(len(sents)>0) :\n",
    "                pop = pop/len(sents)\n",
    "            dict1['popularity'] = pop\n",
    "            pap_list.append(dict1)\n",
    "        dataset[key] = pap_list\n",
    "        \n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "popu = get_popularity(citation_sents, cit_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(popu, open(\"../pickles/en_popularity.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pickle.load(open(\"../pickles/reduced_context.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_density(context, cit_strings):\n",
    "    dataset = {}\n",
    "    for key in context :\n",
    "        papers = context[key]\n",
    "        pap_list = []\n",
    "        for pap in papers :\n",
    "            dict1 = {}\n",
    "            dict1['paper_name'] = pap['paper_name']\n",
    "            contexts = pap['context']\n",
    "            dens = 0\n",
    "            all_val = []\n",
    "            for con in contexts :\n",
    "                con = con.lower()\n",
    "                for citstr in cit_strings :\n",
    "                    if(citstr in con) :\n",
    "                        all_val.append(citstr)\n",
    "            dens = len(list(set(all_val)))\n",
    "            if(len(contexts)>0) :\n",
    "                dens = dens/len(contexts)\n",
    "            dict1['density'] = dens\n",
    "            pap_list.append(dict1)\n",
    "        dataset[key] = pap_list\n",
    "        \n",
    "    return dataset    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens = get_density(context, cit_strings)\n",
    "pickle.dump(popu, open(\"../pickles/en_density.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_words = [\"compare\", \"differ\", \"deviate\", \"contrast\", \"exceed\", \"outperform\", \"opposed\", \"consistent with\", \"significant\"]\n",
    "result_words = [\"result\", \"accuracy\", \"precision\", \"performance\", \"baseline\"]\n",
    "subject_cue = [\"we\", \"our\", \"us\", \"table\", \"figure\", \"paper\", \"algorithm\", \"here\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_stem = []\n",
    "result_stem = []\n",
    "sub_stem = []\n",
    "for word in comp_words :\n",
    "    comp_stem.append(stemmer.stem(word))\n",
    "    \n",
    "for word in result_words :\n",
    "    result_stem.append(stemmer.stem(word))\n",
    "    \n",
    "for word in subject_cue :\n",
    "    sub_stem.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_textual_features(context) :\n",
    "    dataset = {}\n",
    "    for key in context :\n",
    "        papers = context[key]\n",
    "        pap_list = []\n",
    "        for pap in papers :\n",
    "            dict1 = {}\n",
    "            dict1['paper_name'] = pap['paper_name']\n",
    "            contexts = pap['context']\n",
    "            dict1['weight_comp'] = 0\n",
    "            dict1['weight_result'] = 0\n",
    "            dict1['weight_subject'] = 0\n",
    "            for con in contexts :\n",
    "                words = word_tokenize(con)\n",
    "                for word in words :\n",
    "                    word = stemmer.stem(word)\n",
    "                    if(word in comp_stem) :\n",
    "                        dict1['weight_comp']+=1\n",
    "                    if(word in result_stem) :\n",
    "                        dict1['weight_result']+=1\n",
    "                    \n",
    "                    sents = sent_tokenize(con)\n",
    "                    for sent in sents :\n",
    "                        if(\"this_citation\" not in sent) :\n",
    "                            words_sent = word_tokenize(sent)\n",
    "                            for word in words_sent :\n",
    "                                word = stemmer.stem(word)\n",
    "                                if(word in sub_stem) :\n",
    "                                    dict1['weight_subject']+=1\n",
    "            \n",
    "            \n",
    "            if(len(contexts)>0) :\n",
    "                dict1['weight_comp'] /= len(context)\n",
    "                dict1['weight_result'] /= len(context)\n",
    "                dict1['weight_subject'] /= len(context)\n",
    "                \n",
    "            pap_list.append(dict1)\n",
    "            \n",
    "        dataset[key] = pap_list\n",
    "        \n",
    "    return dataset           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feat = get_textual_features(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(text_feat, open(\"../pickles/en_text_feat.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
