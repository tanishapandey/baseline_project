{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import math\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSection :\\nIntroduction \\nRelated work\\nMethod\\nExperiment\\nEvaluation\\nConclusion\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Section :\n",
    "Introduction \n",
    "Related work\n",
    "Method\n",
    "Experiment\n",
    "Evaluation\n",
    "Conclusion\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExistence of any cue word\\nNumber of cue words\\nsubject cue in neighbouring sentences\\nweight of subject cue \\nLocation\\nPopularity\\nDensity\\nAverage Density\\nSynctatic feature\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Existence of any cue word\n",
    "Number of cue words\n",
    "subject cue in neighbouring sentences\n",
    "weight of subject cue \n",
    "Location\n",
    "Popularity\n",
    "Density\n",
    "Average Density\n",
    "Synctatic feature\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting treetaggerwrapper\n",
      "Installing collected packages: treetaggerwrapper\n",
      "Successfully installed treetaggerwrapper-2.3\n"
     ]
    }
   ],
   "source": [
    "! pip3 install treetaggerwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_sents = pickle.load(open(\"../pickles/citation_sentences.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cit_strings = pickle.load(open(\"../pickles/all_cit_strings.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popularity(cit_sents, cit_strings):\n",
    "    dataset = {}\n",
    "    for key in cit_sents :\n",
    "        papers = cit_sents[key]\n",
    "        pap_list = []\n",
    "        for pap in papers :\n",
    "            dict1 = {}\n",
    "            dict1['paper_name'] = pap['paper_name']\n",
    "            sents = pap['sents']\n",
    "            pop = 0\n",
    "            for sent in sents :\n",
    "                sent = sent.lower()\n",
    "                for citstr in cit_strings :\n",
    "                    if(citstr in sent) :\n",
    "                        pop+=1\n",
    "            if(len(sents)>0) :\n",
    "                pop = pop/len(sents)\n",
    "            dict1['popularity'] = pop\n",
    "            pap_list.append(dict1)\n",
    "        dataset[key] = pap_list\n",
    "        \n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "popu = get_popularity(citation_sents, cit_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(popu, open(\"../pickles/en_popularity.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pickle.load(open(\"../pickles/reduced_context.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_density(context, cit_strings):\n",
    "    dataset = {}\n",
    "    for key in context :\n",
    "        papers = context[key]\n",
    "        pap_list = []\n",
    "        for pap in papers :\n",
    "            dict1 = {}\n",
    "            dict1['paper_name'] = pap['paper_name']\n",
    "            contexts = pap['context']\n",
    "            dens = 0\n",
    "            all_val = []\n",
    "            for con in contexts :\n",
    "                con = con.lower()\n",
    "                for citstr in cit_strings :\n",
    "                    if(citstr in con) :\n",
    "                        all_val.append(citstr)\n",
    "            dens = len(list(set(all_val)))\n",
    "            if(len(contexts)>0) :\n",
    "                dens = dens/len(contexts)\n",
    "            dict1['density'] = dens\n",
    "            pap_list.append(dict1)\n",
    "        dataset[key] = pap_list\n",
    "        \n",
    "    return dataset    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens = get_density(context, cit_strings)\n",
    "pickle.dump(dens, open(\"../pickles/en_density.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_words = [\"compare\", \"differ\", \"deviate\", \"contrast\", \"exceed\", \"outperform\", \"opposed\", \"consistent with\", \"significant\"]\n",
    "result_words = [\"result\", \"accuracy\", \"precision\", \"performance\", \"baseline\"]\n",
    "subject_cue = [\"we\", \"our\", \"us\", \"table\", \"figure\", \"paper\", \"algorithm\", \"here\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_stem = []\n",
    "result_stem = []\n",
    "sub_stem = []\n",
    "for word in comp_words :\n",
    "    comp_stem.append(stemmer.stem(word))\n",
    "    \n",
    "for word in result_words :\n",
    "    result_stem.append(stemmer.stem(word))\n",
    "    \n",
    "for word in subject_cue :\n",
    "    sub_stem.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_textual_features(context) :\n",
    "    dataset = {}\n",
    "    count = 0\n",
    "    for key in context :\n",
    "        papers = context[key]\n",
    "        pap_list = []\n",
    "        for pap in papers :\n",
    "            dict1 = {}\n",
    "            dict1['paper_name'] = pap['paper_name']\n",
    "            contexts = pap['context']\n",
    "            dict1['weight_comp'] = 0\n",
    "            dict1['weight_result'] = 0\n",
    "            dict1['weight_subject'] = 0\n",
    "            for con in contexts :\n",
    "                words = word_tokenize(con)\n",
    "                for word in words :\n",
    "                    if(word in comp_words) :\n",
    "                        dict1['weight_comp']+=1\n",
    "                    if(word in result_words) :\n",
    "                        dict1['weight_result']+=1\n",
    "                    \n",
    "                    sents = sent_tokenize(con)\n",
    "                    for sent in sents :\n",
    "                        if(\"this_citation\" not in sent) :\n",
    "                            words_sent = word_tokenize(sent)\n",
    "                            for word in words_sent :\n",
    "                                if(word in subject_cue) :\n",
    "                                    dict1['weight_subject']+=1\n",
    "            \n",
    "            \n",
    "            if(len(contexts)>0) :\n",
    "                dict1['weight_comp'] /= len(context)\n",
    "                dict1['weight_result'] /= len(context)\n",
    "                dict1['weight_subject'] /= len(context)\n",
    "                \n",
    "            pap_list.append(dict1)\n",
    "            \n",
    "        dataset[key] = pap_list\n",
    "        count+=1\n",
    "        if(count%100==0) :\n",
    "            print(count)\n",
    "        \n",
    "    return dataset           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "text_feat = get_textual_features(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(text_feat, open(\"../pickles/en_textfeat_no_stem.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
