{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import math\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "from statistics import stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_dump(obj, filename) :\n",
    "    pickle.dump(obj, open(\"pickles/\"+filename+\".pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGet reduced citation context \\n1. context features \\n2. similarity features :\\n\\n2We use the vector space based model\\n(Turney and Pantel, 2010) after stemming the words using\\nPorter stammer (Porter, 1997).\\n\\n     Then we calculate the cosine-similarity2\\n    between the title (T) of Pj and (i) SF:TTitle. the title, (ii) SF:TAbs. the abstract, SF:TIntro. the introduction, (iv) SF:TConcl. the conclusion, and (v)\\n    SF:TRest. the rest of the sections (sections other\\n    than abstract, introduction and conclusion) of Pi\\n    \\n    . Therefore,\\nwe take the same similarity based approach mentioned above, but replace the title of Pj with its RC\\nand obtain five more features: (vi) SF:RCTitle, (vii)\\nSF:RCAbs, (viii) SF:RCIntro, (ix) SF:RCConcl and\\n(x) SF:RCRest. If a reference appears multiple times\\nin a citing paper, we consider the aggregation of all\\nRCs together.\\n.\\n3. frequency \\nWe count the frequency\\nof Rij in (i) FF:Whole. the entire content, (ii)\\nFF:Intro. the introduction, (iii) FF:Rel. the related\\nwork, (iv) FF:Rest. the rest of the sections (as mentioned in Section 3.3.2) of Pi\\n. We also introduce (v)\\nFF:Sec. to measure the fraction of different sections\\nof Pi where Rij occurs (assuming that appearance of\\nRij in different sections is more influential). These\\nfeatures are further normalized using the number of\\nsentences in Pi\\nin order to avoid unnecessary bias on\\nthe size of the paper.\\n\\n4. Position \\nFor the first two features, we divide\\nthe entire paper into two parts equally based on the\\nsentence count and then see whether Rij appears (i)\\nPF:Begin. in the beginning or (ii) PF:End. in the\\nend of Pi\\n. Importantly, if Rij appears multiple times\\nin Pi\\n, we consider the fraction of times it occurs in\\neach part.\\n\\n (iii) PF:Mean. mean position of appearance, (iv)\\nPF:Std. standard deviation of different appearances.\\nThese features are normalized by the total length\\n(number of sentences) of Pi\\n. , thus ranging from 0\\n(indicating beginning of Pi) to 1 (indicating the end\\nof Pi).\\n\\n5. Linguistic\\n6. Miscellaneous\\n\\nMS:GCount. To answer\\nwhether a highly-cited paper has more academic influence on the citing paper than the one which is less\\ncited, we measure the number of other papers (except Pi) citing Pj .\\n(ii) MS:SelfC. To see the effect of self-citation, we\\ncheck whether at least one author is common in both\\nPi and Pj .\\n(iii) MG:Time. The fact that older papers are rarely\\ncited, may not stipulate that these are less influential.\\nTherefore, we measure the difference of the publication years of Pi and Pj .\\n(iv) MG:CoCite. It measures the co-citation counts\\nof Pi and Pj defined by |Ri∩Rj |\\n|Ri∪Rj |\\n, which in turn answers the significance of reference-based similarity\\ndriving the academic influence '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get reduced citation context \n",
    "1. context features \n",
    "2. similarity features :\n",
    "\n",
    "2We use the vector space based model\n",
    "(Turney and Pantel, 2010) after stemming the words using\n",
    "Porter stammer (Porter, 1997).\n",
    "\n",
    "     Then we calculate the cosine-similarity2\n",
    "    between the title (T) of Pj and (i) SF:TTitle. the title, (ii) SF:TAbs. the abstract, SF:TIntro. the introduction, (iv) SF:TConcl. the conclusion, and (v)\n",
    "    SF:TRest. the rest of the sections (sections other\n",
    "    than abstract, introduction and conclusion) of Pi\n",
    "    \n",
    "    . Therefore,\n",
    "we take the same similarity based approach mentioned above, but replace the title of Pj with its RC\n",
    "and obtain five more features: (vi) SF:RCTitle, (vii)\n",
    "SF:RCAbs, (viii) SF:RCIntro, (ix) SF:RCConcl and\n",
    "(x) SF:RCRest. If a reference appears multiple times\n",
    "in a citing paper, we consider the aggregation of all\n",
    "RCs together.\n",
    ".\n",
    "3. frequency \n",
    "We count the frequency\n",
    "of Rij in (i) FF:Whole. the entire content, (ii)\n",
    "FF:Intro. the introduction, (iii) FF:Rel. the related\n",
    "work, (iv) FF:Rest. the rest of the sections (as mentioned in Section 3.3.2) of Pi\n",
    ". We also introduce (v)\n",
    "FF:Sec. to measure the fraction of different sections\n",
    "of Pi where Rij occurs (assuming that appearance of\n",
    "Rij in different sections is more influential). These\n",
    "features are further normalized using the number of\n",
    "sentences in Pi\n",
    "in order to avoid unnecessary bias on\n",
    "the size of the paper.\n",
    "\n",
    "4. Position \n",
    "For the first two features, we divide\n",
    "the entire paper into two parts equally based on the\n",
    "sentence count and then see whether Rij appears (i)\n",
    "PF:Begin. in the beginning or (ii) PF:End. in the\n",
    "end of Pi\n",
    ". Importantly, if Rij appears multiple times\n",
    "in Pi\n",
    ", we consider the fraction of times it occurs in\n",
    "each part.\n",
    "\n",
    " (iii) PF:Mean. mean position of appearance, (iv)\n",
    "PF:Std. standard deviation of different appearances.\n",
    "These features are normalized by the total length\n",
    "(number of sentences) of Pi\n",
    ". , thus ranging from 0\n",
    "(indicating beginning of Pi) to 1 (indicating the end\n",
    "of Pi).\n",
    "\n",
    "5. Linguistic\n",
    "6. Miscellaneous\n",
    "\n",
    "MS:GCount. To answer\n",
    "whether a highly-cited paper has more academic influence on the citing paper than the one which is less\n",
    "cited, we measure the number of other papers (except Pi) citing Pj .\n",
    "(ii) MS:SelfC. To see the effect of self-citation, we\n",
    "check whether at least one author is common in both\n",
    "Pi and Pj .\n",
    "(iii) MG:Time. The fact that older papers are rarely\n",
    "cited, may not stipulate that these are less influential.\n",
    "Therefore, we measure the difference of the publication years of Pi and Pj .\n",
    "(iv) MG:CoCite. It measures the co-citation counts\n",
    "of Pi and Pj defined by |Ri∩Rj |\n",
    "|Ri∪Rj |\n",
    ", which in turn answers the significance of reference-based similarity\n",
    "driving the academic influence \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "year_regex = re.compile(r'((19[0-9]{2})|(20[0-9]{2}))[a-z]?')\n",
    "conversion_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citations(folder):\n",
    "    citation_list = {}\n",
    "    for file in os.listdir(folder) :\n",
    "        tree = ET.parse(folder+file)\n",
    "        root = tree.getroot()\n",
    "        id = file[:8]\n",
    "        for element in root.iterfind(\"algorithm\"):\n",
    "            if(element.attrib['name']==\"ParsCit\"):\n",
    "                citlist = element.getchildren()\n",
    "                cits = citlist[0].getchildren()\n",
    "                citations = []\n",
    "                for cit in cits:\n",
    "                    cit_dict = {}\n",
    "                    if(cit.attrib['valid']==\"true\"):\n",
    "                        try :\n",
    "                            title = cit.find('title').text.lower()\n",
    "                        except :\n",
    "                            title = cit.find('rawString').text.lower()\n",
    "                        cit_dict['title'] = title\n",
    "                        cit_dict['cit'] = cit\n",
    "                        citations.append(cit_dict)\n",
    "                \n",
    "                citation_list[id] = citations\n",
    "            \n",
    "    return citation_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(citations) :\n",
    "    dataset = {}\n",
    "    dataset1 = {}\n",
    "    count = 0\n",
    "    for key in citations.keys():\n",
    "        context_list = []\n",
    "        con_list = []\n",
    "        for cit in citations[key] :\n",
    "            dict1 = {}\n",
    "            dict1['paper_name'] = cit['title']\n",
    "            dict1['context'] = []\n",
    "            dict2 = {}\n",
    "            dict2['paper_name'] = cit['title']\n",
    "            dict2['sents'] = []\n",
    "            for context in cit['cit'].findall('contexts/context') :\n",
    "                text = context.text\n",
    "                sents = sent_tokenize(text)\n",
    "                citstr = context.get('citStr')\n",
    "                con = \"\"\n",
    "                for sent in sents :\n",
    "                    if(citstr in sent) :\n",
    "                        ind = sents.index(sent)\n",
    "#                         sent = sent.replace(citstr, \"this_citation\")\n",
    "                        con+= sents[ind-1] + \" \"\n",
    "                        con+= sent + \" \"\n",
    "                        dict2['sents'].append(sent)\n",
    "                        if(ind+1<len(sents)):\n",
    "                            con+= sents[ind+1] \n",
    "                        break\n",
    "                con = con.lower()\n",
    "                dict1['context'].append(text)\n",
    "            context_list.append(dict1)\n",
    "            con_list.append(dict2)\n",
    "        dataset[key] = context_list\n",
    "        dataset1[key] = con_list\n",
    "        count+=1\n",
    "        if(count%100==0) :\n",
    "            print(count)\n",
    "    return dataset, dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = get_citations(\"../xmls/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "contexts, citation_sents = get_contexts(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(contexts, \"contexts_info\")\n",
    "pickle_dump(citation_sents, \"citation_sents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1181"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper_name': 'on a measure of divergence between two statistical populations defined by their probability distributions.',\n",
       "  'sents': ['Our technical contribution in this paper is to show that in the case where the context profiles are multinomial distributions, the priors are Dirichlet, and the base similarity measure is the Bhattacharyya coefficient (this_citation), we can derive an analytical form for Eq.',\n",
       "   '2, even in the Dirichlet prior case.l In this study, we employ the Bhattacharyya coefficient (this_citation) (BC for short), which is defined as follows: BC(p1, p2) = The BC is also a similarity measure on probability distributions and is suitable for our purposes as we describe in the next section.',\n",
       "   'BC The Bhattacharyya coefficient (this_citation) between p(fk|w1) and p(fkIw2).']},\n",
       " {'paper_name': 'an empirical study of smoothing techniques for language modeling.',\n",
       "  'sents': ['The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (this_citation; Chen and Rosenfeld, 2000; Cortes and Vapnik, 1995).']},\n",
       " {'paper_name': 'a survey of smoothing techniques for me models.',\n",
       "  'sents': ['The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (Chen and Goodman, 1998; this_citation; Cortes and Vapnik, 1995).']},\n",
       " {'paper_name': 'support vector networks.',\n",
       "  'sents': ['The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (Chen and Goodman, 1998; Chen and Rosenfeld, 2000; this_citation).']},\n",
       " {'paper_name': 'edr electronic dictionary version 2.0 technical guide.',\n",
       "  'sents': ['We extracted such sets from a Japanese dictionary, EDR (V3.0) (this_citation), which contains concept hierarchies and the mapping between words and concepts.']},\n",
       " {'paper_name': 'similarity-based estimation of word cooccurrence probabilities.',\n",
       "  'sents': ['A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; this_citation; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999).',\n",
       "   '(5) K ri k=1 248 2.2 Bhattacharyya coefficient When the context profiles are probability distributions, we usually utilize the measures on probability distributions such as the Jensen-Shannon (JS) divergence to calculate similarities (this_citation; Dagan et al., 1997).',\n",
       "   'JS Jensen-Shannon divergence between p(fk|w1) and p(fk|w2) (this_citation; Dagan et al., 1999).']},\n",
       " {'paper_name': 'contextual word similarity and estimation from sparse data.',\n",
       "  'sents': ['A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; this_citation; Lin, 1998; Dagan et al., 1999).']},\n",
       " {'paper_name': 'similarity-based methods for word sense disambiguation.',\n",
       "  'sents': ['(5) K ri k=1 248 2.2 Bhattacharyya coefficient When the context profiles are probability distributions, we usually utilize the measures on probability distributions such as the Jensen-Shannon (JS) divergence to calculate similarities (Dagan et al., 1994; this_citation).']},\n",
       " {'paper_name': 'similarity-based models of word cooccurrence probabilities.',\n",
       "  'sents': ['A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; this_citation).',\n",
       "   'JS Jensen-Shannon divergence between p(fk|w1) and p(fk|w2) (Dagan et al., 1994; this_citation).']},\n",
       " {'paper_name': 'explorations in automatic thesaurus discovery.',\n",
       "  'sents': ['A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; this_citation; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999).']},\n",
       " {'paper_name': 'distributional structure.',\n",
       "  'sents': ['Many researchers have conducted studies based on the distributional hypothesis (this_citation), which states that words that occur in the same contexts tend to have similar meanings.']},\n",
       " {'paper_name': 'noun classification from predicate-argument structures.',\n",
       "  'sents': ['A number of semantic similarity measures have been proposed based on this hypothesis (this_citation; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999).']},\n",
       " {'paper_name': 'inducing gazetteers for named entity recognition by large-scale clustering of dependency relations.',\n",
       "  'sents': ['5.2 Collecting context profiles Dependency relations are used as context profiles as in this_citation and Kazama et al.',\n",
       "   '(2009) proposed using the Jensen-Shannon divergence between hidden class distributions, p(c|w1) and p(c|w2), which are obtained by using an EM-based clustering of dependency relations with a model p(wi, fk) = ∑ c p(wi|c)p(fk|c)p(c) (this_citation).',\n",
       "   'Although Cls-JS showed very good performance for Set C, note that the EM clustering is very time-consuming (this_citation), and it took about one week with 24 CPU cores to get one clustering result in our computing environment.']},\n",
       " {'paper_name': 'generating a large-scale analogy list using a probabilistic clustering based on noun-verb dependency profiles.',\n",
       "  'sents': []},\n",
       " {'paper_name': 'automatic retrieval and clustering of similar words.',\n",
       "  'sents': ['A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; this_citation; Dagan et al., 1999).']},\n",
       " {'paper_name': 'bayesian unsupervised word segmentation with nested pitman-yor language modeling.',\n",
       "  'sents': ['Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation (Teh, 2006; this_citation).',\n",
       "   'Recent studies have pointed out that the Bayesian framework derives state-of-the-art smoothing methods such as Kneser-Ney smoothing as a special case (Teh, 2006; this_citation).']},\n",
       " {'paper_name': 'database for evaluating extracted terms and tool for visualizing the terms.',\n",
       "  'sents': []},\n",
       " {'paper_name': 'discovering word senses from text.',\n",
       "  'sents': ['PMI-cos The cosine of the context profile vectors, where the k-th dimension is the pointwise mutual information (PMI) between wi and fk defined as: PMI(wi, fk) = log p(wi,fk) p(wi)p(fk) (this_citation; Pantel et al., 2009).3 Cls-JS Kazama et al.',\n",
       "   'Similar methods were used in this_citation, Kazama et al.']},\n",
       " {'paper_name': 'eric crestan, arkady borkovsky, anamaria popescu, and vishnu vyas.',\n",
       "  'sents': []},\n",
       " {'paper_name': 't. w. rauber, t. braun, and k. berns. 2008. probabilistic distance measures of the dirichlet and beta distributions. pattern recognition, 41:637–645.',\n",
       "  'sents': []},\n",
       " {'paper_name': 'tsubaki: an open search engine infrastructure for developing new information access.',\n",
       "  'sents': ['From a large corpus of Japanese Web documents (this_citation) (100 million T S(wi ∈ ans), i=1 AP = 1 N R i=1 250 documents), where each sentence has a dependency parse, we extracted noun-verb and nounnoun dependencies with relation types and then calculated their frequencies in the corpus.']},\n",
       " {'paper_name': 'a hierarchical bayesian language model based on pitman-yor processes.',\n",
       "  'sents': ['Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation (this_citation; Mochihashi et al., 2009).',\n",
       "   'Recent studies have pointed out that the Bayesian framework derives state-of-the-art smoothing methods such as Kneser-Ney smoothing as a special case (this_citation; Mochihashi et al., 2009).']},\n",
       " {'paper_name': 'a tool for constructing a synonym dictionary using context information.',\n",
       "  'sents': []}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_sents[list(citation_sents.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['P10-1026', 'P13-1111', 'W14-4407', 'D10-1003', 'P13-1171', 'J15-1005', 'N09-1036', 'P11-1130', 'P13-1029', 'D15-1136', 'P13-1087', 'N06-1047', 'Q14-1008', 'P91-1041', 'W10-2915', 'D15-1218', 'P15-1135', 'P00-1039', 'P13-1118', 'D12-1002', 'W15-4902', 'S15-1005', 'W06-2914', 'P15-1127', 'P13-1109', 'D12-1120', 'W13-3514', 'P14-1095', 'D12-1019', 'N13-1132', 'P02-1059', 'J12-1003', 'D12-1134', 'W14-4338', 'Q13-1009', 'P13-1059', 'P06-1009', 'N10-1014', 'P10-1107', 'N01-1011', 'P02-1065', 'P14-1136', 'P08-1048', 'D08-1034', 'D08-1090', 'D09-1092', 'P10-1125', 'D15-1089', 'D12-1051', 'E12-1008', 'E14-1075', 'D14-1123', 'D09-1036', 'S15-1017', 'P12-1072', 'P12-1007', 'P03-1011', 'P14-1012', 'W06-1649', 'W15-5618', 'N07-1011', 'D08-1069', 'D08-1113', 'D14-1008', 'P98-2234', 'P11-1149', 'P08-1049', 'P14-1015', 'D13-1191', 'D14-1038', 'P10-1004', 'P14-1033', 'D13-1014', 'D11-1030', 'P05-1004', 'D15-1045', 'P13-1013', 'P01-1041', 'J01-1002', 'P00-1011', 'D13-1009', 'P05-1015', 'D14-1137', 'D15-1040', 'E12-1060', 'N09-1029', 'P15-1001', 'P14-1115', 'P13-1107', 'D14-1204', 'P09-1049', 'D14-1189', 'W14-4325', 'D13-1067', 'D14-1171', 'D14-1045', 'D12-1029', 'P14-1044', 'D13-1047', 'D13-1103', 'E12-1034', 'D15-1156', 'N13-1015', 'J90-3003', 'D11-1083', 'D13-1081', 'P14-1008', 'P96-1025', 'N01-1006', 'D12-1075', 'P00-1061', 'D14-1043', 'P05-1002', 'D14-1014', 'N10-1134', 'P15-1017', 'D11-1005', 'P14-1036', 'D15-1177', 'P02-1054', 'P08-1099', 'D15-1016', 'D10-1102', 'W02-1006', 'P09-1050', 'P12-1011', 'J14-4004', 'E09-1005', 'D15-1296', 'N12-1023', 'P06-1141', 'J10-3005', 'P10-1060', 'D13-1004', 'D15-1061', 'J12-4002', 'D11-1014', 'N13-1107', 'D14-1219', 'P15-1002', 'D14-1093', 'E09-1013', 'P15-1089', 'P96-1010', 'D13-1131', 'N09-1022', 'D15-1037', 'D15-1020', 'N03-1027', 'E99-1023', 'P10-1134', 'D13-1044', 'E14-1073', 'W06-1657', 'D09-1071', 'J94-4002', 'P03-1001', 'D11-1142', 'D14-1035', 'P13-1105', 'D11-1051', 'D13-1164', 'W11-0131', 'D14-1017', 'P13-1086', 'D08-1096', 'Q14-1021', 'E14-1035', 'N10-1076', 'D15-1101', 'N04-1029', 'E12-1038', 'D09-1137', 'P11-1152', 'D14-1119', 'D12-1044', 'D13-1134', 'W15-1824', 'D13-1117', 'P15-1138', 'P10-1074', 'W10-2926', 'P10-1013', 'J05-4002', 'Q14-1003', 'P10-1011', 'W06-1631', 'D12-1006', 'W08-2104', 'N13-1011', 'P10-1065', 'D11-1031', 'W03-0402', 'P10-1090', 'E09-1057', 'W05-0610', 'P05-1035', 'P10-1159', 'P14-1048', 'D13-1018', 'D08-1005', 'N12-1045', 'J94-4001', 'D09-1115', 'D14-1222', 'N06-1007', 'E06-1026', 'K15-1004', 'P98-1090', 'W06-2906', 'E14-1078', 'Q15-1011', 'D14-1021', 'P14-1138', 'N12-1017', 'E14-1039', 'N03-1002', 'D11-1108', 'N06-1031', 'P12-1045', 'N13-1003', 'Q15-1005', 'D13-1105', 'D15-1011', 'P14-1108', 'D14-1063', 'P09-1066', 'P15-1031', 'P15-1102', 'N10-1015', 'E03-1053', 'J95-4002', 'D08-1099', 'P06-1054', 'P92-1006', 'N12-1009', 'D12-1042', 'D15-1159', 'P06-1098', 'P10-1020', 'N13-1135', 'P04-1056', 'D08-1022', 'D15-1310', 'P12-1075', 'A00-1019', 'D13-1089', 'W14-4343', 'D15-1241', 'P00-1022', 'D14-1218', 'J07-1003', 'P98-2237', 'D13-1034', 'A00-2025', 'P13-1010', 'D15-1015', 'Q13-1024', 'D13-1144', 'P06-1113', 'P93-1035', 'E12-1062', 'D14-1225', 'D09-1001', 'D14-1085', 'J08-1001', 'P06-1038', 'K15-1026', 'D15-1025', 'D14-1087', 'P96-1018', 'N13-1055', 'D14-1090', 'P13-1104', 'E14-1002', 'E06-1031', 'K15-1002', 'P02-1002', 'N10-1017', 'P09-1013', 'P14-1005', 'N06-1024', 'D09-1124', 'D12-1012', 'P12-1038', 'D13-1023', 'D13-1033', 'D15-1005', 'J03-3005', 'D09-1122', 'P04-1030', 'K15-1020', 'D11-1091', 'Q15-1004', 'D14-1023', 'A97-1013', 'P14-1114', 'D13-1050', 'N10-1129', 'S14-1003', 'D11-1038', 'W15-4612', 'D14-1024', 'N12-1093', 'Q13-1010', 'E12-1075', 'D13-1040', 'D14-1100', 'D13-1179', 'D12-1100', 'P13-1125', 'J13-4004', 'D13-1156', 'P13-1157', 'P04-1027', 'D12-1092', 'P12-1060', 'D11-1077', 'N13-1009', 'D12-1107', 'D15-1085', 'P09-1005', 'D15-1238', 'D15-1047', 'J99-1003', 'N10-1128', 'P85-1014', 'P11-1042', 'D14-1011', 'P14-1003', 'P15-1043', 'P12-1052', 'P12-1069', 'D14-1082', 'P14-1083', 'P12-1046', 'P10-1129', 'W15-4911', 'N10-1131', 'D14-1166', 'P14-1121', 'P13-1113', 'N09-1019', 'P08-1087', 'D14-1096', 'N09-1073', 'E03-1035', 'P96-1041', 'E12-1009', 'D15-1084', 'D13-1026', 'P15-1005', 'D11-1081', 'P06-1001', 'P96-1032', 'E09-1027', 'D12-1089', 'P14-1050', 'D13-1002', 'J11-4008', 'D14-1216', 'P08-1074', 'P11-1007', 'D14-1142', 'D09-1009', 'P09-1037', 'J02-3001', 'J05-1003', 'D10-1099', 'D12-1064', 'E14-1037', 'P13-1016', 'E09-1044', 'P14-1084', 'D10-1081', 'N13-1071', 'P14-1093', 'D12-1133', 'P11-1117', 'E09-1048', 'D09-1151', 'P11-1016', 'A00-1040', 'P98-2159', 'P15-1004', 'J09-1005', 'D15-1102', 'P06-1105', 'P11-1110', 'P13-1065', 'E12-1014', 'E12-1022', 'Q13-1001', 'D14-1169', 'P06-1104', 'N13-1111', 'P13-1078', 'P06-1043', 'P13-1097', 'P13-1123', 'P14-1057', 'D14-1002', 'P14-1073', 'D15-1017', 'D14-1028', 'D13-1172', 'N03-1026', 'N13-1001', 'W09-3910', 'P09-1036', 'E06-1033', 'P11-1084', 'P00-1063', 'P09-1068', 'D12-1111', 'D08-1062', 'D15-1139', 'P12-1106', 'D14-1200', 'D11-1049', 'D15-1110', 'P15-1079', 'P11-1090', 'P09-1107', 'P04-1020', 'P05-1067', 'E09-1073', 'N13-1060', 'P13-1042', 'W10-2924', 'D15-1039', 'E12-1067', 'P88-1005', 'P10-1103', 'P03-1025', 'P09-1022', 'D12-1056', 'Q14-1030', 'D08-1109', 'D10-1113', 'J14-1003', 'D14-1190', 'P04-1076', 'P13-1069', 'N12-1051', 'W15-1818', 'P06-1004', 'D13-1201', 'P13-1089', 'W04-3212', 'D12-1073', 'E06-1015', 'P11-1075', 'P13-1144', 'Q14-1034', 'P06-1017', 'W12-1506', 'W97-0317', 'P13-1150', 'D12-1117', 'P14-1128', 'P15-1172', 'N12-1029', 'P99-1016', 'D09-1010', 'P11-1048', 'N09-1002', 'P15-1034', 'Q14-1005', 'E14-1044', 'W97-0301', 'W06-1668', 'P09-1109', 'P08-1085', 'P15-1167', 'D15-1041', 'D15-1138', 'D12-1084', 'D11-1120', 'D13-1030', 'P13-1147', 'W03-1026', 'P10-1038', 'D15-1004', 'P15-1078', 'W06-1615', 'D14-1163', 'J00-4001', 'P06-1095', 'D13-1123', 'W15-4903', 'P00-1006', 'D12-1127', 'W09-1116', 'W99-0614', 'Q15-1002', 'D10-1021', 'P14-1126', 'J00-4003', 'N04-1033', 'A94-1026', 'P90-1036', 'P10-1067', 'D13-1135', 'D14-1153', 'D09-1099', 'S15-1010', 'D14-1029', 'D14-1088', 'P14-1046', 'D12-1023', 'P11-1116', 'E14-1046', 'W11-0319', 'K15-1027', 'P11-1053', 'P10-1027', 'P10-1009', 'P10-1046', 'P14-1004', 'W06-1671', 'D12-1069', 'W09-3722', 'D11-1080', 'P11-1163', 'D11-1110', 'Q14-1023', 'E06-1046', 'W06-1603', 'P08-1028', 'P15-1143', 'D09-1021', 'P04-1015', 'D09-1088', 'D14-1157', 'P04-1042', 'P06-1087', 'D11-1129', 'D11-1033', 'W04-3206', 'W06-1665', 'P05-1066', 'W13-4008', 'P15-1074', 'N13-1006', 'E14-1062', 'E09-1096', 'P10-1001', 'P11-1150', 'D13-1001', 'N07-1031', 'D12-1114', 'P08-1108', 'D15-1137', 'D10-1123', 'D11-1019', 'W13-3504', 'E14-1070', 'E09-1076', 'D12-1024', 'P14-1116', 'D12-1032', 'N03-1029', 'E12-1032', 'W02-1028', 'W09-3928', 'D13-1101', 'N07-1051', 'Q14-1019', 'D09-1105', 'E12-1076', 'D15-1142', 'D14-1039', 'D13-1043', 'P11-1092', 'D12-1007', 'D10-1050', 'D13-1065', 'P15-1130', 'W00-1302', 'P14-1070', 'D11-1026', 'P10-1042', 'K15-1022', 'D14-1022', 'E09-1087', 'W13-3512', 'P11-1123', 'D11-1082', 'D10-1125', 'P14-1007', 'P14-1071', 'N13-1106', 'D14-1033', 'P12-1107', 'P09-1054', 'N06-1057', 'D14-1019', 'W11-0124', 'P12-1057', 'P14-1065', 'D11-1131', 'P10-1073', 'E12-1006', 'D13-1054', 'W11-0311', 'D09-1042', 'D13-1013', 'E14-1032', 'D11-1060', 'P06-1088', 'E09-1088', 'J04-1001', 'N07-1055', 'D14-1092', 'Q13-1030', 'P99-1067', 'D15-1038', 'P12-1098', 'W14-1611', 'P04-1078', 'N13-1122', 'P97-1013', 'P15-1050', 'Q13-1004', 'J06-4003', 'W03-1006', 'D12-1047', 'W15-2205', 'W11-0303', 'D13-1069', 'N06-1033', 'D15-1279', 'P14-1006', 'D10-1043', 'P10-1034', 'D10-1048', 'P09-1039', 'N07-1006', 'P02-1011', 'D15-1221', 'W11-4501', 'P13-1034', 'W02-1022', 'P02-1025', 'J88-1003', 'P15-1104', 'D15-1205', 'P06-1059', 'E03-1002', 'D11-1013', 'P11-1061', 'D14-1080', 'D13-1077', 'P13-1001', 'D14-1016', 'K15-1005', 'P09-1105', 'P12-1013', 'P02-1014', 'N12-1031', 'P13-1026', 'N03-1032', 'W04-2402', 'P13-1007', 'P05-1053', 'D13-1169', 'P12-1090', 'P05-1025', 'E09-1097', 'P11-1051', 'P10-1153', 'P06-1092', 'P08-1088', 'P10-1130', 'D12-1077', 'E97-1041', 'P13-1022', 'D14-1220', 'P10-1138', 'P10-1040', 'D15-1035', 'D15-1065', 'E14-1008', 'P15-1136', 'K15-1009', 'D13-1016', 'D11-1124', 'D09-1127', 'D09-1125', 'J01-3002', 'P15-1082', 'P09-1047', 'P03-1013', 'J05-4005', 'P12-1035', 'P14-1035', 'D15-1148', 'J08-2002', 'N06-1021', 'N13-1038', 'D11-1089', 'P09-1021', 'P11-1054', 'P12-1014', 'W03-1012', 'P15-1145', 'N10-1011', 'E09-1084', 'D12-1095', 'E14-1050', 'W01-0511', 'J14-2003', 'W14-6803', 'N03-1030', 'D13-1173', 'N06-1006', 'P11-1087', 'W09-1107', 'D15-1149', 'K15-1008', 'W15-2201', 'D15-1068', 'P05-1049', 'N10-1119', 'N12-1008', 'P11-1109', 'E12-1023', 'J04-1004', 'D13-1007', 'P13-1122', 'J98-4002', 'D12-1063', 'D08-1031', 'N13-1137', 'P15-1040', 'D11-1023', 'P96-1043', 'D14-1193', 'N10-1069', 'P98-2152', 'P14-1069', 'D10-1049', 'D11-1064', 'E14-1014', 'P15-1007', 'W06-1606', 'P15-1071', 'N12-1047', 'W13-4005', 'J08-1003', 'W04-3230', 'P04-1088', 'D10-1061', 'P14-1137', 'D14-1044', 'P15-1110', 'P09-1019', 'P09-1008', 'D08-1023', 'D12-1016', 'D13-1120', 'W06-1634', 'N07-1026', 'E14-1016', 'D12-1015', 'Q14-1027', 'P13-1014', 'D12-1125', 'P14-1072', 'E14-1013', 'D15-1165', 'P15-1108', 'D09-1043', 'D09-1070', 'D10-1019', 'P08-1078', 'P00-1070', 'D12-1110', 'W14-4305', 'W10-2905', 'N12-1011', 'D13-1130', 'N03-1008', 'N13-1109', 'D13-1193', 'N07-1061', 'P10-1083', 'N10-1021', 'W14-1609', 'N07-1072', 'W10-4216', 'P09-1028', 'D13-1046', 'Q14-1020', 'P03-1036', 'P03-1066', 'P12-1051', 'J01-4003', 'P12-1032', 'N07-1022', 'W15-4608', 'P15-1009', 'D09-1013', 'P14-1130', 'P09-1001', 'D12-1037', 'D09-1094', 'P12-1054', 'D12-1028', 'P13-1049', 'E91-1012', 'P08-1112', 'D09-1075', 'P03-1035', 'D13-1015', 'P05-1020', 'N12-1052', 'D15-1012', 'W12-1643', 'P10-1116', 'D09-1087', 'P12-1061', 'E14-1051', 'D13-1143', 'D13-1025', 'W06-2912', 'P15-1053', 'P14-1022', 'N10-1125', 'D11-1012', 'P15-1115', 'N09-1064', 'W05-0604', 'P06-1101', 'W06-1669', 'E93-1036', 'D15-1286', 'P03-1064', 'P02-1017', 'P13-1173', 'W00-1317', 'P15-1171', 'N10-1115', 'D11-1018', 'D13-1183', 'W11-0328', 'D10-1079', 'D15-1166', 'D10-1002', 'E09-1054', 'D15-1010', 'E12-1029', 'P14-1009', 'S15-1020', 'P08-1064', 'E97-1055', 'D11-1122', 'P14-1087', 'D12-1085', 'N07-1017', 'P10-1018', 'D13-1032', 'N10-1122', 'D14-1111', 'W01-0506', 'E12-1072', 'D12-1138', 'D09-1140', 'D10-1031', 'D10-1111', 'D09-1111', 'P12-1078', 'D14-1124', 'D09-1015', 'W13-4034', 'W14-4404', 'P05-1059', 'N12-1048', 'N04-1023', 'D15-1154', 'W03-0416', 'D13-1180', 'D11-1141', 'E14-1003', 'E14-1061', 'D12-1102', 'W13-3508', 'D09-1037', 'P98-1103', 'W00-1305', 'D11-1058', 'D15-1146', 'N12-1090', 'D13-1056', 'P04-1048', 'N13-1117', 'P00-1032', 'D09-1090', 'D15-1058', 'P10-1044', 'E06-1038', 'P12-1081', 'P08-1113', 'W99-0605', 'P10-1048', 'N07-1047', 'A92-1018', 'N13-1007', 'N09-1009', 'D11-1063', 'P99-1048', 'D15-1289', 'N13-1123', 'D13-1116', 'P13-1002', 'S14-1010', 'P15-1132', 'P12-1101', 'P08-1053', 'N06-1052', 'N12-1018', 'D14-1052', 'P01-1010', 'W11-0125', 'P06-1012', 'J01-2002', 'P09-1016', 'E14-1064', 'P03-1026', 'P10-1017', 'E06-1005', 'D12-1058', 'W04-2403', 'J15-2004', 'P03-1003', 'W06-1659', 'A00-2040', 'N13-1090', 'D14-1004', 'K15-1024', 'E12-1013', 'P06-1116', 'E09-1035', 'D12-1017', 'P96-1006', 'D13-1041', 'N13-1023', 'P12-1022', 'W03-1015', 'P94-1025', 'P12-1084', 'D08-1021', 'D15-1019', 'P15-1041', 'D14-1037', 'P11-1089', 'P11-1144', 'N09-1010', 'W06-1640', 'D12-1022', 'P06-1144', 'P06-1072', 'D14-1205', 'P13-1130', 'P08-1027', 'P09-1070', 'D15-1031', 'D14-1194', 'D10-1028', 'N09-1045', 'Q15-1010', 'W04-3248', 'N13-1080', 'P15-1015', 'W15-4924', 'D13-1136', 'P15-1113', 'P06-1102', 'W06-1607', 'Q13-1023', 'D15-1028', 'S15-1003', 'D09-1052', 'W09-1106', 'N06-1015', 'P03-1046', 'N10-1005', 'D14-1003', 'D13-1022', 'W04-2408', 'J81-3002', 'E06-1021', 'P15-1061', 'E99-1026', 'D14-1192', 'P05-1024', 'D12-1013', 'P10-1111', 'D15-1027', 'P01-1065', 'E97-1063', 'J10-4006', 'D09-1074', 'P14-1011', 'D15-1135', 'P13-1017', 'D13-1083', 'K15-1006', 'D09-1108', 'P13-1128', 'N10-1016', 'D14-1139', 'E12-1025', 'D12-1001', 'N13-1102', 'W11-0307', 'D09-1159', 'Q14-1031', 'J15-3005', 'J98-2002', 'N07-1044', 'D15-1053', 'W03-1010', 'D11-1102', 'P12-1049', 'E14-1006', 'J10-4007', 'P04-1013', 'P00-1019', 'P12-1055', 'W15-2138', 'P10-1105', 'P14-1140', 'P11-1038', 'P15-1042', 'P13-1068', 'D12-1131', 'D13-1029', 'E06-1039', 'P14-1032', 'P08-1023', 'P03-1057', 'P11-1008', 'D14-1135', 'E12-1073', 'P15-1028', 'W11-0315', 'P14-1129', 'P08-1047', 'P05-1077', 'W01-0517', 'P13-1052', 'D15-1030', 'P08-1043', 'N13-1074', 'P09-1006', 'P03-1062', 'J09-4009', 'D15-1042', 'D08-1048', 'W15-4627', 'W12-1639', 'E09-1038', 'K15-1003', 'P04-1014', 'Q14-1028', 'W06-1641', 'S14-1002', 'D14-1168', 'D12-1088', 'N10-1081', 'P14-1066', 'N07-1005', 'P03-1042', 'P14-1034', 'D14-1116', 'P13-1161', 'N10-1085', 'P14-1019', 'D15-1164', 'P10-1033', 'D15-1050', 'D14-1196', 'N12-1004', 'P14-1013', 'P10-1031', 'P14-1062', 'W15-4626', 'P02-1057', 'P10-1080', 'D12-1060', 'P05-1023', 'D14-1032', 'P04-1016', 'D08-1066', 'P08-1004', 'W10-2921', 'E06-1044', 'N07-1045', 'D15-1189', 'P06-1047', 'P10-1145', 'E09-1046', 'N12-1013', 'P04-1007', 'W03-1002', 'D11-1022', 'D08-1037', 'J96-4004', 'D12-1122', 'J14-2007', 'P08-1040', 'E14-1021', 'P14-1002', 'D14-1012', 'W04-3226', 'P15-1064', 'N06-1036', 'P11-1074', 'D13-1028', 'D14-1075', 'P13-1137', 'P15-1021', 'W13-3510', 'D12-1055', 'P13-1088', 'P06-1136', 'P98-2204', 'P11-1063', 'J06-2003', 'N09-1050', 'Q13-1022', 'E09-1068', 'D11-1099', 'P08-1097', 'J01-2004', 'P99-1046', 'D14-1009', 'P15-1173', 'D09-1012', 'D14-1150', 'N09-1023', 'D12-1039', 'P15-1154', 'P14-1107', 'P95-1016', 'D15-1183', 'P15-1152', 'N03-1025', 'P04-1028', 'D12-1034', 'E14-1042', 'Q15-1007', 'K15-1021', 'P06-1085', 'P08-1093', 'N13-1075', 'N13-1030', 'P95-1036', 'N13-1138', 'N06-1014', 'E06-1014', 'D12-1046', 'D15-1227', 'W11-0301', 'P08-1106', 'P14-1090', 'P06-1109', 'P06-1073', 'D10-1054', 'D14-1175', 'D14-1213', 'S15-1028', 'D12-1090', 'D15-1081', 'A00-2009', 'P15-1065', 'P15-1144', 'S15-1002', 'N09-1062'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(contexts, open(\"pickles/reduced_context.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(citation_sents, open(\"pickles/citation_sentences.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_diff() :\n",
    "    year_diff = pickle.load(open(\"../pickles_data/year_diff.pkl\", \"rb\"))\n",
    "    pickle.dump(year_diff, open(\"pickles/year_diff.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location() :\n",
    "    loc_feat = pickle.load(open(\"../pickles_data/location_feature.pkl\", \"rb\"))\n",
    "    dataset = {}\n",
    "    for key in loc_feat :\n",
    "        papers = loc_feat[key]\n",
    "        pap_list = []\n",
    "        for pap in papers :\n",
    "            dict1 = {}\n",
    "            dict1['paper_name'] = pap['paper_name']\n",
    "            locs = pap['location_feature']\n",
    "            dict1['whole'] = sum(locs)\n",
    "            dict1['intro'] = locs[3]\n",
    "            dict1['relwork'] = locs[-1]\n",
    "            dict1['rest'] = sum(locs) - locs[3] -locs[1]\n",
    "            pap_list.append(dict1)\n",
    "        dataset[key] = pap_list\n",
    "        \n",
    "    return dataset         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = location()\n",
    "year_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(loc, open(\"pickles/alf_location.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_feature(section_labels, citations):\n",
    "    dataset = {}\n",
    "    for key in citations.keys() :\n",
    "        feat_list = []\n",
    "        for cit in citations[key] :\n",
    "            data1 = {}\n",
    "            data1['paper_name'] = cit['title']\n",
    "            citStrs = list(set([context.get('citStr').lower() for context in cit['cit'].findall('contexts/context')]))\n",
    "            section_feature = {'experiment':0, 'conclusion':0, 'other_sections':0, 'related_work':0, 'method':0, 'introduction':0, 'evaluation':0}\n",
    "            for section in section_labels[key]:\n",
    "                if section=='overall':\n",
    "                    continue\n",
    "                for subsection in section_labels[key][section]:\n",
    "                    text = section_labels[key][section][subsection]\n",
    "                    for citStr in citStrs:\n",
    "                        count = text.count(citStr)\n",
    "                        section_feature[section]+=count\n",
    "                        text = text.replace(citStr, '')\n",
    "            data1['section_feature'] = section_feature\n",
    "            feat_list.append(data1)\n",
    "        dataset[key] = feat_list \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sect_feat = get_section_feature(pickle.load(open(\"pickles/en_section_labels.pkl\", \"rb\")),citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sect_feat, open(\"pickles/en_section_feature.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for all citation strings\n",
    "    global_citStr = []\n",
    "    for key in citations :\n",
    "        papers = citations[key]\n",
    "        for paper in papers :\n",
    "            global_citStr.extend(list(set([context.get('citStr').lower() for context in paper['cit'].findall('contexts/context')])))\n",
    "    global_citStr = list(set(global_citStr))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_feat(citations) :\n",
    "    section_labels = pickle.load(open(\"../pickles_data/section_labels.pkl\", \"rb\"))\n",
    "    dataset = {}\n",
    "    count = 0\n",
    "    ct = 0\n",
    "    ct2 = 0\n",
    "    for key in citations :\n",
    "        paper = section_labels[key]['overall']\n",
    "        first_half = paper[:len(paper)//2]\n",
    "        second_half = paper[len(paper)//2:]\n",
    "        first_half = first_half.lower()\n",
    "        second_half = second_half.lower()\n",
    "        papers = citations[key] \n",
    "        data_list = []\n",
    "        for pap in papers :\n",
    "            data = {}\n",
    "            data['paper_name'] = pap['title']\n",
    "            cit = pap['cit']\n",
    "            citStrs = list(set([context.get('citStr').lower() for context in pap['cit'].findall('contexts/context')]))\n",
    "            upper = 0\n",
    "            lower = 0\n",
    "            for citstr in citStrs :\n",
    "                upper = first_half.count(citstr)\n",
    "                lower = second_half.count(citstr)\n",
    "                \n",
    "            if(upper+lower != 0) :\n",
    "                data['position_feat'] = []\n",
    "                data['position_feat'].append(upper/(upper+lower))\n",
    "                data['position_feat'].append(lower/(upper+lower))\n",
    "                sentences = sent_tokenize(paper)\n",
    "                indices = []\n",
    "                for i in range(len(sentences)) :\n",
    "                    for citstr in citStrs :\n",
    "                        if(citstr in sentences[i]) :\n",
    "                            indices.append(i)\n",
    "\n",
    "                if(len(indices)==0) :\n",
    "                    data['position_feat'].append(0)\n",
    "                    data['position_feat'].append(0)\n",
    "                    ct2+=1\n",
    "                else :\n",
    "                    data['position_feat'].append(sum(indices)/len(indices))\n",
    "                    if(len(indices)>2) :\n",
    "                        data['position_feat'].append(stdev(indices))\n",
    "                    else :\n",
    "                        data['position_feat'].append(0)\n",
    "                data_list.append(data)\n",
    "            else :\n",
    "                data['position_feat'] = [0,0,0,0]\n",
    "                ct+=1\n",
    "                \n",
    "        dataset[key] = data_list\n",
    "        count+=1\n",
    "        if(count%1000==0) :\n",
    "            print(count)\n",
    "\n",
    "    print(ct)\n",
    "    print(ct2)\n",
    "    return dataset       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "4344\n",
      "1980\n"
     ]
    }
   ],
   "source": [
    "pos_feat = position_feat(citations)\n",
    "pickle_dump(pos_feat, \"alf_pos_feat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rel = \"pivotal, comparable, innovative, relevant, relevantly, inspiring, related, relatedly, similar, similarly, applicable, appropriate,\n",
    "pertinent, influential, influenced, original, originally, useful, suggested, interesting, inspired, likewise\n",
    "recent, recently, latest, later, late, latest, up-to-date, continuing, continued, upcoming, expected, update, renewed, extended\"\n",
    "Rec = [subsequent, subsequently, initial, initially, sudden, current, currently, future, unexpected, previous, previously, old,\n",
    "ongoing, imminent, anticipated, unprecedented, proposed, startling, preliminary, ensuing, repeated, reported, new, earlier,\n",
    "earliest, early, existing, further, revised, improved]\n",
    "Ext = [greatly, awfully, drastically, intensely, acutely, almighty, exceptionally, excessively, exceedingly, tremendously, importantly\n",
    "significantly, notably, outstandingly]\n",
    "Comp = [easy, easier, easiest, vague, vaguer, vaguest, weak, weaker, weakest, strong, stronger, strongest, bogus, unclear]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "28313\n"
     ]
    }
   ],
   "source": [
    "rel = []\n",
    "rec = []\n",
    "ext = []\n",
    "comp = []\n",
    "rels = \"pivotal, comparable, innovative, relevant, relevantly, inspiring, related, relatedly, similar, similarly, applicable, appropriate, pertinent, influential, influenced, original, originally, useful, suggested, interesting, inspired, likewise, recent, recently, latest, later, late, latest, up-to-date, continuing, continued, upcoming, expected, update, renewed, extended\"\n",
    "recs = \"subsequent, subsequently, initial, initially, sudden, current, currently, future, unexpected, previous, previously, old, ongoing, imminent, anticipated, unprecedented, proposed, startling, preliminary, ensuing, repeated, reported, new, earlier, earliest, early, existing, further, revised, improved\"\n",
    "exts = \"greatly, awfully, drastically, intensely, acutely, almighty, exceptionally, excessively, exceedingly, tremendously, importantly, significantly, notably, outstandingly\"\n",
    "comps = \"easy, easier, easiest, vague, vaguer, vaguest, weak, weaker, weakest, strong, stronger, strongest, bogus, unclear\"\n",
    "rel = rels.split(\", \")\n",
    "rec = recs.split(\", \")\n",
    "ext = exts.split(\", \")\n",
    "comp = comps.split(\", \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_feat(citations, rel, rec,ext,comp) :\n",
    "    global_citStr = []\n",
    "    for key in citations :\n",
    "        papers = citations[key]\n",
    "        for paper in papers :\n",
    "            global_citStr.extend(list(set([context.get('citStr').lower() for context in paper['cit'].findall('contexts/context')])))\n",
    "    global_citStr = list(set(global_citStr))\n",
    "    \n",
    "    dataset = {}\n",
    "    count=0\n",
    "    for key in citations :\n",
    "        papers = citations[key]\n",
    "        data = []\n",
    "        for paper in papers :\n",
    "            dict1 = {}\n",
    "            dict1['paper_name'] = paper['title']\n",
    "            dict1['context_feat'] = []\n",
    "            citStrs = list(set([context.get('citStr').lower() for context in paper['cit'].findall('contexts/context')]))\n",
    "            other_cits = list(set(global_citStr)-set(citStrs))\n",
    "            count_pop = 0\n",
    "            alone = 0\n",
    "            first = 1\n",
    "            relevant = 0\n",
    "            recent = 0\n",
    "            extreme = 0\n",
    "            compare = 0\n",
    "            for context in paper['cit'].findall('contexts/context') :\n",
    "                con_text = context.text.lower()\n",
    "                other_con = []\n",
    "                for other in other_cits :\n",
    "                    if(other in con_text) :\n",
    "                        other_con.append(other)\n",
    "                if(len(other_con)>0) :\n",
    "                    alone = 1\n",
    "\n",
    "                other_ind = []\n",
    "                for other in other_con :\n",
    "                    other_ind.append(con_text.find(other))\n",
    "\n",
    "                con_ind = con_text.find(citStrs[0])\n",
    "                for i in range(1, len(citStrs)) :\n",
    "                    ind = con_text.find(citStrs[i])\n",
    "                    if(ind<con_ind) :\n",
    "                        con_ind = ind\n",
    "\n",
    "                for ind in other_ind :\n",
    "                    if(ind<con_ind) :\n",
    "                        first = 0\n",
    "                        break\n",
    "                \n",
    "                for word in rel :\n",
    "                    if(word in con_text) :\n",
    "                        relevant = 1\n",
    "                        break\n",
    "                        \n",
    "                for word in rec :\n",
    "                    if(word in con_text) :\n",
    "                        recent = 1\n",
    "                        break\n",
    "                        \n",
    "                for word in ext :\n",
    "                    if(word in con_text) :\n",
    "                        extreme = 1\n",
    "                        break\n",
    "                    \n",
    "                for word in comp :\n",
    "                    if(word in con_text) :\n",
    "                        compare = 1\n",
    "                        break\n",
    "                \n",
    "            dict1['context_feat'] = []\n",
    "            dict1['context_feat'].append(alone)\n",
    "            dict1['context_feat'].append(first)\n",
    "            dict1['context_feat'].append(relevant)\n",
    "            dict1['context_feat'].append(recent)\n",
    "            dict1['context_feat'].append(extreme)\n",
    "            dict1['context_feat'].append(compare)\n",
    "            \n",
    "            \n",
    "            data.append(dict1)\n",
    "        dataset[key] = data\n",
    "        count+=1\n",
    "        if(count%100==0) :\n",
    "            print(count)\n",
    "        \n",
    "        \n",
    "    return dataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "confeat = context_feat(citations, rel, rec, ext, comp)\n",
    "pickle_dump(confeat, \"alf_context_feat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
