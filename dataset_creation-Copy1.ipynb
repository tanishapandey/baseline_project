{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = pickle.load(open(\"pickles_data/abstracts_total.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_feat = pickle.load(open(\"pickles/author_level.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper_name': 'scalable training of l1-regularized log-linear models.',\n",
       "  'arnet_id': '53e99f2eb7602d97027ff009',\n",
       "  'hindex': 40.5,\n",
       "  'productivity': 371.5},\n",
       " {'paper_name': 'joint language and translation modeling with recurrent neural networks.',\n",
       "  'arnet_id': '53e9afa6b7602d97039f69a4',\n",
       "  'hindex': 11.5,\n",
       "  'productivity': 42.75},\n",
       " {'paper_name': 'learning deep architectures for ai. fundamental trends',\n",
       "  'arnet_id': '53e9afbab7602d9703a074c5',\n",
       "  'hindex': 125.0,\n",
       "  'productivity': 632.0},\n",
       " {'paper_name': 'a neural probabilistic language model.',\n",
       "  'arnet_id': '53e9ab37b7602d97034c4455',\n",
       "  'hindex': 40.75,\n",
       "  'productivity': 177.5},\n",
       " {'paper_name': 'latent dirichlet allocation.',\n",
       "  'arnet_id': '56d85479dabfae2eee1db078',\n",
       "  'hindex': 0,\n",
       "  'productivity': 0},\n",
       " {'paper_name': 'natural language processing (almost) from scratch.',\n",
       "  'arnet_id': '53e99beab7602d970249335e',\n",
       "  'hindex': 35.166666666666664,\n",
       "  'productivity': 79.16666666666667},\n",
       " {'paper_name': 'indexing by latent semantic analysis.',\n",
       "  'arnet_id': '53e9ae97b7602d97038b9f73',\n",
       "  'hindex': 8.0,\n",
       "  'productivity': 21.4},\n",
       " {'paper_name': 'why generative phrase models underperform surface heuristics.',\n",
       "  'arnet_id': '53e9a6d8b7602d97030106ed',\n",
       "  'hindex': 23.5,\n",
       "  'productivity': 63.0},\n",
       " {'paper_name': 'scalable stacking and learning for building deep architectures.',\n",
       "  'arnet_id': '53e99ef4b7602d97027bee47',\n",
       "  'hindex': 39.666666666666664,\n",
       "  'productivity': 117.0},\n",
       " {'paper_name': 'principle component ieural ietworks: theory and applications.',\n",
       "  'arnet_id': '53e9ba3fb7602d970464dab8',\n",
       "  'hindex': 20.5,\n",
       "  'productivity': 164.5},\n",
       " {'paper_name': 'automatic cross-language retrieval using latent semantic indexing.',\n",
       "  'arnet_id': '56d8b3f4dabfae2eeef8d042',\n",
       "  'hindex': 0,\n",
       "  'productivity': 0},\n",
       " {'paper_name': 'posterior regularization for structured latent variable models.',\n",
       "  'arnet_id': '53e9ad5db7602d9703746035',\n",
       "  'hindex': 23.0,\n",
       "  'productivity': 45.75},\n",
       " {'paper_name': 'training mrf-based translation models using gradient ascent.',\n",
       "  'arnet_id': '53e9a8bdb7602d970320b858',\n",
       "  'hindex': 57.5,\n",
       "  'productivity': 418.0},\n",
       " {'paper_name': 'clickthrough-based latent semantic models for web search.',\n",
       "  'arnet_id': '53e9abfdb7602d97035bb87a',\n",
       "  'hindex': 48.333333333333336,\n",
       "  'productivity': 302.6666666666667},\n",
       " {'paper_name': 'maximum expected bleu training of phrase and lexicon translation models.',\n",
       "  'arnet_id': '53e9b2b1b7602d9703d5827b',\n",
       "  'hindex': 22.5,\n",
       "  'productivity': 54.0},\n",
       " {'paper_name': 'discovering binary codes for documents by learning deep generative models. topics in cognitive science,',\n",
       "  'arnet_id': '53e9bc0fb7602d9704872dda',\n",
       "  'hindex': 106.0,\n",
       "  'productivity': 310.5},\n",
       " {'paper_name': 'deep neural networks for acoustic modeling in speech recognition.',\n",
       "  'arnet_id': '56d8b1badabfae2eeee6ee7e',\n",
       "  'hindex': 36.285714285714285,\n",
       "  'productivity': 108.85714285714286},\n",
       " {'paper_name': 'probabilistic latent semantic indexing.',\n",
       "  'arnet_id': '56d85d03dabfae2eee5e3d27',\n",
       "  'hindex': 53.0,\n",
       "  'productivity': 180.0},\n",
       " {'paper_name': 'learning deep structured semantic models for web search using clickthrough data. in cikm.',\n",
       "  'arnet_id': '53e9affab7602d9703a4f291',\n",
       "  'hindex': 43.0,\n",
       "  'productivity': 210.33333333333334},\n",
       " {'paper_name': 'recurrent continuous translation models.',\n",
       "  'arnet_id': '53e99b04b7602d9702392284',\n",
       "  'hindex': 22.5,\n",
       "  'productivity': 68.5},\n",
       " {'paper_name': 'moses: open source toolkit for statistical machine translation.',\n",
       "  'arnet_id': '5b076eb4da5629516ce73f52',\n",
       "  'hindex': 0,\n",
       "  'productivity': 0},\n",
       " {'paper_name': 'manual and automatic evaluation of machine translation between european languages.',\n",
       "  'arnet_id': '53e9a627b7602d9702f53ff8',\n",
       "  'hindex': 42.5,\n",
       "  'productivity': 182.5},\n",
       " {'paper_name': 'statistical phrase-based translation.',\n",
       "  'arnet_id': '53e99b10b7602d97023a06bd',\n",
       "  'hindex': 50.666666666666664,\n",
       "  'productivity': 143.66666666666666},\n",
       " {'paper_name': 'data inferred multi-word expressions for statistical machine translation.',\n",
       "  'arnet_id': '53e99ea7b7602d9702772132',\n",
       "  'hindex': 10.5,\n",
       "  'productivity': 32.5},\n",
       " {'paper_name': 'recursive autoencoders for itg-based translation.',\n",
       "  'arnet_id': '53e9a388b7602d9702c9a740',\n",
       "  'hindex': 36.0,\n",
       "  'productivity': 284.0},\n",
       " {'paper_name': 'an end-to-end discriminative approach to machine translation.',\n",
       "  'arnet_id': '53e9a099b7602d97029836e5',\n",
       "  'hindex': 38.75,\n",
       "  'productivity': 89.5},\n",
       " {'paper_name': 'a phrase-based, joint probability model for statistical machine translation.',\n",
       "  'arnet_id': '53e9b3c1b7602d9703eaaf8b',\n",
       "  'hindex': 27.0,\n",
       "  'productivity': 62.5},\n",
       " {'paper_name': 'recurrent neural network based language model.',\n",
       "  'arnet_id': '53e9ab48b7602d97034dc28f',\n",
       "  'hindex': 31.0,\n",
       "  'productivity': 96.4},\n",
       " {'paper_name': 'extensions of recurrent neural network language model.',\n",
       "  'arnet_id': '558c197f84ae6766fdf10bdc',\n",
       "  'hindex': 7.75,\n",
       "  'productivity': 15.25},\n",
       " {'paper_name': 'statistical language model based on neural networks.',\n",
       "  'arnet_id': '5b076eb4da5629516ce742bc',\n",
       "  'hindex': 0,\n",
       "  'productivity': 0},\n",
       " {'paper_name': 'exploiting similarities among languages for machine translation.',\n",
       "  'arnet_id': '53e9b895b7602d970446ec0b',\n",
       "  'hindex': 42.333333333333336,\n",
       "  'productivity': 71.66666666666667},\n",
       " {'paper_name': 'linguistic regularities in continuous space word representations.',\n",
       "  'arnet_id': '53e99acab7602d9702348ab6',\n",
       "  'hindex': 36.0,\n",
       "  'productivity': 100.0},\n",
       " {'paper_name': 'polylingual topic models.',\n",
       "  'arnet_id': '53e99832b7602d9702055bfa',\n",
       "  'hindex': 36.0,\n",
       "  'productivity': 127.4},\n",
       " {'paper_name': 'wider context by using bilingual language models in machine translation.',\n",
       "  'arnet_id': '53e9b05cb7602d9703ac1a6e',\n",
       "  'hindex': 31.0,\n",
       "  'productivity': 178.75},\n",
       " {'paper_name': 'minimum error rate training in statistical machine translation.',\n",
       "  'arnet_id': '53e9a4c8b7602d9702dead7d',\n",
       "  'hindex': 41.0,\n",
       "  'productivity': 91.0},\n",
       " {'paper_name': 'the alignment template approach to statistical machine translation.',\n",
       "  'arnet_id': '53e9bb0fb7602d970474a1d8',\n",
       "  'hindex': 67.0,\n",
       "  'productivity': 484.0},\n",
       " {'paper_name': 'bleu: a method for automatic evaluation of machine translation.',\n",
       "  'arnet_id': '53e9a3abb7602d9702cbbf7e',\n",
       "  'hindex': 22.75,\n",
       "  'productivity': 47.5},\n",
       " {'paper_name': 'translingual document representations from discriminative projections.',\n",
       "  'arnet_id': '53e9b1d1b7602d9703c62ccd',\n",
       "  'hindex': 51.0,\n",
       "  'productivity': 145.33333333333334},\n",
       " {'paper_name': 'expected bleu training for graphs: bbn system description for wmt system combination task.',\n",
       "  'arnet_id': '53e9b3bcb7602d9703ea37b2',\n",
       "  'hindex': 0,\n",
       "  'productivity': 0},\n",
       " {'paper_name': 'smooth bilingual n-gram translation.',\n",
       "  'arnet_id': '53e9b4e4b7602d970400d04c',\n",
       "  'hindex': 15.0,\n",
       "  'productivity': 65.33333333333333},\n",
       " {'paper_name': 'continuous space translation models for phrase-based statistical machine translation.',\n",
       "  'arnet_id': '53e9b47db7602d9703f82e3c',\n",
       "  'hindex': 31.0,\n",
       "  'productivity': 162.0},\n",
       " {'paper_name': 'large, pruned or continuous space language models on a gpu for statistical machine translation.',\n",
       "  'arnet_id': '53e9b403b7602d9703ef5d92',\n",
       "  'hindex': 12.666666666666666,\n",
       "  'productivity': 58.333333333333336},\n",
       " {'paper_name': 'discriminative training of 150 million translation parameters and its application to pruning.',\n",
       "  'arnet_id': '53e9ba2db7602d970463d1f4',\n",
       "  'hindex': 4.5,\n",
       "  'productivity': 40.5},\n",
       " {'paper_name': 'semantic compositionality through recursive matrix-vector spaces.',\n",
       "  'arnet_id': '53e9a4fbb7602d9702e1ec18',\n",
       "  'hindex': 69.0,\n",
       "  'productivity': 215.5},\n",
       " {'paper_name': 'parsing natural scenes and natural language with recursive neural networks.',\n",
       "  'arnet_id': '5b076eb4da5629516ce7375e',\n",
       "  'hindex': 0,\n",
       "  'productivity': 0},\n",
       " {'paper_name': 'continuous space translation models with neural networks.',\n",
       "  'arnet_id': '53e99ca8b7602d970255830b',\n",
       "  'hindex': 16.333333333333332,\n",
       "  'productivity': 75.33333333333333},\n",
       " {'paper_name': 'comparison of feed forward and recurrent neural network language models.',\n",
       "  'arnet_id': '53e9b281b7602d9703d24fe3',\n",
       "  'hindex': 34.666666666666664,\n",
       "  'productivity': 205.0},\n",
       " {'paper_name': 'towards deeper understanding: deep convex networks for semantic utterance classification.',\n",
       "  'arnet_id': '53e9a408b7602d9702d24c7e',\n",
       "  'hindex': 26.25,\n",
       "  'productivity': 73.5},\n",
       " {'paper_name': 'inferring a semantic representation of text via cross-language correlation analysis.',\n",
       "  'arnet_id': '53e9a169b7602d9702a59062',\n",
       "  'hindex': 39.666666666666664,\n",
       "  'productivity': 225.66666666666666},\n",
       " {'paper_name': 'large scale image annotation: learning to rank with joint word-image embeddings.',\n",
       "  'arnet_id': '5b663feaab2dfb45920c65fd',\n",
       "  'hindex': 0,\n",
       "  'productivity': 0},\n",
       " {'paper_name': 'training phrase translation models with leaving-oneout.',\n",
       "  'arnet_id': '53e9b3e9b7602d9703ed8693',\n",
       "  'hindex': 37.333333333333336,\n",
       "  'productivity': 310.3333333333333},\n",
       " {'paper_name': 'learning discriminative projections for text similarity measures.',\n",
       "  'arnet_id': '53e9acf7b7602d97036d73b7',\n",
       "  'hindex': 47.25,\n",
       "  'productivity': 132.25},\n",
       " {'paper_name': 'a novel decision function and the associated decision-feedback learning for speech translation.',\n",
       "  'arnet_id': '53e9b527b7602d970405b59d',\n",
       "  'hindex': 30.5,\n",
       "  'productivity': 103.25},\n",
       " {'paper_name': 'combining heterogeneous models for measuring relational similarity.',\n",
       "  'arnet_id': '53e9ade9b7602d97037f18af',\n",
       "  'hindex': 28.8,\n",
       "  'productivity': 78.8},\n",
       " {'paper_name': 'bilingual word embeddings for phrase-based machine translation.',\n",
       "  'arnet_id': '53e9a98eb7602d97032e6cd7',\n",
       "  'hindex': 42.25,\n",
       "  'productivity': 138.0}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_feat['P14-1066']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_feature = pickle.load(open(\"pickles_data/location_feature.pkl\",\"rb\"))\n",
    "title_overlap = pickle.load(open(\"pickles_data/title_overlap.pkl\",\"rb\"))\n",
    "context_count = pickle.load(open(\"pickles_data/context_count.pkl\",\"rb\"))\n",
    "cue_words = pickle.load(open(\"pickles_data/cue_count.pkl\", \"rb\"))\n",
    "tags = pickle.load(open(\"pickles_data/baseline_tags.pkl\", \"rb\"))\n",
    "contexts = pickle.load(open(\"pickles_data/context_words.pkl\", \"rb\"))\n",
    "popularity = pickle.load(open(\"pickles_data/popularity_sent.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_info = pickle.load(open(\"pickles/paper_info (1).pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_info_1 = pickle.load(open(\"pickles/paper_info (1).pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_sim = pickle.load(open(\"pickles_data/abstract_similarity.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper_name': 'scalable training of l1-regularized log-linear models.',\n",
       "  'citation': 456,\n",
       "  'abstract': 'The L-BFGS limited-memory quasi-Newton method is the algorithm of choice for optimizing the parameters of large-scale log-linear models with L2 regularization, but it cannot be used for an L1-regularized loss due to its non-differentiability whenever some parameter is zero. Efficient algorithms have been proposed for this task, but they are impractical when the number of parameters is very large. We present an algorithm Orthant-Wise Limited-memory Quasi-Newton (OWL-QN), based on L-BFGS, that can efficiently optimize the L1-regularized log-likelihood of log-linear models with millions of parameters. In our experiments on a parse reranking task, our algorithm was several orders of magnitude faster than an alternative algorithm, and substantially faster than L-BFGS on the analogous L2-regularized problem. We also present a proof that OWL-QN is guaranteed to converge to a globally optimal parameter vector.',\n",
       "  'arnet_id': '53e99f2eb7602d97027ff009',\n",
       "  'venue_cit': 83.42026009582477},\n",
       " {'paper_name': 'joint language and translation modeling with recurrent neural networks.',\n",
       "  'citation': 176,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9afa6b7602d97039f69a4',\n",
       "  'venue_cit': 41.31937172774869},\n",
       " {'paper_name': 'learning deep architectures for ai. fundamental trends',\n",
       "  'citation': 5713,\n",
       "  'abstract': 'Theoretical results suggest that in order to learn the kind of com-plicated functions that can represent high-level abstractions (e.g., invision, language, and other AI-level tasks), one may needdeep architec-tures. Deep architectures are composed of multiple levels of non-linearoperations, such as in neural nets with many hidden layers or in com-plicated propositional formulae re-using many sub-formulae. Searchingthe parameter space of deep architectures is a difficult task, but learningalgorithms such as those for Deep Belief Networks have recently beenproposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivationsand principles regarding learning algorithms for deep architectures, inparticular those exploiting as building blocks unsupervised learning ofsingle-layer models such as Restricted Boltzmann Machines, used toconstruct deeper models such as Deep Belief Networks.',\n",
       "  'arnet_id': '53e9afbab7602d9703a074c5',\n",
       "  'venue_cit': 598.8709677419355},\n",
       " {'paper_name': 'a neural probabilistic language model.',\n",
       "  'citation': 3765,\n",
       "  'abstract': 'A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.',\n",
       "  'arnet_id': '53e9ab37b7602d97034c4455',\n",
       "  'venue_cit': 136.72063037249285},\n",
       " {'paper_name': 'latent dirichlet allocation.',\n",
       "  'citation': 0,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '56d85479dabfae2eee1db078',\n",
       "  'venue_cit': None},\n",
       " {'paper_name': 'natural language processing (almost) from scratch.',\n",
       "  'citation': 3421,\n",
       "  'abstract': 'We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.',\n",
       "  'arnet_id': '53e99beab7602d970249335e',\n",
       "  'venue_cit': 136.72063037249285},\n",
       " {'paper_name': 'indexing by latent semantic analysis.',\n",
       "  'citation': 2,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9ae97b7602d97038b9f73',\n",
       "  'venue_cit': 22.499118165784832},\n",
       " {'paper_name': 'why generative phrase models underperform surface heuristics.',\n",
       "  'citation': 86,\n",
       "  'abstract': 'We investigate why weights from generative models underperform heuristic estimates in phrase-based machine translation. We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics. The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM. In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased deter-minization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score.',\n",
       "  'arnet_id': '53e9a6d8b7602d97030106ed',\n",
       "  'venue_cit': 36.111111111111114},\n",
       " {'paper_name': 'scalable stacking and learning for building deep architectures.',\n",
       "  'citation': 166,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e99ef4b7602d97027bee47',\n",
       "  'venue_cit': 12.382211727666274},\n",
       " {'paper_name': 'principle component ieural ietworks: theory and applications.',\n",
       "  'citation': 1327,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9ba3fb7602d970464dab8',\n",
       "  'venue_cit': 1327.0},\n",
       " {'paper_name': 'automatic cross-language retrieval using latent semantic indexing.',\n",
       "  'citation': 0,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '56d8b3f4dabfae2eeef8d042',\n",
       "  'venue_cit': None},\n",
       " {'paper_name': 'posterior regularization for structured latent variable models.',\n",
       "  'citation': 308,\n",
       "  'abstract': 'We present posterior regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efficiently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efficiency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efficient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment.',\n",
       "  'arnet_id': '53e9ad5db7602d9703746035',\n",
       "  'venue_cit': 136.72063037249285},\n",
       " {'paper_name': 'training mrf-based translation models using gradient ascent.',\n",
       "  'citation': 19,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9a8bdb7602d970320b858',\n",
       "  'venue_cit': 49.58017727639001},\n",
       " {'paper_name': 'clickthrough-based latent semantic models for web search.',\n",
       "  'citation': 58,\n",
       "  'abstract': 'This paper presents two new document ranking models for Web search based upon the methods of semantic representation and the statistical translation-based approach to information retrieval (IR). Assuming that a query is parallel to the titles of the documents clicked on for that query, large amounts of query-title pairs are constructed from clickthrough data; two latent semantic models are learned from this data. One is a bilingual topic model within the language modeling framework. It ranks documents for a query by the likelihood of the query being a semantics-based translation of the documents. The semantic representation is language independent and learned from query-title pairs, with the assumption that a query and its paired titles share the same distribution over semantic topics. The other is a discriminative projection model within the vector space modeling framework. Unlike Latent Semantic Analysis and its variants, the projection matrix in our model, which is used to map from term vectors into sematic space, is learned discriminatively such that the distance between a query and its paired title, both represented as vectors in the projected semantic space, is smaller than that between the query and the titles of other documents which have no clicks for that query. These models are evaluated on the Web search task using a real world data set. Results show that they significantly outperform their corresponding baseline models, which are state-of-the-art.',\n",
       "  'arnet_id': '53e9abfdb7602d97035bb87a',\n",
       "  'venue_cit': 52.84563591022444},\n",
       " {'paper_name': 'maximum expected bleu training of phrase and lexicon translation models.',\n",
       "  'citation': 49,\n",
       "  'abstract': 'This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.',\n",
       "  'arnet_id': '53e9b2b1b7602d9703d5827b',\n",
       "  'venue_cit': 31.555555555555557},\n",
       " {'paper_name': 'discovering binary codes for documents by learning deep generative models. topics in cognitive science,',\n",
       "  'citation': 0,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9bc0fb7602d9704872dda',\n",
       "  'venue_cit': 3.8846153846153846},\n",
       " {'paper_name': 'deep neural networks for acoustic modeling in speech recognition.',\n",
       "  'citation': 8,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '56d8b1badabfae2eeee6ee7e',\n",
       "  'venue_cit': None},\n",
       " {'paper_name': 'probabilistic latent semantic indexing.',\n",
       "  'citation': 0,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '56d85d03dabfae2eee5e3d27',\n",
       "  'venue_cit': None},\n",
       " {'paper_name': 'learning deep structured semantic models for web search using clickthrough data. in cikm.',\n",
       "  'citation': 499,\n",
       "  'abstract': 'Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.',\n",
       "  'arnet_id': '53e9affab7602d9703a4f291',\n",
       "  'venue_cit': 29.216761633428302},\n",
       " {'paper_name': 'recurrent continuous translation models.',\n",
       "  'citation': 543,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e99b04b7602d9702392284',\n",
       "  'venue_cit': 41.31937172774869},\n",
       " {'paper_name': 'moses: open source toolkit for statistical machine translation.',\n",
       "  'citation': 4118,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '5b076eb4da5629516ce73f52',\n",
       "  'venue_cit': None},\n",
       " {'paper_name': 'manual and automatic evaluation of machine translation between european languages.',\n",
       "  'citation': 227,\n",
       "  'abstract': 'We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back. Evaluation was done automatically using the Bleu score and manually on fluency and adequacy.',\n",
       "  'arnet_id': '53e9a627b7602d9702f53ff8',\n",
       "  'venue_cit': 36.111111111111114},\n",
       " {'paper_name': 'statistical phrase-based translation.',\n",
       "  'citation': 3440,\n",
       "  'abstract': 'We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.',\n",
       "  'arnet_id': '53e99b10b7602d97023a06bd',\n",
       "  'venue_cit': 49.58017727639001},\n",
       " {'paper_name': 'data inferred multi-word expressions for statistical machine translation.',\n",
       "  'citation': 46,\n",
       "  'abstract': 'This paper presents a strategy for detecting and using multi-word expressions in Statistical Ma- chine Translation. Performance of the proposed strategy is evaluated in terms of alignment qual- ity as well as translation accuracy. Evaluations are performed by using the Verbmobil corpus. Results from translation tasks from English-to- Spanish and from Spanish-to-English are pre- sented and discussed.',\n",
       "  'arnet_id': '53e99ea7b7602d9702772132',\n",
       "  'venue_cit': None},\n",
       " {'paper_name': 'recursive autoencoders for itg-based translation.',\n",
       "  'citation': 58,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9a388b7602d9702c9a740',\n",
       "  'venue_cit': 41.31937172774869},\n",
       " {'paper_name': 'an end-to-end discriminative approach to machine translation.',\n",
       "  'citation': 287,\n",
       "  'abstract': 'We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited. Unlike discriminative reranking approaches, our system can take advantage of learned features in all stages of decoding. We first discuss several challenges to error-driven discriminative approaches. In particular, we explore different ways of updating parameters given a training example. We find that making frequent but smaller updates is preferable to making fewer but larger updates. Then, we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples. One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process, which has previously been entirely heuristic.',\n",
       "  'arnet_id': '53e9a099b7602d97029836e5',\n",
       "  'venue_cit': 49.22071307300509},\n",
       " {'paper_name': 'a phrase-based, joint probability model for statistical machine translation.',\n",
       "  'citation': 558,\n",
       "  'abstract': 'We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.',\n",
       "  'arnet_id': '53e9b3c1b7602d9703eaaf8b',\n",
       "  'venue_cit': 41.31937172774869},\n",
       " {'paper_name': 'recurrent neural network based language model.',\n",
       "  'citation': 2268,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9ab48b7602d97034dc28f',\n",
       "  'venue_cit': 12.439666001625655},\n",
       " {'paper_name': 'extensions of recurrent neural network language model.',\n",
       "  'citation': 0,\n",
       "  'abstract': 'We present several modifications of the original recurrent neural net work language model (RNN LM). While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.',\n",
       "  'arnet_id': '558c197f84ae6766fdf10bdc',\n",
       "  'venue_cit': 1.8969986357435198},\n",
       " {'paper_name': 'statistical language model based on neural networks.',\n",
       "  'citation': 453,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '5b076eb4da5629516ce742bc',\n",
       "  'venue_cit': None},\n",
       " {'paper_name': 'exploiting similarities among languages for machine translation.',\n",
       "  'citation': 494,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9b895b7602d970446ec0b',\n",
       "  'venue_cit': 7.095939070705416},\n",
       " {'paper_name': 'linguistic regularities in continuous space word representations.',\n",
       "  'citation': 1649,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e99acab7602d9702348ab6',\n",
       "  'venue_cit': 49.58017727639001},\n",
       " {'paper_name': 'polylingual topic models.',\n",
       "  'citation': 298,\n",
       "  'abstract': \"Topic models are a useful tool for analyz- ing large text collections, but have previ- ously been applied in only monolingual, or at most bilingual, contexts. Mean- while, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize con- tent in many languages. We introduce a polylingual topic model that discovers top- ics aligned across multiple languages. We explore the model's characteristics using two large corpora, each with over ten dif- ferent languages, and demonstrate its use- fulness in supporting machine translation and tracking topic trends across languages.\",\n",
       "  'arnet_id': '53e99832b7602d9702055bfa',\n",
       "  'venue_cit': 89.29032258064517},\n",
       " {'paper_name': 'wider context by using bilingual language models in machine translation.',\n",
       "  'citation': 59,\n",
       "  'abstract': 'In past Evaluations for Machine Translation of European Languages, it could be shown that the translation performance of SMT systems can be increased by integrating a bilingual language model into a phrase-based SMT system. In the bilingual language model, target words with their aligned source words build the tokens of an n-gram based language model. We analyzed the effect of bilingual language models and show where they could help to better model the translation process. We could show improvements of translation quality on German-to-English and Arabic-to-English. In addition, for the Arabic-to-English task, training an extra bilingual language model on the POS tags instead of the surface word forms led to further improvements.',\n",
       "  'arnet_id': '53e9b05cb7602d9703ac1a6e',\n",
       "  'venue_cit': 21.110169491525422},\n",
       " {'paper_name': 'minimum error rate training in statistical machine translation.',\n",
       "  'citation': 1583,\n",
       "  'abstract': 'Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure.',\n",
       "  'arnet_id': '53e9a4c8b7602d9702dead7d',\n",
       "  'venue_cit': 49.22071307300509},\n",
       " {'paper_name': 'the alignment template approach to statistical machine translation.',\n",
       "  'citation': 1008,\n",
       "  'abstract': 'A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German–English speech VERBMOBIL task, we analyze the effect of various system components. On the French–English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.',\n",
       "  'arnet_id': '53e9bb0fb7602d970474a1d8',\n",
       "  'venue_cit': 72.52115942028985},\n",
       " {'paper_name': 'bleu: a method for automatic evaluation of machine translation.',\n",
       "  'citation': 7964,\n",
       "  'abstract': 'Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.',\n",
       "  'arnet_id': '53e9a3abb7602d9702cbbf7e',\n",
       "  'venue_cit': 353.6},\n",
       " {'paper_name': 'translingual document representations from discriminative projections.',\n",
       "  'citation': 94,\n",
       "  'abstract': 'Representing documents by vectors that are independent of language enhances machine translation and multilingual text categorization. We use discriminative training to create a projection of documents from multiple languages into a single translingual vector space. We explore two variants to create these projections: Oriented Principal Component Analysis (OPCA) and Coupled Probabilistic Latent Semantic Analysis (CPLSA). Both of these variants start with a basic model of documents (PCA and PLSA). Each model is then made discriminative by encouraging comparable document pairs to have similar vector representations. We evaluate these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters. The two discriminative variants, OPCA and CPLSA, significantly outperform their corresponding baselines. The largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel. The OPCA method is shown to perform best.',\n",
       "  'arnet_id': '53e9b1d1b7602d9703c62ccd',\n",
       "  'venue_cit': 41.31937172774869},\n",
       " {'paper_name': 'expected bleu training for graphs: bbn system description for wmt system combination task.',\n",
       "  'citation': 21,\n",
       "  'abstract': 'BBN submitted system combination outputs for Czech-English, German-English, Spanish-English, and French-English language pairs. All combinations were based on confusion network decoding. The confusion networks were built using incremental hypothesis alignment algorithm with flexible matching. A novel bi-gram count feature, which can penalize bi-grams not present in the input hypotheses corresponding to a source sentence, was introduced in addition to the usual decoder features. The system combination weights were tuned using a graph based expected BLEU as the objective function while incrementally expanding the networks to bi-gram and 5-gram contexts. The expected BLEU tuning described in this paper naturally generalizes to hypergraphs and can be used to optimize thousands of weights. The combination gained about 0.5-4.0 BLEU points over the best individual systems on the official WMT11 language pairs. A 39 system multi-source combination achieved an 11.1 BLEU point gain.',\n",
       "  'arnet_id': '53e9b3bcb7602d9703ea37b2',\n",
       "  'venue_cit': 21.110169491525422},\n",
       " {'paper_name': 'smooth bilingual n-gram translation.',\n",
       "  'citation': 34,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9b4e4b7602d970400d04c',\n",
       "  'venue_cit': 94.4954128440367},\n",
       " {'paper_name': 'continuous space translation models for phrase-based statistical machine translation.',\n",
       "  'citation': 0,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9b47db7602d9703f82e3c',\n",
       "  'venue_cit': 20.486068111455108},\n",
       " {'paper_name': 'large, pruned or continuous space language models on a gpu for statistical machine translation.',\n",
       "  'citation': 89,\n",
       "  'abstract': 'Language models play an important role in large vocabulary speech recognition and statistical machine translation systems. The dominant approach since several decades are back-off language models. Some years ago, there was a clear tendency to build huge language models trained on hundreds of billions of words. Lately, this tendency has changed and recent works concentrate on data selection. Continuous space methods are a very competitive approach, but they have a high computational complexity and are not yet in widespread use. This paper presents an experimental comparison of all these approaches on a large statistical machine translation task. We also describe an open-source implementation to train and use continuous space language models (CSLM) for such large tasks. We describe an efficient implementation of the CSLM using graphical processing units from Nvidia. By these means, we are able to train an CSLM on more than 500 million words in 20 hours. This CSLM provides an improvement of up to 1.8 BLEU points with respect to the best back-off language model that we were able to build.',\n",
       "  'arnet_id': '53e9b403b7602d9703ef5d92',\n",
       "  'venue_cit': 30.77777777777778},\n",
       " {'paper_name': 'discriminative training of 150 million translation parameters and its application to pruning.',\n",
       "  'citation': 6,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9ba2db7602d970463d1f4',\n",
       "  'venue_cit': 49.58017727639001},\n",
       " {'paper_name': 'semantic compositionality through recursive matrix-vector spaces.',\n",
       "  'citation': 813,\n",
       "  'abstract': 'Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.',\n",
       "  'arnet_id': '53e9a4fbb7602d9702e1ec18',\n",
       "  'venue_cit': 70.20553359683794},\n",
       " {'paper_name': 'parsing natural scenes and natural language with recursive neural networks.',\n",
       "  'citation': 757,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '5b076eb4da5629516ce7375e',\n",
       "  'venue_cit': None},\n",
       " {'paper_name': 'continuous space translation models with neural networks.',\n",
       "  'citation': 97,\n",
       "  'abstract': 'The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models. For lack of sufficient training data, most models only consider a small amount of context. As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture. In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance.',\n",
       "  'arnet_id': '53e99ca8b7602d970255830b',\n",
       "  'venue_cit': 49.58017727639001},\n",
       " {'paper_name': 'comparison of feed forward and recurrent neural network language models.',\n",
       "  'citation': 84,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9b281b7602d9703d24fe3',\n",
       "  'venue_cit': 12.382211727666274},\n",
       " {'paper_name': 'towards deeper understanding: deep convex networks for semantic utterance classification.',\n",
       "  'citation': 74,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9a408b7602d9702d24c7e',\n",
       "  'venue_cit': 12.382211727666274},\n",
       " {'paper_name': 'inferring a semantic representation of text via cross-language correlation analysis.',\n",
       "  'citation': 248,\n",
       "  'abstract': 'The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short En- glish document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a stan- dard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the first and in the second space that are max- imally correlated. Since we assume the two representations are com- pletely independent apart from the semantic content, any correlation be- tween them should reflect some semantic similarity. Certain patterns of English words that relate to a specific meaning should correlate with cer- tain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we first demonstrate that the correlations detected between the two versions of the corpus are significantly higher than random, and hence that a rep- resentation based on such features does capture statistical patterns that should reflect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and significantly superior to LSI on the same data.',\n",
       "  'arnet_id': '53e9a169b7602d9702a59062',\n",
       "  'venue_cit': 79.68112969390975},\n",
       " {'paper_name': 'large scale image annotation: learning to rank with joint word-image embeddings.',\n",
       "  'citation': 0,\n",
       "  'abstract': 'Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at  of the ranked list of annotations for a given image  learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where annotations with alternate spellings or even languages are close in the embedding space. Hence, even when our model does not predict the exact annotation given by a human labeler, it often predicts similar annotations, a fact that we try to quantify by measuring the newly introduced “sibling” precision metric, where our method also obtains excellent results.',\n",
       "  'arnet_id': '5b663feaab2dfb45920c65fd',\n",
       "  'venue_cit': 175.19750430292598},\n",
       " {'paper_name': 'training phrase translation models with leaving-oneout.',\n",
       "  'citation': 86,\n",
       "  'abstract': 'Several attempts have been made to learn phrase translation probabilities for phrase-based statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with over-fitting. We describe a novel leaving-one-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering models in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%.',\n",
       "  'arnet_id': '53e9b3e9b7602d9703ed8693',\n",
       "  'venue_cit': 49.22071307300509},\n",
       " {'paper_name': 'learning discriminative projections for text similarity measures.',\n",
       "  'citation': 102,\n",
       "  'abstract': 'Traditional text similarity measures consider each term similar only to itself and do not model semantic relatedness of terms. We propose a novel discriminative training method that projects the raw term vectors into a common, low-dimensional vector space. Our approach operates by finding the optimal matrix to minimize the loss of the pre-selected similarity function (e.g., cosine) of the projected vectors, and is able to efficiently handle a large number of training examples in the high-dimensional space. Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient.',\n",
       "  'arnet_id': '53e9acf7b7602d97036d73b7',\n",
       "  'venue_cit': 35.661211129296234},\n",
       " {'paper_name': 'a novel decision function and the associated decision-feedback learning for speech translation.',\n",
       "  'citation': 22,\n",
       "  'abstract': 'In this paper we report our recent development of an end-to-end integrative design methodology for speech translation. Specifically, a novel decision function is proposed based on the Bayesian analysis, and the associated discriminative learning technique is presented based on the decision-feedback principle. The decision function in our end-to-end design methodology integrates acoustic scores, language model scores and translation scores to refine the translation hypotheses and to determine the best translation candidate. This Bayesian-guided decision function is then embedded into the training process that jointly learns the parameters in speech recognition and machine translation sub-systems in the overall speech translation system. The resulting decision-feedback learning takes a functional form similar to the minimum classification error training. Experimental results obtained on the IWSLT DIALOG 2010 database showed that the proposed system outperformed the baseline system in terms of BLEU score by 2.3 points.',\n",
       "  'arnet_id': '53e9b527b7602d970405b59d',\n",
       "  'venue_cit': 12.382211727666274},\n",
       " {'paper_name': 'combining heterogeneous models for measuring relational similarity.',\n",
       "  'citation': 31,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9ade9b7602d97037f18af',\n",
       "  'venue_cit': 49.58017727639001},\n",
       " {'paper_name': 'bilingual word embeddings for phrase-based machine translation.',\n",
       "  'citation': 347,\n",
       "  'abstract': None,\n",
       "  'arnet_id': '53e9a98eb7602d97032e6cd7',\n",
       "  'venue_cit': 41.31937172774869}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_info[list(paper_info.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_diff = pickle.load(open(\"pickles_data/year_diff.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_context = pickle.load(open(\"pickles_data/fixed_context.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_cue = pickle.load(open(\"pickles_data/weighted_cue_words.pkl\", \"rb\"))\n",
    "bert_embeddings = pickle.load(open(\"pickles_data/bert_embeddings_relu.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181\n"
     ]
    }
   ],
   "source": [
    "print(len(context_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_table = pickle.load(open(\"pickles_data/num_table.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cit_titles = pickle.load(open(\"pickles_data/citation_titles.pkl\",\"rb\"))\n",
    "ids = tags.keys()\n",
    "ids = list(set(ids).intersection(set(num_table.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181\n"
     ]
    }
   ],
   "source": [
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "27636\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "c = 0\n",
    "cbert = 0\n",
    "count_cc = 0\n",
    "for key in ids :\n",
    "    papers = cit_titles[key]\n",
    "    for paper in papers :\n",
    "        data = {}\n",
    "        data['paper_id'] = key\n",
    "        data['citation_title'] = paper\n",
    "        \n",
    "        if(key in bert_embeddings):\n",
    "            embedding_found = 0\n",
    "            for pap in bert_embeddings[key] :\n",
    "                if(pap['paper_name']==paper) :\n",
    "                    cbert+=1\n",
    "                    data['bert_embed'] = pap['embedding']\n",
    "                    embedding_found = 1\n",
    "                    break\n",
    "                    \n",
    "            if(embedding_found==1) :\n",
    "                \n",
    "                if(key in paper_info) :\n",
    "                    info_found = 0 \n",
    "                    for pap in paper_info[key] :\n",
    "                        if(pap['paper_name']==paper) :\n",
    "                            try: \n",
    "                                data['citation_count'] = int(pap['citation'])\n",
    "                            except :\n",
    "                                data['citation_count'] = 1\n",
    "                                count_cc += 1\n",
    "                            try :\n",
    "                                data['venue_cit'] = int(pap['venue_cit'])\n",
    "                            except :\n",
    "                                data['venue_cit'] = 20\n",
    "                            \n",
    "                            info_found = 1\n",
    "                            break\n",
    "                            \n",
    "                    if(info_found==1) :\n",
    "                        \n",
    "                        for pap in abs_sim[key] :\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                data['abstract_sim'] = pap['abs_sim']\n",
    "                                break\n",
    "                                \n",
    "                        for pap in location_feature[key]:\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                data['location_feature'] = pap['location_feature']\n",
    "                                break\n",
    "                                \n",
    "                        for pap in author_feat[key] :\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                data['author_hindex'] = pap['hindex']\n",
    "                                data['author_prod'] = pap['productivity']\n",
    "\n",
    "                        for pap in num_table[key]:\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                data['num_table'] = pap['num_table']\n",
    "                                break\n",
    "\n",
    "\n",
    "                        for pap in year_diff[key]:\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                data['year_diff'] = pap['diff']\n",
    "                                break\n",
    "\n",
    "                        for pap in title_overlap[key]:\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                data['title_overlap'] = pap['overlap']\n",
    "                                break\n",
    "\n",
    "                        for pap in popularity[key]:\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                data['popularity'] = pap['popularity']\n",
    "                                break\n",
    "\n",
    "                        for pap in context_count[key]:\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                data['context_count'] = pap['context_count']\n",
    "                                break\n",
    "\n",
    "                        for pap in contexts[key] :\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                data['context'] = pap['context']\n",
    "                                break\n",
    "\n",
    "                        for pap in cue_words[key]:\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                data['cue_count'] = pap['cue_count']\n",
    "                                break\n",
    "\n",
    "                        for pap in weighted_cue[key]:\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                max_data = pap['cue_weights_max']\n",
    "                                data['wcue_max'] = []\n",
    "                                for key1 in max_data :\n",
    "                                    data['wcue_max'].append(max_data[key1])\n",
    "                                add_data = pap['cue_weights_add']\n",
    "                                data['wcue_add'] = []\n",
    "                                for key1 in add_data :\n",
    "                                    data['wcue_add'].append(add_data[key1])\n",
    "                                break\n",
    "\n",
    "                        for pap in tags[key]:\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                if(pap['tag']==1):\n",
    "                                    data['label'] = 'baseline'\n",
    "                                else :\n",
    "                                    data['label'] = 'non_baseline'\n",
    "                                break\n",
    "\n",
    "                        for pap in fixed_context[key]:\n",
    "                            if(pap['paper_name']==paper) :\n",
    "                                data['fixed_context'] = pap['fixed_context']                   \n",
    "                        dataset.append(data)\n",
    "                        \n",
    "                        \n",
    "        \n",
    "print(c)\n",
    "print(cbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26329\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(count_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26323\n"
     ]
    }
   ],
   "source": [
    "final_dataset = []\n",
    "for data in dataset :\n",
    "    if(data['context_count']!=0):\n",
    "        final_dataset.append(data)\n",
    "print(len(final_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_baselines = []\n",
    "baselines = []\n",
    "for data in dataset :\n",
    "    if(data['label']==\"baseline\"):\n",
    "        baselines.append(data)\n",
    "    else :\n",
    "        non_baselines.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2663\n",
      "23660\n"
     ]
    }
   ],
   "source": [
    "print(len(baselines))\n",
    "print(len(non_baselines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = np.random.choice(len(non_baselines), len(baselines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_base = []\n",
    "for x in ar :\n",
    "    non_base.append(non_baselines[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines.extend(non_base)\n",
    "dataset = baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5326\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = dataset[:int(0.8*len(dataset))]\n",
    "testset = dataset[int(0.8*len(dataset)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2119\n",
      "544\n"
     ]
    }
   ],
   "source": [
    "count_train = 0\n",
    "count_test = 0\n",
    "for data in trainset :\n",
    "    if(data['label']=='baseline') :\n",
    "        count_train+=1\n",
    "for data in testset :\n",
    "    if(data['label']=='baseline') :\n",
    "        count_test+=1\n",
    "\n",
    "print(count_train)\n",
    "print(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(probs):\n",
    "\n",
    "\tsum = 0\n",
    "\tfor key, value in probs.items():\n",
    "\t\tsum+=value\n",
    "\tsum = round(sum, 7)\n",
    "\treturn sum==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_model(trainset, testset):\n",
    "\n",
    "    word_baseline_frequency = {}\n",
    "    word_non_baseline_frequency = {}\n",
    "    baseline_length = 0\n",
    "    non_baseline_length = 0\n",
    "\n",
    "    for i in trainset:\n",
    "        # if len(i['fixed_context'])>1 and i['label']=='baseline':\n",
    "        # \tcontinue\n",
    "        for context in i['fixed_context']:\n",
    "            if i['label']=='baseline':\n",
    "                baseline_length+=len(context)\n",
    "            else:\n",
    "                non_baseline_length+=len(context)\n",
    "            for word in context:\n",
    "                if word not in word_baseline_frequency:\n",
    "                    word_baseline_frequency[word] = 0\n",
    "                    word_non_baseline_frequency[word] = 0\n",
    "                if i['label']=='baseline':\n",
    "                    word_baseline_frequency[word]+=1\n",
    "                else:\n",
    "                    word_non_baseline_frequency[word]+=1\n",
    "\n",
    "    final_word_baseline_frequency = {}\n",
    "    final_word_baseline_frequency['<unk>'] = 0\n",
    "    word_non_baseline_frequency['<unk>'] = 0\n",
    "\n",
    "    for word in word_baseline_frequency:\n",
    "        if word_baseline_frequency[word]>5:\t\n",
    "            final_word_baseline_frequency[word] = word_baseline_frequency[word]\n",
    "        else:\n",
    "            final_word_baseline_frequency['<unk>'] += word_baseline_frequency[word]\n",
    "            word_non_baseline_frequency['<unk>'] += word_non_baseline_frequency[word]\n",
    "            del word_non_baseline_frequency[word]\n",
    "\n",
    "    word_baseline_frequency = final_word_baseline_frequency\n",
    "\n",
    "    word_baseline_probability = {}\n",
    "    word_non_baseline_probability = {}\n",
    "\n",
    "    for word in word_baseline_frequency:\n",
    "        word_baseline_probability[word] = (word_baseline_frequency[word]+1)/(baseline_length+len(word_baseline_frequency))\n",
    "        word_non_baseline_probability[word] = (word_non_baseline_frequency[word]+1)/(non_baseline_length+len(word_non_baseline_frequency))\n",
    "\n",
    "    # print(word_baseline_probability['<unk>'])\n",
    "    # print(word_non_baseline_probability['<unk>'])\n",
    "\n",
    "    assert sanity_check(word_baseline_probability) and sanity_check(word_non_baseline_probability)\n",
    "\n",
    "    conf_mat1 = np.zeros((2,2))\n",
    "    conf_mat2 = np.zeros((2,2))\n",
    "\n",
    "    min_p = 1\n",
    "\n",
    "    for i in trainset+testset:\n",
    "        P_b = 0\n",
    "        P_nb = 0\n",
    "        maxP_b = 0\n",
    "        minP_nb = 1\n",
    "        n_better = 0\n",
    "        for context in i['fixed_context']:\n",
    "            \n",
    "            P_b_con = 1\n",
    "            P_nb_con = 1\n",
    "            for word in context:\n",
    "                if word not in word_baseline_probability:\n",
    "                    P_b_con*=word_baseline_probability['<unk>']\n",
    "                    P_nb_con*=word_non_baseline_probability['<unk>']\n",
    "                else:\n",
    "                    P_b_con*=word_baseline_probability[word]\n",
    "                    P_nb_con*=word_non_baseline_probability[word]\n",
    "            if P_b_con>P_nb_con:\n",
    "                n_better+=1\n",
    "            P_b+=P_b_con\n",
    "            P_nb+=P_nb_con\n",
    "            maxP_b = max(P_b, P_b_con)\n",
    "            minP_nb = min(P_nb, P_nb_con)\n",
    "        P_b/=len(i['fixed_context'])\n",
    "        P_nb/=len(i['fixed_context'])\n",
    "\n",
    "        # if P_b>P_nb:\n",
    "        # \tprint('yes')\n",
    "\n",
    "        # i['P_b'] = P_b\n",
    "        # i['P_nb'] = P_nb\n",
    "#         n_better = n_better/len(i['fixed_context'])\n",
    "        min_p = min(min_p, P_b, P_nb)\n",
    "#         i['lmp'] = [P_b, P_nb, n_better]\n",
    "        i['lmp'] = [P_b, P_nb, maxP_b, minP_nb, P_b/P_nb, maxP_b/minP_nb, n_better]\n",
    "\n",
    "    for i in trainset:\t\n",
    "        # if len(i['fixed_context'])>1 and i['label']=='baseline':\n",
    "        # \ttestset.append(i)\n",
    "        # \tcontinue\n",
    "        i['lmp'][0]/=min_p\n",
    "        i['lmp'][1]/=min_p\n",
    "        i['lmp'][2]/=min_p\n",
    "        i['lmp'][3]/=min_p\n",
    "        if i['lmp'][0]>i['lmp'][1]:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat1[0, 0]+=1\n",
    "            else:\n",
    "                conf_mat1[0, 1]+=1\n",
    "        else:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat1[1, 0]+=1\n",
    "            else:\n",
    "                conf_mat1[1, 1]+=1\n",
    "        if i['lmp'][2]>i['lmp'][3]:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat2[0, 0]+=1\n",
    "            else:\n",
    "                conf_mat2[0, 1]+=1\n",
    "        else:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat2[1, 0]+=1\n",
    "            else:\n",
    "                conf_mat2[1, 1]+=1\n",
    "\n",
    "    # print(conf_mat1)\n",
    "    prec = conf_mat1[0,0]/(conf_mat1[0,0]+conf_mat1[0,1])\n",
    "    rec = conf_mat1[0,0]/(conf_mat1[0,0]+conf_mat1[1,0])\n",
    "    print(prec, rec, 2*prec*rec/(prec+rec))\n",
    "    conf_mat1 = np.zeros((2,2))\n",
    "\n",
    "    # print(conf_mat2)\n",
    "    prec = conf_mat2[0,0]/(conf_mat2[0,0]+conf_mat2[0,1])\n",
    "    rec = conf_mat2[0,0]/(conf_mat2[0,0]+conf_mat2[1,0])\n",
    "    print(prec, rec, 2*prec*rec/(prec+rec))\n",
    "#     conf_mat2 = np.zeros((2,2))\n",
    "\n",
    "    for i in testset:\n",
    "        i['lmp'][0]/=min_p\n",
    "        i['lmp'][1]/=min_p\n",
    "        i['lmp'][2]/=min_p\n",
    "        i['lmp'][3]/=min_p\n",
    "        if i['lmp'][0]>i['lmp'][1]:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat1[0, 0]+=1\n",
    "            else:\n",
    "                conf_mat1[0, 1]+=1\n",
    "        else:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat1[1, 0]+=1\n",
    "            else:\n",
    "                conf_mat1[1, 1]+=1\n",
    "        if i['lmp'][2]>i['lmp'][3]:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat2[0, 0]+=1\n",
    "            else:\n",
    "                conf_mat2[0, 1]+=1\n",
    "        else:\n",
    "            if i['label']=='baseline':\n",
    "                conf_mat2[1, 0]+=1\n",
    "            else:\n",
    "                conf_mat2[1, 1]+=1\n",
    "\n",
    "    # print(conf_mat1)\n",
    "    prec = conf_mat1[0,0]/(conf_mat1[0,0]+conf_mat1[0,1])\n",
    "    rec = conf_mat1[0,0]/(conf_mat1[0,0]+conf_mat1[1,0])\n",
    "    print(prec, rec, 2*prec*rec/(prec+rec))\n",
    "    conf_mat1 = np.zeros((2,2))\n",
    "\n",
    "    # print(conf_mat2)\n",
    "    prec = conf_mat2[0,0]/(conf_mat2[0,0]+conf_mat2[0,1])\n",
    "    rec = conf_mat2[0,0]/(conf_mat2[0,0]+conf_mat2[1,0])\n",
    "    print(prec, rec, 2*prec*rec/(prec+rec))\n",
    "    conf_mat2 = np.zeros((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8105153565851119 0.7347805568664464 0.7707920792079207\n",
      "0.7762882279011312 0.8744690891930156 0.8224589436307147\n",
      "0.7521367521367521 0.6470588235294118 0.6956521739130436\n",
      "0.7686143572621035 0.8644386030792339 0.8137150936726759\n"
     ]
    }
   ],
   "source": [
    "lm_model(trainset,testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def context_feature(trainset, testset) :\n",
    "#     total_counts = {}\n",
    "#     citation_frequency = {}\n",
    "#     paper_frequency = {}\n",
    "#     curr_doc = trainset[0]['paper_id']\n",
    "#     paper_words = set([])\n",
    "\n",
    "#     for i in trainset:\n",
    "#         if i['paper_id']!=curr_doc:\n",
    "#             for word in paper_words:\n",
    "#                 if word not in paper_frequency:\n",
    "#                     paper_frequency[word] = 0\n",
    "#                 paper_frequency[word]+=1\n",
    "#             paper_words = set([])\n",
    "#             curr_doc = i['paper_id']\n",
    "#         words = set(i['context'])\n",
    "#         paper_words = paper_words.union(words)\n",
    "#         for word in words:\n",
    "#             if word not in total_counts:\n",
    "#                 total_counts[word] = 0\n",
    "#                 citation_frequency[word] = 0\n",
    "#             citation_frequency[word]+=1\n",
    "#         counts = {word:0 for word in words}\n",
    "#         for word in i['context']:\n",
    "#             counts[word]+=1\n",
    "#             total_counts[word]+=1\n",
    "\n",
    "#     for word in paper_words:\n",
    "#         if word not in paper_frequency:\n",
    "#             paper_frequency[word] = 0\n",
    "#         paper_frequency[word]+=1\n",
    "\n",
    "#     final_citation_frequency = {word:count for word, count in citation_frequency.items()}\n",
    "#     for i in citation_frequency:\n",
    "#         if paper_frequency[i]<=20:\n",
    "#             del total_counts[i]\n",
    "#             del final_citation_frequency[i]\n",
    "\n",
    "#     citation_frequency = final_citation_frequency\n",
    "    \n",
    "#     dataset = trainset+testset\n",
    "    \n",
    "#     idfs = {}\n",
    "\n",
    "#     for word in citation_frequency:\n",
    "#         idfs[word] = math.log10(len(dataset)*0.8/citation_frequency[word])\n",
    "    \n",
    "#     total_counts['<unk>'] = 0\n",
    "    \n",
    "#     for i in range(len(dataset)):\n",
    "#         words = set(dataset[i]['context']) & set(total_counts.keys())\n",
    "#         unigram_counts = {word:0 for word in words}\n",
    "#         try:\t\n",
    "#             del unigram_counts['<SOS>']\n",
    "#             del unigram_counts['<EOS>']\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#         for j in range(len(dataset[i]['context'])):\n",
    "#             word = dataset[i]['context'][j]\n",
    "#             if word not in unigram_counts:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 unigram_counts[word]+=1-dataset[i]['distances'][j]\n",
    "\n",
    "\n",
    "#         dataset[i]['unigrams'] = unigram_counts\n",
    "\n",
    "\n",
    "#     total_unigram_counts = total_counts\n",
    "#     features = list(total_unigram_counts.keys())\n",
    "#     ngram_to_idx = {features[i]:i for i in range(len(features))}\n",
    "    \n",
    "#     pickle.dump(total_counts, open('pickles/total_counts.pkl', 'wb'))\n",
    "#     pickle.dump(dataset, open('pickles/data_ngram.pkl', 'wb'))\n",
    "#     pickle.dump(ngram_to_idx, open('pickles/ngram_to_idx.pkl', 'wb'))\n",
    "#     pickle.dump(idfs, open('pickles/idfs.pkl', 'wb'))\n",
    "    \n",
    "#     data_mat = np.zeros((len(dataset), len(ngram_to_idx)+35+1+5+1))\n",
    "\n",
    "#     for i in range(len(dataset)):\n",
    "#         context_counts = {}\n",
    "#         for word in dataset[i]['unigrams']:\n",
    "#             if word not in context_counts:\n",
    "#                 context_counts[word] = 0\n",
    "#             context_counts[word]+=dataset[i]['unigrams'][word]\n",
    "#             if word in idfs:\n",
    "#                 data_mat[i, ngram_to_idx[word]] = dataset[i]['unigrams'][word]\n",
    "\n",
    "#         dataset[i]['context_feature'] = data_mat[i, :len(ngram_to_idx)]\n",
    "        \n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = context_feature(trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = trainset+testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in dataset :\n",
    "#     wcue_max = data['wcue_max']\n",
    "#     cue_list = ['among', 'precis', 'implement', 'maximum', 'experi', 'strategi', \n",
    "#  'overal', 'previou', 'correl', 'calcul', 'achiev', 'gold', \n",
    "#  'top', 'obtain', 'significantli', 'report', 'best', 'evalu', \n",
    "#  'result', 'base', 'modifi', 'extend', 'metric', 'baselin', \n",
    "#  'accord', 'perform', 'recal', 'standard', 'figur', 'comparison',\n",
    "#  'yield', 'outperform', 'increas', 'stateoftheart', 'accuraci', 'method', \n",
    "#  'procedur', 'score', 'origin', 'highest', 'signific', 'higher', \n",
    "#  'fscore', 'compar']\n",
    "#     final_ar = []\n",
    "#     rem_word = ['method', 'recal', 'correl', 'figur', 'origin', 'among', 'precis', 'fscore', 'procedur', 'score']\n",
    "#     for i in range(len(wcue_max)):\n",
    "#         if(cue_list[i] not in rem_word):\n",
    "#             final_ar.append(wcue_max[i])\n",
    "#     data['wcue_max'] = final_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "for data in dataset :\n",
    "    print(len(data['wcue_max']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1+1+1+1+ 1+1+1+ 6+7+ 44+256\n",
    "values = []\n",
    "output = []\n",
    "for data in dataset :\n",
    "    ar = []\n",
    "    count = 0\n",
    "    ar.append(data['context_count'])   \n",
    "    ar.append(data['title_overlap'])\n",
    "#     ar.append(data['cue_count'])\n",
    "#     ar.append(sum(data['location_feature']))\n",
    "#     ar.append(sum(data['wcue_add']))\n",
    "#     ar.append(data['abstract_sim'])\n",
    "    ar.append(data['author_hindex'])\n",
    "    ar.append(data['author_prod'])\n",
    "#     ar.append(data['venue_cit'])\n",
    "    ar.append(data['popularity'])\n",
    "    ar.append(data['citation_count'])\n",
    "    ar.append(data['num_table'])\n",
    "    ar.extend(data['location_feature'])\n",
    "    ar.extend(data['lmp'])\n",
    "    ar.extend(data['wcue_max'])\n",
    "    ar.extend(data['bert_embed'])\n",
    "    values.append(ar)\n",
    "    if(data['label']=='baseline'):\n",
    "        output.append(1)\n",
    "    else :\n",
    "        output.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.array(values)\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5326, 320)\n"
     ]
    }
   ],
   "source": [
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std_scale = MinMaxScaler((0,1))\n",
    "# values[:,0:-6] = std_scale.fit_transform(values[:,0:-6])\n",
    "scaler = MinMaxScaler((0,1))\n",
    "values = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, output):\n",
    "    n = len(data)\n",
    "    last = int(0.8*n)\n",
    "    train_data = data[:last]\n",
    "    train_output = output[:last]\n",
    "    test_data = data[last:]\n",
    "    test_output = output[last:]\n",
    "    return train_data, test_data, train_output, test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_output, test_output = split(values, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(train_data, train_output) :\n",
    "    baselines = []\n",
    "    non_baselines = []\n",
    "    for i in range(len(train_output)) :\n",
    "        if(train_output[i]==1) :\n",
    "            baselines.append(train_data[i])\n",
    "        else :\n",
    "            non_baselines.append(train_data[i])\n",
    "    \n",
    "    n = len(baselines)\n",
    "    ar = np.random.choice(len(non_baselines), len(baselines))\n",
    "    nb_ar = []\n",
    "    for x in ar :\n",
    "        nb_ar.append(non_baselines[x])\n",
    "        \n",
    "    data = []\n",
    "    data.extend(nb_ar)\n",
    "    output = []\n",
    "    for i in range(len(data)):\n",
    "        output.append(0)\n",
    "    \n",
    "    data.extend(baselines)\n",
    "    for i in range(n) :\n",
    "        output.append(1)\n",
    "        \n",
    "    total_data = []\n",
    "    for i in range(len(data)):\n",
    "        ar = []\n",
    "        ar.append(data[i])\n",
    "        ar.append(output[i])\n",
    "        total_data.append(ar)\n",
    "        \n",
    "    total_data = np.array(total_data)\n",
    "    np.random.shuffle(total_data)\n",
    "    \n",
    "    data = []\n",
    "    output = []\n",
    "    for ar in total_data :\n",
    "        data.append(ar[0])\n",
    "        output.append(ar[1])\n",
    "        \n",
    "    data = np.array(data)\n",
    "    output = np.array(output)\n",
    "    \n",
    "    return data, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, train_output = shuffle(train_data, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87       522\n",
      "           1       0.89      0.84      0.87       544\n",
      "\n",
      "    accuracy                           0.87      1066\n",
      "   macro avg       0.87      0.87      0.87      1066\n",
      "weighted avg       0.87      0.87      0.87      1066\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92      2141\n",
      "           1       0.93      0.90      0.91      2119\n",
      "\n",
      "    accuracy                           0.91      4260\n",
      "   macro avg       0.91      0.91      0.91      4260\n",
      "weighted avg       0.91      0.91      0.91      4260\n",
      "\n",
      "{'C': 2, 'fit_intercept': False}\n",
      "[ 1.95e+00  1.64e+00 -1.02e+00  8.69e-01  7.12e-02 -5.39e-01  6.00e+00\n",
      "  6.34e-01  6.55e-01  6.55e+00 -1.00e-01 -1.20e+00 -2.10e+00 -7.69e-02\n",
      " -1.13e-01 -7.69e-02 -1.13e-01  3.24e-01  2.51e-03  1.23e+01  8.26e-01\n",
      " -1.58e-01  8.69e-01  1.70e+00 -9.42e-01 -2.32e-01  8.48e-02  1.44e+00\n",
      " -2.95e-01  5.84e-01  6.06e-01 -1.67e+00  8.26e-01 -5.68e-02  9.25e-02\n",
      "  1.00e+00  1.97e+00 -1.25e+00  2.54e-01  7.95e-01  1.91e-01  9.96e-01\n",
      "  6.22e-01  3.61e+00 -1.61e-02 -1.45e-01 -4.68e-01  3.21e-01 -1.06e+00\n",
      "  1.44e+00  5.59e-01  1.72e+00  6.75e-01  1.74e+00  8.42e-01  2.62e-01\n",
      "  6.99e-01  4.89e-02  9.71e-02  1.61e+00 -4.07e-01  7.49e-01 -6.08e-01\n",
      "  1.89e+00]\n",
      "12.299272318587061 19\n",
      "6.551112214656565 9\n",
      "5.998534944570543 6\n",
      "3.6143365917720485 43\n",
      "1.9749413595720156 36\n",
      "1.9544936904946173 0\n",
      "1.8922956226976797 63\n",
      "1.8164515639910388 183\n",
      "1.7742184807010053 77\n",
      "1.7575394161179687 157\n"
     ]
    }
   ],
   "source": [
    "params = {'C':[0.1, 0.5, 1, 2], 'fit_intercept':[True, False]}\n",
    "modelin = LogisticRegression(solver='lbfgs', max_iter=1e4, n_jobs=5, random_state=1, warm_start=False)\n",
    "model = GridSearchCV(modelin, params, cv=2, n_jobs=5)\n",
    "# model = LogisticRegression(solver='lbfgs',max_iter=1e4, n_jobs=5, random_state=1, warm_start=False, C=1, fit_intercept=True)\n",
    "clf = model.fit(train_data, train_output)\n",
    "predict_test = clf.predict(test_data)\n",
    "print(classification_report(test_output, predict_test))\n",
    "predictions = clf.predict(train_data)\n",
    "print(classification_report(train_output, predictions))\n",
    "print(model.best_params_)\n",
    "# print(type(model.best_estimator_))\n",
    "\n",
    "import copy\n",
    "normal = copy.deepcopy(clf.best_estimator_.coef_[0])\n",
    "coef = clf.best_estimator_.coef_[0][:64]\n",
    "print(coef)\n",
    "ar = clf.best_estimator_.coef_[0]\n",
    "ar.sort()\n",
    "# print(ar)\n",
    "\n",
    "for i in range(10) :\n",
    "    print(ar[len(ar)-1-i], list(normal).index(ar[len(ar)-1-i]))\n",
    "# print(clf.best_estimator_.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       522\n",
      "           1       0.89      0.85      0.87       544\n",
      "\n",
      "    accuracy                           0.87      1066\n",
      "   macro avg       0.87      0.87      0.87      1066\n",
      "weighted avg       0.87      0.87      0.87      1066\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "[[467  55]\n",
      " [ 80 464]]\n",
      "Normalized confusion matrix\n",
      "[[0.89 0.11]\n",
      " [0.15 0.85]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "params = {'C':[0.1, 0.5, 1, 2, 3], 'class_weight':[{0:1,1:2}, {0:1,1:3}, {0:1,1:4}, {0:1,1:5}]}\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma='scale', max_iter=1e4, C=1 )\n",
    "# model = GridSearchCV(rbf_svc, params, cv=2, n_jobs=5)\n",
    "clf = rbf_svc.fit(train_data, train_output)\n",
    "predict_test = clf.predict(test_data)\n",
    "print(classification_report(test_output, predict_test))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = ['non_baseline','baseline']\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(clf, test_data, test_output,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanisha17116/.local/lib/python3.6/site-packages/sklearn/tree/_classes.py:319: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.81       522\n",
      "           1       0.83      0.80      0.81       544\n",
      "\n",
      "    accuracy                           0.81      1066\n",
      "   macro avg       0.81      0.81      0.81      1066\n",
      "weighted avg       0.81      0.81      0.81      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'criterion':['entropy', 'gini'], 'splitter':['best', 'random'], 'min_samples_split':[2, 10, 20, 40],  'min_samples_leaf':[2, 10, 20, 40], 'max_depth':[20, 50, 100]}\n",
    "model = DecisionTreeClassifier(random_state=1, presort=True, criterion='entropy', max_depth=20,min_samples_leaf=2,min_samples_split=2,splitter='best')\n",
    "# clf = GridSearchCV(model, params, cv=2, n_jobs=5)\n",
    "clf = model.fit(train_data, train_output)\n",
    "predictions = clf.predict(test_data)\n",
    "print(classification_report(test_output, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       522\n",
      "           1       0.87      0.87      0.87       544\n",
      "\n",
      "    accuracy                           0.87      1066\n",
      "   macro avg       0.87      0.87      0.87      1066\n",
      "weighted avg       0.87      0.87      0.87      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=1, n_jobs=5, max_leaf_nodes=200, class_weight={0:1,1:5})\n",
    "clf = model.fit(train_data, train_output)\n",
    "predictions = clf.predict(test_data)\n",
    "print(classification_report(test_output, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_jobs=5, n_neighbors=5)\n",
    "clf = model.fit(train_data, train_output)\n",
    "predictions = clf.predict(test_data)\n",
    "print(classification_report(test_output, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "clf = model.fit(train_data, train_output)\n",
    "predictions = clf.predict(test_data)\n",
    "print(classification_report(test_output, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'hidden_layer_sizes':[(500, 100), (50,)], 'alpha':[10, 0.1, 0.001, 0.00001]}\n",
    "model = MLPClassifier(random_state=1, learning_rate='constant', batch_size=1500, max_iter=200, warm_start=True, hidden_layer_sizes=(500,100), alpha=0.01)\n",
    "#clf = GridSearchCV(model, params, cv=2, n_jobs=5)\n",
    "clf = model.fit(train_data, train_output)\n",
    "predictions = clf.predict(test_data)\n",
    "print(classification_report(test_output, predictions))\n",
    "#print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = model.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method, recal, correl, figur, origin, among, precis, fscore, procedur, score, origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef1 = coef[0][0:4]\n",
    "['among', 'precis', 'implement', 'maximum', 'experi', 'strategi', \n",
    " 'overal', 'previou', 'correl', 'calcul', 'achiev', 'gold', \n",
    " 'top', 'obtain', 'significantli', 'report', 'best', 'evalu', \n",
    " 'result', 'base', 'modifi', 'extend', 'metric', 'baselin', \n",
    " 'accord', 'perform', 'recal', 'standard', 'figur', 'comparison',\n",
    " 'yield', 'outperform', 'increas', 'stateoftheart', 'accuraci', 'method', \n",
    " 'procedur', 'score', 'origin', 'highest', 'signific', 'higher', \n",
    " 'fscore', 'compar']\n",
    "print(coef1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset :\n",
    "    print(len(data['wcue_max']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_as_nb = []\n",
    "nb_as_base = []\n",
    "base_as_base = []\n",
    "nb_as_nb = []\n",
    "\n",
    "for i in range(len(predict_test)):\n",
    "    testset[i].pop('fixed_context')\n",
    "    testset[i].pop('context')\n",
    "    testset[i].pop('bert_embed')\n",
    "    if(test_output[i]==1 and predict_test[i]==0) :\n",
    "        base_as_nb.append(testset[i])\n",
    "    if(test_output[i]==0 and predict_test[i]==1) :\n",
    "        nb_as_base.append(testset[i])\n",
    "    if(test_output[i]==1 and predict_test[i]==1) :\n",
    "        base_as_base.append(testset[i])\n",
    "    if(test_output[i]==0 and predict_test[i]==0) :\n",
    "        nb_as_nb.append(testset[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nb_as_base, open(\"nb_as_base.pkl\",\"wb\"))\n",
    "pickle.dump(base_as_nb, open(\"base_as_nb.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
