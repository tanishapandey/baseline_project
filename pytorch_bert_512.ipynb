{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tanisha17116/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import pickle\n",
    "import os\n",
    "import keras_bert\n",
    "import zipfile\n",
    "from sklearn.datasets import load_files\n",
    "import chardet\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch.nn.functional as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_to_utf8(encoding):\n",
    "    \"\"\"\n",
    "    standardize to utf-8 if necessary.\n",
    "    NOTE: mainly used to use utf-8 if ASCII is detected, as\n",
    "    BERT performance suffers otherwise.\n",
    "    \"\"\"\n",
    "    encoding = 'utf-8' if encoding.lower() in ['ascii', 'utf8', 'utf-8'] else encoding\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_names = [\"train\", \"test\"]\n",
    "datadir = \"dataset_up\"\n",
    "classes = [\"class0\", \"class1\"]\n",
    "train_str = train_test_names[0]\n",
    "train_b = load_files(os.path.join(datadir, train_str), shuffle=False, categories=classes)\n",
    "test_str = train_test_names[1]\n",
    "test_b = load_files(os.path.join(datadir,  test_str), shuffle=False, categories=classes)\n",
    "x_train = train_b.data\n",
    "y_train = train_b.target\n",
    "x_test = test_b.data\n",
    "y_test = test_b.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [chardet.detect(doc)['encoding'] for doc in x_train[:32]]\n",
    "encoding = max(set(lst), key=lst.count)\n",
    "encoding = standardize_to_utf8(encoding)\n",
    "x_train = [x.decode(encoding) for x in x_train]\n",
    "x_test = [x.decode(encoding) for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25788"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ids(x_train, x_test):\n",
    "    \n",
    "    token_tr = []\n",
    "    token_tst = []\n",
    "    count = 0\n",
    "    for sent in x_train :\n",
    "        tokens = tokenizer.encode(sent, add_special_tokens = True, max_length=512)\n",
    "        token_tr.append(tokens)\n",
    "        count+=1\n",
    "        if(count%1000==0):\n",
    "            print(count)\n",
    "    \n",
    "    for sent1 in x_test :\n",
    "        tokens1 = tokenizer.encode(sent1, add_special_tokens = True, max_length=512)\n",
    "        token_tst.append(tokens1)\n",
    "        count+=1\n",
    "        if(count%1000==0):\n",
    "            print(count)\n",
    "            \n",
    "    return token_tr, token_tst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "xtr_token, xtst_token = get_token_ids(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_token = pad_sequences(xtr_token, maxlen=512, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "xtst_token = pad_sequences(xtst_token, maxlen=512, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask_tr = []\n",
    "attention_mask_tst = []\n",
    "for sent in xtr_token:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_mask_tr.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in xtst_token:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_mask_tst.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(xtr_token, open(\"pickles/train_input_tokens.pkl\", \"wb\"))\n",
    "# pickle.dump(xtst_token, open(\"pickles/test_input_tokens.pkl\", \"wb\"))\n",
    "# pickle.dump(attention_mask_tr, open(\"pickles/attention_mask_train.pkl\" , \"wb\"))\n",
    "# pickle.dump(attention_mask_tst, open(\"pickles/attention_mask_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = torch.tensor(xtr_token)\n",
    "test_input = torch.tensor(xtst_token)\n",
    "\n",
    "train_label = torch.tensor(y_train)\n",
    "test_label = torch.tensor(y_test)\n",
    "\n",
    "train_mask = torch.tensor(attention_mask_tr)\n",
    "test_mask = torch.tensor(attention_mask_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32\n",
    "\n",
    "Train_data = TensorDataset(train_input, train_mask, train_label)\n",
    "Train_sampler = SequentialSampler(Train_data)\n",
    "Train_dataloader = DataLoader(Train_data, sampler=Train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "Test_data = TensorDataset(test_input, test_mask, test_label)\n",
    "Test_sampler = SequentialSampler(Test_data)\n",
    "Test_dataloader = DataLoader(Test_data, sampler=Test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_input, train_mask, train_label)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "test_data = TensorDataset(test_input, test_mask, test_label)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class ContextEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, freeze_bert = True):\n",
    "        super(ContextEmbeddings, self).__init__()\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "        self.embedding_layer = nn.Linear(768, 512)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding_layer.weight)\n",
    "        self.embedding_layer.bias.data.fill_(0.01)\n",
    "        self.cls_layer = nn.Linear(512, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.cls_layer.weight)\n",
    "        self.cls_layer.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
    "\n",
    "        #Obtaining the representation of [CLS] head\n",
    "        embedding_rep = cont_reps[:, 0]\n",
    "        embeds = self.embedding_layer(embedding_rep)\n",
    "        embeds = func.relu(embeds)\n",
    "#         cls_rep = embeds[:,0]\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(embeds)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ContextEmbeddings(freeze_bert = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, opti, train_loader, test_loader, max_eps):\n",
    "\n",
    "    for ep in range(max_eps):\n",
    "        \n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "\n",
    "            if (it + 1) % 1 == 0:\n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss : {} Accuracy : {}\".format(it+1, ep+1, loss.item(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of epoch 1 complete. Loss : 0.5709898471832275 Accuracy : 0.875\n",
      "Iteration 2 of epoch 1 complete. Loss : 0.5330638885498047 Accuracy : 0.9375\n",
      "Iteration 3 of epoch 1 complete. Loss : 0.5297664403915405 Accuracy : 0.9375\n",
      "Iteration 4 of epoch 1 complete. Loss : 0.5648213624954224 Accuracy : 0.84375\n",
      "Iteration 5 of epoch 1 complete. Loss : 0.4747810661792755 Accuracy : 0.96875\n",
      "Iteration 6 of epoch 1 complete. Loss : 0.46808508038520813 Accuracy : 0.96875\n",
      "Iteration 7 of epoch 1 complete. Loss : 0.513116180896759 Accuracy : 0.84375\n",
      "Iteration 8 of epoch 1 complete. Loss : 0.4571857154369354 Accuracy : 0.9375\n",
      "Iteration 9 of epoch 1 complete. Loss : 0.4486050307750702 Accuracy : 0.9375\n",
      "Iteration 10 of epoch 1 complete. Loss : 0.4029240906238556 Accuracy : 0.96875\n",
      "Iteration 11 of epoch 1 complete. Loss : 0.42757290601730347 Accuracy : 0.9375\n",
      "Iteration 12 of epoch 1 complete. Loss : 0.4493497610092163 Accuracy : 0.90625\n",
      "Iteration 13 of epoch 1 complete. Loss : 0.40939217805862427 Accuracy : 0.9375\n",
      "Iteration 14 of epoch 1 complete. Loss : 0.5155276656150818 Accuracy : 0.8125\n",
      "Iteration 15 of epoch 1 complete. Loss : 0.5276868343353271 Accuracy : 0.8125\n",
      "Iteration 16 of epoch 1 complete. Loss : 0.3167104721069336 Accuracy : 1.0\n",
      "Iteration 17 of epoch 1 complete. Loss : 0.4393256902694702 Accuracy : 0.875\n",
      "Iteration 18 of epoch 1 complete. Loss : 0.36703526973724365 Accuracy : 0.9375\n",
      "Iteration 19 of epoch 1 complete. Loss : 0.4096021056175232 Accuracy : 0.875\n",
      "Iteration 20 of epoch 1 complete. Loss : 0.38883963227272034 Accuracy : 0.90625\n",
      "Iteration 21 of epoch 1 complete. Loss : 0.3866775631904602 Accuracy : 0.90625\n",
      "Iteration 22 of epoch 1 complete. Loss : 0.4861544370651245 Accuracy : 0.8125\n",
      "Iteration 23 of epoch 1 complete. Loss : 0.4145689010620117 Accuracy : 0.875\n",
      "Iteration 24 of epoch 1 complete. Loss : 0.3293123245239258 Accuracy : 0.9375\n",
      "Iteration 25 of epoch 1 complete. Loss : 0.40164801478385925 Accuracy : 0.875\n",
      "Iteration 26 of epoch 1 complete. Loss : 0.37294837832450867 Accuracy : 0.90625\n",
      "Iteration 27 of epoch 1 complete. Loss : 0.23786801099777222 Accuracy : 1.0\n",
      "Iteration 28 of epoch 1 complete. Loss : 0.2756318747997284 Accuracy : 0.96875\n",
      "Iteration 29 of epoch 1 complete. Loss : 0.34809035062789917 Accuracy : 0.90625\n",
      "Iteration 30 of epoch 1 complete. Loss : 0.2529040575027466 Accuracy : 0.96875\n",
      "Iteration 31 of epoch 1 complete. Loss : 0.2142028510570526 Accuracy : 1.0\n",
      "Iteration 32 of epoch 1 complete. Loss : 0.4385055899620056 Accuracy : 0.84375\n",
      "Iteration 33 of epoch 1 complete. Loss : 0.3457131087779999 Accuracy : 0.90625\n",
      "Iteration 34 of epoch 1 complete. Loss : 0.24509358406066895 Accuracy : 0.96875\n",
      "Iteration 35 of epoch 1 complete. Loss : 0.4957178831100464 Accuracy : 0.8125\n",
      "Iteration 36 of epoch 1 complete. Loss : 0.444083034992218 Accuracy : 0.84375\n",
      "Iteration 37 of epoch 1 complete. Loss : 0.3357636332511902 Accuracy : 0.90625\n",
      "Iteration 38 of epoch 1 complete. Loss : 0.3244748115539551 Accuracy : 0.90625\n",
      "Iteration 39 of epoch 1 complete. Loss : 0.443501353263855 Accuracy : 0.84375\n",
      "Iteration 40 of epoch 1 complete. Loss : 0.33550959825515747 Accuracy : 0.90625\n",
      "Iteration 41 of epoch 1 complete. Loss : 0.450271874666214 Accuracy : 0.84375\n",
      "Iteration 42 of epoch 1 complete. Loss : 0.31681495904922485 Accuracy : 0.90625\n",
      "Iteration 43 of epoch 1 complete. Loss : 0.2213761955499649 Accuracy : 0.96875\n",
      "Iteration 44 of epoch 1 complete. Loss : 0.3522612452507019 Accuracy : 0.90625\n",
      "Iteration 45 of epoch 1 complete. Loss : 0.6017367839813232 Accuracy : 0.75\n",
      "Iteration 46 of epoch 1 complete. Loss : 0.3315894305706024 Accuracy : 0.90625\n",
      "Iteration 47 of epoch 1 complete. Loss : 0.2679882049560547 Accuracy : 0.9375\n",
      "Iteration 48 of epoch 1 complete. Loss : 0.3168215751647949 Accuracy : 0.90625\n",
      "Iteration 49 of epoch 1 complete. Loss : 0.26871564984321594 Accuracy : 0.9375\n",
      "Iteration 50 of epoch 1 complete. Loss : 0.3038966655731201 Accuracy : 0.90625\n",
      "Iteration 51 of epoch 1 complete. Loss : 0.51338791847229 Accuracy : 0.8125\n",
      "Iteration 52 of epoch 1 complete. Loss : 0.31001347303390503 Accuracy : 0.90625\n",
      "Iteration 53 of epoch 1 complete. Loss : 0.2667509615421295 Accuracy : 0.9375\n",
      "Iteration 54 of epoch 1 complete. Loss : 0.2650071382522583 Accuracy : 0.9375\n",
      "Iteration 55 of epoch 1 complete. Loss : 0.19694526493549347 Accuracy : 0.96875\n",
      "Iteration 56 of epoch 1 complete. Loss : 0.2629052400588989 Accuracy : 0.9375\n",
      "Iteration 57 of epoch 1 complete. Loss : 0.4391604959964752 Accuracy : 0.84375\n",
      "Iteration 59 of epoch 1 complete. Loss : 0.3872598111629486 Accuracy : 0.875\n",
      "Iteration 60 of epoch 1 complete. Loss : 0.41458237171173096 Accuracy : 0.84375\n",
      "Iteration 61 of epoch 1 complete. Loss : 0.2485252469778061 Accuracy : 0.9375\n",
      "Iteration 62 of epoch 1 complete. Loss : 0.201180562376976 Accuracy : 0.96875\n",
      "Iteration 63 of epoch 1 complete. Loss : 0.1371949017047882 Accuracy : 1.0\n",
      "Iteration 64 of epoch 1 complete. Loss : 0.18942607939243317 Accuracy : 0.96875\n",
      "Iteration 65 of epoch 1 complete. Loss : 0.38467276096343994 Accuracy : 0.875\n",
      "Iteration 66 of epoch 1 complete. Loss : 0.20018354058265686 Accuracy : 0.96875\n",
      "Iteration 67 of epoch 1 complete. Loss : 0.2432495504617691 Accuracy : 0.9375\n",
      "Iteration 68 of epoch 1 complete. Loss : 0.1880636215209961 Accuracy : 0.96875\n",
      "Iteration 69 of epoch 1 complete. Loss : 0.25342443585395813 Accuracy : 0.9375\n",
      "Iteration 70 of epoch 1 complete. Loss : 0.38241317868232727 Accuracy : 0.875\n",
      "Iteration 71 of epoch 1 complete. Loss : 0.36524367332458496 Accuracy : 0.875\n",
      "Iteration 72 of epoch 1 complete. Loss : 0.5811489820480347 Accuracy : 0.78125\n",
      "Iteration 73 of epoch 1 complete. Loss : 0.3716198205947876 Accuracy : 0.875\n",
      "Iteration 74 of epoch 1 complete. Loss : 0.30215466022491455 Accuracy : 0.90625\n",
      "Iteration 75 of epoch 1 complete. Loss : 0.45644333958625793 Accuracy : 0.84375\n",
      "Iteration 76 of epoch 1 complete. Loss : 0.5729477405548096 Accuracy : 0.78125\n",
      "Iteration 77 of epoch 1 complete. Loss : 0.35377123951911926 Accuracy : 0.875\n",
      "Iteration 78 of epoch 1 complete. Loss : 0.5915148854255676 Accuracy : 0.78125\n",
      "Iteration 79 of epoch 1 complete. Loss : 0.313344269990921 Accuracy : 0.90625\n",
      "Iteration 80 of epoch 1 complete. Loss : 0.5123592615127563 Accuracy : 0.8125\n",
      "Iteration 81 of epoch 1 complete. Loss : 0.3749796450138092 Accuracy : 0.875\n",
      "Iteration 82 of epoch 1 complete. Loss : 0.38076579570770264 Accuracy : 0.875\n",
      "Iteration 83 of epoch 1 complete. Loss : 0.32854706048965454 Accuracy : 0.90625\n",
      "Iteration 84 of epoch 1 complete. Loss : 0.3710247278213501 Accuracy : 0.875\n",
      "Iteration 85 of epoch 1 complete. Loss : 0.25309062004089355 Accuracy : 0.9375\n",
      "Iteration 86 of epoch 1 complete. Loss : 0.2479386031627655 Accuracy : 0.9375\n",
      "Iteration 87 of epoch 1 complete. Loss : 0.2986154556274414 Accuracy : 0.90625\n",
      "Iteration 88 of epoch 1 complete. Loss : 0.11985256522893906 Accuracy : 1.0\n",
      "Iteration 89 of epoch 1 complete. Loss : 0.30993276834487915 Accuracy : 0.90625\n",
      "Iteration 90 of epoch 1 complete. Loss : 0.5258896946907043 Accuracy : 0.8125\n",
      "Iteration 91 of epoch 1 complete. Loss : 0.30564364790916443 Accuracy : 0.90625\n",
      "Iteration 92 of epoch 1 complete. Loss : 0.4471171498298645 Accuracy : 0.84375\n",
      "Iteration 93 of epoch 1 complete. Loss : 0.25159522891044617 Accuracy : 0.9375\n",
      "Iteration 94 of epoch 1 complete. Loss : 0.3195068836212158 Accuracy : 0.90625\n",
      "Iteration 95 of epoch 1 complete. Loss : 0.2389577031135559 Accuracy : 0.9375\n",
      "Iteration 96 of epoch 1 complete. Loss : 0.39351239800453186 Accuracy : 0.875\n",
      "Iteration 97 of epoch 1 complete. Loss : 0.4322124719619751 Accuracy : 0.84375\n",
      "Iteration 98 of epoch 1 complete. Loss : 0.3091893792152405 Accuracy : 0.90625\n",
      "Iteration 99 of epoch 1 complete. Loss : 0.39207208156585693 Accuracy : 0.875\n",
      "Iteration 100 of epoch 1 complete. Loss : 0.31154847145080566 Accuracy : 0.90625\n",
      "Iteration 101 of epoch 1 complete. Loss : 0.3818114697933197 Accuracy : 0.875\n",
      "Iteration 102 of epoch 1 complete. Loss : 0.16380900144577026 Accuracy : 0.96875\n",
      "Iteration 103 of epoch 1 complete. Loss : 0.24516624212265015 Accuracy : 0.9375\n",
      "Iteration 104 of epoch 1 complete. Loss : 0.37566614151000977 Accuracy : 0.875\n",
      "Iteration 105 of epoch 1 complete. Loss : 0.18507352471351624 Accuracy : 0.96875\n",
      "Iteration 106 of epoch 1 complete. Loss : 0.12124188989400864 Accuracy : 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 107 of epoch 1 complete. Loss : 0.3242058753967285 Accuracy : 0.90625\n",
      "Iteration 108 of epoch 1 complete. Loss : 0.31794828176498413 Accuracy : 0.90625\n",
      "Iteration 109 of epoch 1 complete. Loss : 0.25826749205589294 Accuracy : 0.9375\n",
      "Iteration 110 of epoch 1 complete. Loss : 0.461330771446228 Accuracy : 0.84375\n",
      "Iteration 111 of epoch 1 complete. Loss : 0.524827241897583 Accuracy : 0.8125\n",
      "Iteration 112 of epoch 1 complete. Loss : 0.4413797855377197 Accuracy : 0.84375\n",
      "Iteration 113 of epoch 1 complete. Loss : 0.2994282841682434 Accuracy : 0.90625\n",
      "Iteration 114 of epoch 1 complete. Loss : 0.2582840323448181 Accuracy : 0.9375\n",
      "Iteration 115 of epoch 1 complete. Loss : 0.23391863703727722 Accuracy : 0.9375\n",
      "Iteration 116 of epoch 1 complete. Loss : 0.24176079034805298 Accuracy : 0.9375\n",
      "Iteration 117 of epoch 1 complete. Loss : 0.5358160138130188 Accuracy : 0.8125\n",
      "Iteration 118 of epoch 1 complete. Loss : 0.4345898926258087 Accuracy : 0.84375\n",
      "Iteration 119 of epoch 1 complete. Loss : 0.23881137371063232 Accuracy : 0.9375\n",
      "Iteration 120 of epoch 1 complete. Loss : 0.1129247322678566 Accuracy : 1.0\n",
      "Iteration 121 of epoch 1 complete. Loss : 0.2922341525554657 Accuracy : 0.90625\n",
      "Iteration 122 of epoch 1 complete. Loss : 0.2613183856010437 Accuracy : 0.9375\n",
      "Iteration 123 of epoch 1 complete. Loss : 0.2672017216682434 Accuracy : 0.9375\n",
      "Iteration 124 of epoch 1 complete. Loss : 0.3157831132411957 Accuracy : 0.90625\n",
      "Iteration 125 of epoch 1 complete. Loss : 0.10919363796710968 Accuracy : 1.0\n",
      "Iteration 126 of epoch 1 complete. Loss : 0.438597172498703 Accuracy : 0.84375\n",
      "Iteration 127 of epoch 1 complete. Loss : 0.25215938687324524 Accuracy : 0.9375\n",
      "Iteration 128 of epoch 1 complete. Loss : 0.3728736937046051 Accuracy : 0.875\n",
      "Iteration 129 of epoch 1 complete. Loss : 0.23789523541927338 Accuracy : 0.9375\n",
      "Iteration 130 of epoch 1 complete. Loss : 0.24155542254447937 Accuracy : 0.9375\n",
      "Iteration 131 of epoch 1 complete. Loss : 0.30260393023490906 Accuracy : 0.90625\n",
      "Iteration 132 of epoch 1 complete. Loss : 0.11288635432720184 Accuracy : 1.0\n",
      "Iteration 133 of epoch 1 complete. Loss : 0.3282868564128876 Accuracy : 0.90625\n",
      "Iteration 134 of epoch 1 complete. Loss : 0.29558858275413513 Accuracy : 0.90625\n",
      "Iteration 135 of epoch 1 complete. Loss : 0.45715728402137756 Accuracy : 0.84375\n",
      "Iteration 136 of epoch 1 complete. Loss : 0.3735975921154022 Accuracy : 0.875\n",
      "Iteration 137 of epoch 1 complete. Loss : 0.22068333625793457 Accuracy : 0.9375\n",
      "Iteration 138 of epoch 1 complete. Loss : 0.1784692257642746 Accuracy : 0.96875\n",
      "Iteration 139 of epoch 1 complete. Loss : 0.2629898190498352 Accuracy : 0.9375\n",
      "Iteration 140 of epoch 1 complete. Loss : 0.3714022636413574 Accuracy : 0.875\n",
      "Iteration 141 of epoch 1 complete. Loss : 0.36755475401878357 Accuracy : 0.875\n",
      "Iteration 142 of epoch 1 complete. Loss : 0.3612498939037323 Accuracy : 0.875\n",
      "Iteration 143 of epoch 1 complete. Loss : 0.1770860254764557 Accuracy : 0.96875\n",
      "Iteration 144 of epoch 1 complete. Loss : 0.17106091976165771 Accuracy : 0.96875\n",
      "Iteration 145 of epoch 1 complete. Loss : 0.30409181118011475 Accuracy : 0.90625\n",
      "Iteration 146 of epoch 1 complete. Loss : 0.22395263612270355 Accuracy : 0.9375\n",
      "Iteration 147 of epoch 1 complete. Loss : 0.42398691177368164 Accuracy : 0.84375\n",
      "Iteration 148 of epoch 1 complete. Loss : 0.45121997594833374 Accuracy : 0.84375\n",
      "Iteration 149 of epoch 1 complete. Loss : 0.37626540660858154 Accuracy : 0.875\n",
      "Iteration 150 of epoch 1 complete. Loss : 0.35291919112205505 Accuracy : 0.875\n",
      "Iteration 151 of epoch 1 complete. Loss : 0.389909565448761 Accuracy : 0.875\n",
      "Iteration 152 of epoch 1 complete. Loss : 0.39806243777275085 Accuracy : 0.875\n",
      "Iteration 153 of epoch 1 complete. Loss : 0.3855198919773102 Accuracy : 0.875\n",
      "Iteration 154 of epoch 1 complete. Loss : 0.5260694026947021 Accuracy : 0.8125\n",
      "Iteration 155 of epoch 1 complete. Loss : 0.31027209758758545 Accuracy : 0.90625\n",
      "Iteration 156 of epoch 1 complete. Loss : 0.24686430394649506 Accuracy : 0.9375\n",
      "Iteration 157 of epoch 1 complete. Loss : 0.17173174023628235 Accuracy : 0.96875\n",
      "Iteration 158 of epoch 1 complete. Loss : 0.16914628446102142 Accuracy : 0.96875\n",
      "Iteration 159 of epoch 1 complete. Loss : 0.23616474866867065 Accuracy : 0.9375\n",
      "Iteration 160 of epoch 1 complete. Loss : 0.4358829855918884 Accuracy : 0.84375\n",
      "Iteration 161 of epoch 1 complete. Loss : 0.4325258731842041 Accuracy : 0.84375\n",
      "Iteration 162 of epoch 1 complete. Loss : 0.3287906348705292 Accuracy : 0.90625\n",
      "Iteration 163 of epoch 1 complete. Loss : 0.3871294856071472 Accuracy : 0.875\n",
      "Iteration 164 of epoch 1 complete. Loss : 0.3664501905441284 Accuracy : 0.875\n",
      "Iteration 165 of epoch 1 complete. Loss : 0.37222182750701904 Accuracy : 0.875\n",
      "Iteration 166 of epoch 1 complete. Loss : 0.25513625144958496 Accuracy : 0.9375\n",
      "Iteration 167 of epoch 1 complete. Loss : 0.30231159925460815 Accuracy : 0.90625\n",
      "Iteration 168 of epoch 1 complete. Loss : 0.24374818801879883 Accuracy : 0.9375\n",
      "Iteration 169 of epoch 1 complete. Loss : 0.39250442385673523 Accuracy : 0.875\n",
      "Iteration 170 of epoch 1 complete. Loss : 0.3180703818798065 Accuracy : 0.90625\n",
      "Iteration 171 of epoch 1 complete. Loss : 0.36516210436820984 Accuracy : 0.875\n",
      "Iteration 172 of epoch 1 complete. Loss : 0.3096909523010254 Accuracy : 0.90625\n",
      "Iteration 173 of epoch 1 complete. Loss : 0.31821754574775696 Accuracy : 0.90625\n",
      "Iteration 174 of epoch 1 complete. Loss : 0.22784534096717834 Accuracy : 0.9375\n",
      "Iteration 175 of epoch 1 complete. Loss : 0.2118406742811203 Accuracy : 0.9375\n",
      "Iteration 176 of epoch 1 complete. Loss : 0.4928414821624756 Accuracy : 0.8125\n",
      "Iteration 177 of epoch 1 complete. Loss : 0.24879029393196106 Accuracy : 0.9375\n",
      "Iteration 178 of epoch 1 complete. Loss : 0.21405582129955292 Accuracy : 0.9375\n",
      "Iteration 179 of epoch 1 complete. Loss : 0.32003822922706604 Accuracy : 0.90625\n",
      "Iteration 180 of epoch 1 complete. Loss : 0.10920385271310806 Accuracy : 1.0\n",
      "Iteration 181 of epoch 1 complete. Loss : 0.38587892055511475 Accuracy : 0.875\n",
      "Iteration 182 of epoch 1 complete. Loss : 0.3181127607822418 Accuracy : 0.90625\n",
      "Iteration 183 of epoch 1 complete. Loss : 0.3195234537124634 Accuracy : 0.90625\n",
      "Iteration 184 of epoch 1 complete. Loss : 0.3073348104953766 Accuracy : 0.90625\n",
      "Iteration 185 of epoch 1 complete. Loss : 0.30096378922462463 Accuracy : 0.90625\n",
      "Iteration 186 of epoch 1 complete. Loss : 0.2948452830314636 Accuracy : 0.90625\n",
      "Iteration 187 of epoch 1 complete. Loss : 0.18096275627613068 Accuracy : 0.96875\n",
      "Iteration 188 of epoch 1 complete. Loss : 0.35858461260795593 Accuracy : 0.875\n",
      "Iteration 189 of epoch 1 complete. Loss : 0.3691404461860657 Accuracy : 0.875\n",
      "Iteration 190 of epoch 1 complete. Loss : 0.235396608710289 Accuracy : 0.9375\n",
      "Iteration 191 of epoch 1 complete. Loss : 0.33012354373931885 Accuracy : 0.90625\n",
      "Iteration 192 of epoch 1 complete. Loss : 0.44666481018066406 Accuracy : 0.84375\n",
      "Iteration 193 of epoch 1 complete. Loss : 0.23817497491836548 Accuracy : 0.9375\n",
      "Iteration 194 of epoch 1 complete. Loss : 0.39978981018066406 Accuracy : 0.875\n",
      "Iteration 195 of epoch 1 complete. Loss : 0.1773003339767456 Accuracy : 0.96875\n",
      "Iteration 196 of epoch 1 complete. Loss : 0.4067571759223938 Accuracy : 0.875\n",
      "Iteration 197 of epoch 1 complete. Loss : 0.30738571286201477 Accuracy : 0.90625\n",
      "Iteration 198 of epoch 1 complete. Loss : 0.10257060080766678 Accuracy : 1.0\n",
      "Iteration 199 of epoch 1 complete. Loss : 0.23302388191223145 Accuracy : 0.9375\n",
      "Iteration 200 of epoch 1 complete. Loss : 0.3023413419723511 Accuracy : 0.90625\n",
      "Iteration 201 of epoch 1 complete. Loss : 0.3546401858329773 Accuracy : 0.875\n",
      "Iteration 202 of epoch 1 complete. Loss : 0.24105709791183472 Accuracy : 0.9375\n",
      "Iteration 203 of epoch 1 complete. Loss : 0.31258776783943176 Accuracy : 0.90625\n",
      "Iteration 204 of epoch 1 complete. Loss : 0.3774225115776062 Accuracy : 0.875\n",
      "Iteration 205 of epoch 1 complete. Loss : 0.1638580858707428 Accuracy : 0.96875\n",
      "Iteration 206 of epoch 1 complete. Loss : 0.24050134420394897 Accuracy : 0.9375\n",
      "Iteration 207 of epoch 1 complete. Loss : 0.2333870381116867 Accuracy : 0.9375\n",
      "Iteration 208 of epoch 1 complete. Loss : 0.38308247923851013 Accuracy : 0.875\n",
      "Iteration 209 of epoch 1 complete. Loss : 0.30511918663978577 Accuracy : 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 210 of epoch 1 complete. Loss : 0.3752800226211548 Accuracy : 0.875\n",
      "Iteration 211 of epoch 1 complete. Loss : 0.4962935745716095 Accuracy : 0.8125\n",
      "Iteration 212 of epoch 1 complete. Loss : 0.246051624417305 Accuracy : 0.9375\n",
      "Iteration 213 of epoch 1 complete. Loss : 0.2294645607471466 Accuracy : 0.9375\n",
      "Iteration 214 of epoch 1 complete. Loss : 0.3727647066116333 Accuracy : 0.875\n",
      "Iteration 215 of epoch 1 complete. Loss : 0.3658784329891205 Accuracy : 0.875\n",
      "Iteration 216 of epoch 1 complete. Loss : 0.18039849400520325 Accuracy : 0.96875\n",
      "Iteration 217 of epoch 1 complete. Loss : 0.16866834461688995 Accuracy : 0.96875\n",
      "Iteration 218 of epoch 1 complete. Loss : 0.09776192903518677 Accuracy : 1.0\n",
      "Iteration 219 of epoch 1 complete. Loss : 0.45689594745635986 Accuracy : 0.84375\n",
      "Iteration 220 of epoch 1 complete. Loss : 0.24304978549480438 Accuracy : 0.9375\n",
      "Iteration 221 of epoch 1 complete. Loss : 0.3737296164035797 Accuracy : 0.875\n",
      "Iteration 222 of epoch 1 complete. Loss : 0.5591159462928772 Accuracy : 0.78125\n",
      "Iteration 223 of epoch 1 complete. Loss : 0.31296342611312866 Accuracy : 0.90625\n",
      "Iteration 224 of epoch 1 complete. Loss : 0.37267035245895386 Accuracy : 0.875\n",
      "Iteration 225 of epoch 1 complete. Loss : 0.43377813696861267 Accuracy : 0.84375\n",
      "Iteration 226 of epoch 1 complete. Loss : 0.09963861852884293 Accuracy : 1.0\n",
      "Iteration 227 of epoch 1 complete. Loss : 0.3357682228088379 Accuracy : 0.875\n",
      "Iteration 228 of epoch 1 complete. Loss : 0.23929908871650696 Accuracy : 0.9375\n",
      "Iteration 229 of epoch 1 complete. Loss : 0.3905336558818817 Accuracy : 0.875\n",
      "Iteration 230 of epoch 1 complete. Loss : 0.22656185925006866 Accuracy : 0.9375\n",
      "Iteration 231 of epoch 1 complete. Loss : 0.37546342611312866 Accuracy : 0.875\n",
      "Iteration 232 of epoch 1 complete. Loss : 0.24815993010997772 Accuracy : 0.9375\n",
      "Iteration 233 of epoch 1 complete. Loss : 0.24410563707351685 Accuracy : 0.9375\n",
      "Iteration 234 of epoch 1 complete. Loss : 0.30720067024230957 Accuracy : 0.90625\n",
      "Iteration 235 of epoch 1 complete. Loss : 0.3594343960285187 Accuracy : 0.875\n",
      "Iteration 236 of epoch 1 complete. Loss : 0.2988119125366211 Accuracy : 0.90625\n",
      "Iteration 237 of epoch 1 complete. Loss : 0.31571799516677856 Accuracy : 0.90625\n",
      "Iteration 238 of epoch 1 complete. Loss : 0.22955745458602905 Accuracy : 0.9375\n",
      "Iteration 239 of epoch 1 complete. Loss : 0.16676048934459686 Accuracy : 0.96875\n",
      "Iteration 240 of epoch 1 complete. Loss : 0.32729679346084595 Accuracy : 0.90625\n",
      "Iteration 241 of epoch 1 complete. Loss : 0.6685389280319214 Accuracy : 0.75\n",
      "Iteration 242 of epoch 1 complete. Loss : 0.23432868719100952 Accuracy : 0.9375\n",
      "Iteration 243 of epoch 1 complete. Loss : 0.42971083521842957 Accuracy : 0.84375\n",
      "Iteration 244 of epoch 1 complete. Loss : 0.17054589092731476 Accuracy : 0.96875\n",
      "Iteration 245 of epoch 1 complete. Loss : 0.17579954862594604 Accuracy : 0.96875\n",
      "Iteration 246 of epoch 1 complete. Loss : 0.33968937397003174 Accuracy : 0.875\n",
      "Iteration 247 of epoch 1 complete. Loss : 0.44913017749786377 Accuracy : 0.84375\n",
      "Iteration 248 of epoch 1 complete. Loss : 0.37791740894317627 Accuracy : 0.875\n",
      "Iteration 249 of epoch 1 complete. Loss : 0.4078778624534607 Accuracy : 0.84375\n",
      "Iteration 250 of epoch 1 complete. Loss : 0.2680279612541199 Accuracy : 0.90625\n",
      "Iteration 251 of epoch 1 complete. Loss : 0.31419309973716736 Accuracy : 0.90625\n",
      "Iteration 253 of epoch 1 complete. Loss : 0.317414790391922 Accuracy : 0.90625\n",
      "Iteration 254 of epoch 1 complete. Loss : 0.23114676773548126 Accuracy : 0.9375\n",
      "Iteration 255 of epoch 1 complete. Loss : 0.16326986253261566 Accuracy : 0.96875\n",
      "Iteration 256 of epoch 1 complete. Loss : 0.1652313470840454 Accuracy : 0.96875\n",
      "Iteration 257 of epoch 1 complete. Loss : 0.3990217447280884 Accuracy : 0.84375\n",
      "Iteration 258 of epoch 1 complete. Loss : 0.18052497506141663 Accuracy : 0.96875\n",
      "Iteration 259 of epoch 1 complete. Loss : 0.2305913120508194 Accuracy : 0.9375\n",
      "Iteration 260 of epoch 1 complete. Loss : 0.37867867946624756 Accuracy : 0.875\n",
      "Iteration 261 of epoch 1 complete. Loss : 0.3802594840526581 Accuracy : 0.875\n",
      "Iteration 262 of epoch 1 complete. Loss : 0.3081458508968353 Accuracy : 0.90625\n",
      "Iteration 263 of epoch 1 complete. Loss : 0.2273203283548355 Accuracy : 0.9375\n",
      "Iteration 264 of epoch 1 complete. Loss : 0.3303682506084442 Accuracy : 0.875\n",
      "Iteration 265 of epoch 1 complete. Loss : 0.34882786870002747 Accuracy : 0.875\n",
      "Iteration 266 of epoch 1 complete. Loss : 0.2447257786989212 Accuracy : 0.9375\n",
      "Iteration 267 of epoch 1 complete. Loss : 0.4380073845386505 Accuracy : 0.84375\n",
      "Iteration 268 of epoch 1 complete. Loss : 0.31755560636520386 Accuracy : 0.90625\n",
      "Iteration 269 of epoch 1 complete. Loss : 0.2308427095413208 Accuracy : 0.9375\n",
      "Iteration 270 of epoch 1 complete. Loss : 0.17022189497947693 Accuracy : 0.96875\n",
      "Iteration 271 of epoch 1 complete. Loss : 0.4492347240447998 Accuracy : 0.84375\n",
      "Iteration 272 of epoch 1 complete. Loss : 0.3090028166770935 Accuracy : 0.90625\n",
      "Iteration 273 of epoch 1 complete. Loss : 0.10248369723558426 Accuracy : 1.0\n",
      "Iteration 274 of epoch 1 complete. Loss : 0.4087505340576172 Accuracy : 0.84375\n",
      "Iteration 275 of epoch 1 complete. Loss : 0.41639629006385803 Accuracy : 0.84375\n",
      "Iteration 276 of epoch 1 complete. Loss : 0.22744977474212646 Accuracy : 0.9375\n",
      "Iteration 277 of epoch 1 complete. Loss : 0.37127014994621277 Accuracy : 0.875\n",
      "Iteration 278 of epoch 1 complete. Loss : 0.5231155157089233 Accuracy : 0.8125\n",
      "Iteration 279 of epoch 1 complete. Loss : 0.3744742274284363 Accuracy : 0.875\n",
      "Iteration 280 of epoch 1 complete. Loss : 0.2578738331794739 Accuracy : 0.90625\n",
      "Iteration 281 of epoch 1 complete. Loss : 0.0993790328502655 Accuracy : 1.0\n",
      "Iteration 282 of epoch 1 complete. Loss : 0.3489948511123657 Accuracy : 0.875\n",
      "Iteration 283 of epoch 1 complete. Loss : 0.17444373667240143 Accuracy : 0.96875\n",
      "Iteration 284 of epoch 1 complete. Loss : 0.15569813549518585 Accuracy : 0.96875\n",
      "Iteration 285 of epoch 1 complete. Loss : 0.48528286814689636 Accuracy : 0.8125\n",
      "Iteration 286 of epoch 1 complete. Loss : 0.31832587718963623 Accuracy : 0.90625\n",
      "Iteration 287 of epoch 1 complete. Loss : 0.22584941983222961 Accuracy : 0.9375\n",
      "Iteration 288 of epoch 1 complete. Loss : 0.16897480189800262 Accuracy : 0.96875\n",
      "Iteration 289 of epoch 1 complete. Loss : 0.3845239579677582 Accuracy : 0.875\n",
      "Iteration 290 of epoch 1 complete. Loss : 0.2113540917634964 Accuracy : 0.9375\n",
      "Iteration 291 of epoch 1 complete. Loss : 0.23392190039157867 Accuracy : 0.9375\n",
      "Iteration 292 of epoch 1 complete. Loss : 0.36297357082366943 Accuracy : 0.875\n",
      "Iteration 293 of epoch 1 complete. Loss : 0.38028496503829956 Accuracy : 0.875\n",
      "Iteration 294 of epoch 1 complete. Loss : 0.32206907868385315 Accuracy : 0.875\n",
      "Iteration 295 of epoch 1 complete. Loss : 0.24451279640197754 Accuracy : 0.9375\n",
      "Iteration 296 of epoch 1 complete. Loss : 0.30425190925598145 Accuracy : 0.90625\n",
      "Iteration 297 of epoch 1 complete. Loss : 0.22737905383110046 Accuracy : 0.9375\n",
      "Iteration 298 of epoch 1 complete. Loss : 0.2908344268798828 Accuracy : 0.90625\n",
      "Iteration 299 of epoch 1 complete. Loss : 0.41553160548210144 Accuracy : 0.84375\n",
      "Iteration 300 of epoch 1 complete. Loss : 0.33184128999710083 Accuracy : 0.90625\n",
      "Iteration 301 of epoch 1 complete. Loss : 0.3170301616191864 Accuracy : 0.90625\n",
      "Iteration 302 of epoch 1 complete. Loss : 0.43737539649009705 Accuracy : 0.84375\n",
      "Iteration 303 of epoch 1 complete. Loss : 0.34342366456985474 Accuracy : 0.875\n",
      "Iteration 304 of epoch 1 complete. Loss : 0.35720521211624146 Accuracy : 0.875\n",
      "Iteration 305 of epoch 1 complete. Loss : 0.2303887903690338 Accuracy : 0.9375\n",
      "Iteration 306 of epoch 1 complete. Loss : 0.3400152921676636 Accuracy : 0.875\n",
      "Iteration 307 of epoch 1 complete. Loss : 0.10022465884685516 Accuracy : 1.0\n",
      "Iteration 308 of epoch 1 complete. Loss : 0.14937500655651093 Accuracy : 0.96875\n",
      "Iteration 309 of epoch 1 complete. Loss : 0.36129212379455566 Accuracy : 0.875\n",
      "Iteration 310 of epoch 1 complete. Loss : 0.39890870451927185 Accuracy : 0.84375\n",
      "Iteration 311 of epoch 1 complete. Loss : 0.22787874937057495 Accuracy : 0.9375\n",
      "Iteration 312 of epoch 1 complete. Loss : 0.2436230331659317 Accuracy : 0.9375\n",
      "Iteration 313 of epoch 1 complete. Loss : 0.22974029183387756 Accuracy : 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 314 of epoch 1 complete. Loss : 0.5352298617362976 Accuracy : 0.78125\n",
      "Iteration 315 of epoch 1 complete. Loss : 0.24874338507652283 Accuracy : 0.9375\n",
      "Iteration 316 of epoch 1 complete. Loss : 0.22733451426029205 Accuracy : 0.9375\n",
      "Iteration 317 of epoch 1 complete. Loss : 0.3787602484226227 Accuracy : 0.875\n",
      "Iteration 318 of epoch 1 complete. Loss : 0.10276566445827484 Accuracy : 1.0\n",
      "Iteration 319 of epoch 1 complete. Loss : 0.15545925498008728 Accuracy : 0.96875\n",
      "Iteration 320 of epoch 1 complete. Loss : 0.23911389708518982 Accuracy : 0.9375\n",
      "Iteration 321 of epoch 1 complete. Loss : 0.296213299036026 Accuracy : 0.90625\n",
      "Iteration 322 of epoch 1 complete. Loss : 0.24457409977912903 Accuracy : 0.9375\n",
      "Iteration 323 of epoch 1 complete. Loss : 0.3091733157634735 Accuracy : 0.875\n",
      "Iteration 324 of epoch 1 complete. Loss : 0.24511009454727173 Accuracy : 0.90625\n",
      "Iteration 325 of epoch 1 complete. Loss : 0.35519343614578247 Accuracy : 0.875\n",
      "Iteration 326 of epoch 1 complete. Loss : 0.24699829518795013 Accuracy : 0.9375\n",
      "Iteration 327 of epoch 1 complete. Loss : 0.17006386816501617 Accuracy : 0.96875\n",
      "Iteration 328 of epoch 1 complete. Loss : 0.20413866639137268 Accuracy : 0.9375\n",
      "Iteration 329 of epoch 1 complete. Loss : 0.24590466916561127 Accuracy : 0.9375\n",
      "Iteration 330 of epoch 1 complete. Loss : 0.37443339824676514 Accuracy : 0.875\n",
      "Iteration 331 of epoch 1 complete. Loss : 0.3566030263900757 Accuracy : 0.875\n",
      "Iteration 332 of epoch 1 complete. Loss : 0.4283124506473541 Accuracy : 0.84375\n",
      "Iteration 334 of epoch 1 complete. Loss : 0.3427239656448364 Accuracy : 0.875\n",
      "Iteration 335 of epoch 1 complete. Loss : 0.27034127712249756 Accuracy : 0.90625\n",
      "Iteration 336 of epoch 1 complete. Loss : 0.29398322105407715 Accuracy : 0.90625\n",
      "Iteration 337 of epoch 1 complete. Loss : 0.24457919597625732 Accuracy : 0.9375\n",
      "Iteration 338 of epoch 1 complete. Loss : 0.22858431935310364 Accuracy : 0.9375\n",
      "Iteration 339 of epoch 1 complete. Loss : 0.45152613520622253 Accuracy : 0.84375\n",
      "Iteration 340 of epoch 1 complete. Loss : 0.2737397253513336 Accuracy : 0.90625\n",
      "Iteration 341 of epoch 1 complete. Loss : 0.34562504291534424 Accuracy : 0.875\n",
      "Iteration 342 of epoch 1 complete. Loss : 0.36165305972099304 Accuracy : 0.875\n",
      "Iteration 343 of epoch 1 complete. Loss : 0.29896876215934753 Accuracy : 0.90625\n",
      "Iteration 344 of epoch 1 complete. Loss : 0.23388206958770752 Accuracy : 0.9375\n",
      "Iteration 345 of epoch 1 complete. Loss : 0.43811601400375366 Accuracy : 0.84375\n",
      "Iteration 346 of epoch 1 complete. Loss : 0.4286652207374573 Accuracy : 0.84375\n",
      "Iteration 347 of epoch 1 complete. Loss : 0.16750410199165344 Accuracy : 0.96875\n",
      "Iteration 348 of epoch 1 complete. Loss : 0.36304277181625366 Accuracy : 0.875\n",
      "Iteration 349 of epoch 1 complete. Loss : 0.28119346499443054 Accuracy : 0.90625\n",
      "Iteration 350 of epoch 1 complete. Loss : 0.4722934067249298 Accuracy : 0.8125\n",
      "Iteration 351 of epoch 1 complete. Loss : 0.21727436780929565 Accuracy : 0.9375\n",
      "Iteration 352 of epoch 1 complete. Loss : 0.2323227971792221 Accuracy : 0.9375\n",
      "Iteration 353 of epoch 1 complete. Loss : 0.20384059846401215 Accuracy : 0.9375\n",
      "Iteration 354 of epoch 1 complete. Loss : 0.2863304615020752 Accuracy : 0.90625\n",
      "Iteration 355 of epoch 1 complete. Loss : 0.10055112093687057 Accuracy : 1.0\n",
      "Iteration 356 of epoch 1 complete. Loss : 0.33025768399238586 Accuracy : 0.90625\n",
      "Iteration 357 of epoch 1 complete. Loss : 0.2658735513687134 Accuracy : 0.90625\n",
      "Iteration 358 of epoch 1 complete. Loss : 0.22673895955085754 Accuracy : 0.9375\n",
      "Iteration 359 of epoch 1 complete. Loss : 0.20728549361228943 Accuracy : 0.9375\n",
      "Iteration 360 of epoch 1 complete. Loss : 0.2727257013320923 Accuracy : 0.90625\n",
      "Iteration 361 of epoch 1 complete. Loss : 0.3057231605052948 Accuracy : 0.90625\n",
      "Iteration 362 of epoch 1 complete. Loss : 0.23329545557498932 Accuracy : 0.9375\n",
      "Iteration 363 of epoch 1 complete. Loss : 0.2749030590057373 Accuracy : 0.90625\n",
      "Iteration 364 of epoch 1 complete. Loss : 0.2875906527042389 Accuracy : 0.90625\n",
      "Iteration 365 of epoch 1 complete. Loss : 0.23493041098117828 Accuracy : 0.9375\n",
      "Iteration 366 of epoch 1 complete. Loss : 0.26128196716308594 Accuracy : 0.90625\n",
      "Iteration 367 of epoch 1 complete. Loss : 0.2159484475851059 Accuracy : 0.9375\n",
      "Iteration 368 of epoch 1 complete. Loss : 0.2474099099636078 Accuracy : 0.9375\n",
      "Iteration 369 of epoch 1 complete. Loss : 0.2999408543109894 Accuracy : 0.90625\n",
      "Iteration 370 of epoch 1 complete. Loss : 0.4871295690536499 Accuracy : 0.8125\n",
      "Iteration 371 of epoch 1 complete. Loss : 0.24122266471385956 Accuracy : 0.9375\n",
      "Iteration 372 of epoch 1 complete. Loss : 0.30374443531036377 Accuracy : 0.90625\n",
      "Iteration 373 of epoch 1 complete. Loss : 0.17380203306674957 Accuracy : 0.96875\n",
      "Iteration 374 of epoch 1 complete. Loss : 0.2828989326953888 Accuracy : 0.90625\n",
      "Iteration 375 of epoch 1 complete. Loss : 0.3631468415260315 Accuracy : 0.875\n",
      "Iteration 376 of epoch 1 complete. Loss : 0.42380353808403015 Accuracy : 0.84375\n",
      "Iteration 377 of epoch 1 complete. Loss : 0.24560639262199402 Accuracy : 0.9375\n",
      "Iteration 378 of epoch 1 complete. Loss : 0.34996068477630615 Accuracy : 0.875\n",
      "Iteration 379 of epoch 1 complete. Loss : 0.21640868484973907 Accuracy : 0.9375\n",
      "Iteration 380 of epoch 1 complete. Loss : 0.29703137278556824 Accuracy : 0.90625\n",
      "Iteration 381 of epoch 1 complete. Loss : 0.14968779683113098 Accuracy : 0.96875\n",
      "Iteration 382 of epoch 1 complete. Loss : 0.2148422747850418 Accuracy : 0.9375\n",
      "Iteration 383 of epoch 1 complete. Loss : 0.09178092330694199 Accuracy : 1.0\n",
      "Iteration 384 of epoch 1 complete. Loss : 0.23608805239200592 Accuracy : 0.9375\n",
      "Iteration 385 of epoch 1 complete. Loss : 0.1484782099723816 Accuracy : 0.96875\n",
      "Iteration 386 of epoch 1 complete. Loss : 0.2223210632801056 Accuracy : 0.9375\n",
      "Iteration 387 of epoch 1 complete. Loss : 0.5871555209159851 Accuracy : 0.78125\n",
      "Iteration 388 of epoch 1 complete. Loss : 0.44656720757484436 Accuracy : 0.84375\n",
      "Iteration 389 of epoch 1 complete. Loss : 0.15597277879714966 Accuracy : 0.96875\n",
      "Iteration 390 of epoch 1 complete. Loss : 0.494181364774704 Accuracy : 0.8125\n",
      "Iteration 391 of epoch 1 complete. Loss : 0.35134077072143555 Accuracy : 0.875\n",
      "Iteration 392 of epoch 1 complete. Loss : 0.31180065870285034 Accuracy : 0.90625\n",
      "Iteration 393 of epoch 1 complete. Loss : 0.3057941794395447 Accuracy : 0.90625\n",
      "Iteration 394 of epoch 1 complete. Loss : 0.23699387907981873 Accuracy : 0.9375\n",
      "Iteration 395 of epoch 1 complete. Loss : 0.3108278214931488 Accuracy : 0.90625\n",
      "Iteration 396 of epoch 1 complete. Loss : 0.2762778699398041 Accuracy : 0.90625\n",
      "Iteration 397 of epoch 1 complete. Loss : 0.43960464000701904 Accuracy : 0.84375\n",
      "Iteration 398 of epoch 1 complete. Loss : 0.23714688420295715 Accuracy : 0.9375\n",
      "Iteration 399 of epoch 1 complete. Loss : 0.3484765589237213 Accuracy : 0.875\n",
      "Iteration 400 of epoch 1 complete. Loss : 0.35122549533843994 Accuracy : 0.875\n",
      "Iteration 401 of epoch 1 complete. Loss : 0.2435588836669922 Accuracy : 0.9375\n",
      "Iteration 402 of epoch 1 complete. Loss : 0.43244990706443787 Accuracy : 0.84375\n",
      "Iteration 403 of epoch 1 complete. Loss : 0.28052186965942383 Accuracy : 0.90625\n",
      "Iteration 404 of epoch 1 complete. Loss : 0.1547621339559555 Accuracy : 0.96875\n",
      "Iteration 405 of epoch 1 complete. Loss : 0.29456663131713867 Accuracy : 0.90625\n",
      "Iteration 406 of epoch 1 complete. Loss : 0.34498897194862366 Accuracy : 0.875\n",
      "Iteration 407 of epoch 1 complete. Loss : 0.24433507025241852 Accuracy : 0.9375\n",
      "Iteration 408 of epoch 1 complete. Loss : 0.36984962224960327 Accuracy : 0.875\n",
      "Iteration 409 of epoch 1 complete. Loss : 0.21771857142448425 Accuracy : 0.9375\n",
      "Iteration 410 of epoch 1 complete. Loss : 0.17424839735031128 Accuracy : 0.96875\n",
      "Iteration 411 of epoch 1 complete. Loss : 0.4131336510181427 Accuracy : 0.84375\n",
      "Iteration 412 of epoch 1 complete. Loss : 0.3185406029224396 Accuracy : 0.90625\n",
      "Iteration 413 of epoch 1 complete. Loss : 0.18458208441734314 Accuracy : 0.96875\n",
      "Iteration 414 of epoch 1 complete. Loss : 0.1633639931678772 Accuracy : 0.96875\n",
      "Iteration 415 of epoch 1 complete. Loss : 0.23186004161834717 Accuracy : 0.9375\n",
      "Iteration 416 of epoch 1 complete. Loss : 0.10414542257785797 Accuracy : 1.0\n",
      "Iteration 417 of epoch 1 complete. Loss : 0.23956038057804108 Accuracy : 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 418 of epoch 1 complete. Loss : 0.2360352873802185 Accuracy : 0.9375\n",
      "Iteration 419 of epoch 1 complete. Loss : 0.42025160789489746 Accuracy : 0.84375\n",
      "Iteration 420 of epoch 1 complete. Loss : 0.3323921263217926 Accuracy : 0.875\n",
      "Iteration 421 of epoch 1 complete. Loss : 0.2841224670410156 Accuracy : 0.90625\n",
      "Iteration 422 of epoch 1 complete. Loss : 0.3339076042175293 Accuracy : 0.875\n",
      "Iteration 423 of epoch 1 complete. Loss : 0.2074977308511734 Accuracy : 0.9375\n",
      "Iteration 424 of epoch 1 complete. Loss : 0.2152089774608612 Accuracy : 0.9375\n",
      "Iteration 425 of epoch 1 complete. Loss : 0.09013529121875763 Accuracy : 1.0\n",
      "Iteration 426 of epoch 1 complete. Loss : 0.24770162999629974 Accuracy : 0.9375\n",
      "Iteration 427 of epoch 1 complete. Loss : 0.29565683007240295 Accuracy : 0.90625\n",
      "Iteration 428 of epoch 1 complete. Loss : 0.41615164279937744 Accuracy : 0.84375\n",
      "Iteration 429 of epoch 1 complete. Loss : 0.2902584671974182 Accuracy : 0.90625\n",
      "Iteration 430 of epoch 1 complete. Loss : 0.5070613026618958 Accuracy : 0.8125\n",
      "Iteration 431 of epoch 1 complete. Loss : 0.2826682925224304 Accuracy : 0.90625\n",
      "Iteration 432 of epoch 1 complete. Loss : 0.1400565803050995 Accuracy : 0.96875\n",
      "Iteration 433 of epoch 1 complete. Loss : 0.3313712477684021 Accuracy : 0.875\n",
      "Iteration 434 of epoch 1 complete. Loss : 0.15058915317058563 Accuracy : 0.96875\n",
      "Iteration 435 of epoch 1 complete. Loss : 0.3137359321117401 Accuracy : 0.90625\n",
      "Iteration 436 of epoch 1 complete. Loss : 0.21780304610729218 Accuracy : 0.9375\n",
      "Iteration 437 of epoch 1 complete. Loss : 0.35220569372177124 Accuracy : 0.875\n",
      "Iteration 438 of epoch 1 complete. Loss : 0.1067332848906517 Accuracy : 1.0\n",
      "Iteration 439 of epoch 1 complete. Loss : 0.16362401843070984 Accuracy : 0.96875\n",
      "Iteration 440 of epoch 1 complete. Loss : 0.28343552350997925 Accuracy : 0.90625\n",
      "Iteration 441 of epoch 1 complete. Loss : 0.40220877528190613 Accuracy : 0.875\n",
      "Iteration 442 of epoch 1 complete. Loss : 0.3711584210395813 Accuracy : 0.875\n",
      "Iteration 443 of epoch 1 complete. Loss : 0.3612740933895111 Accuracy : 0.875\n",
      "Iteration 444 of epoch 1 complete. Loss : 0.3605683445930481 Accuracy : 0.875\n",
      "Iteration 445 of epoch 1 complete. Loss : 0.35547223687171936 Accuracy : 0.875\n",
      "Iteration 446 of epoch 1 complete. Loss : 0.21379101276397705 Accuracy : 0.9375\n",
      "Iteration 447 of epoch 1 complete. Loss : 0.2967454791069031 Accuracy : 0.90625\n",
      "Iteration 448 of epoch 1 complete. Loss : 0.2720007598400116 Accuracy : 0.90625\n",
      "Iteration 449 of epoch 1 complete. Loss : 0.2127089947462082 Accuracy : 0.9375\n",
      "Iteration 450 of epoch 1 complete. Loss : 0.192009836435318 Accuracy : 0.9375\n",
      "Iteration 451 of epoch 1 complete. Loss : 0.21838617324829102 Accuracy : 0.9375\n",
      "Iteration 452 of epoch 1 complete. Loss : 0.46818339824676514 Accuracy : 0.84375\n",
      "Iteration 453 of epoch 1 complete. Loss : 0.1669873595237732 Accuracy : 0.96875\n",
      "Iteration 454 of epoch 1 complete. Loss : 0.2517147362232208 Accuracy : 0.9375\n",
      "Iteration 455 of epoch 1 complete. Loss : 0.28301846981048584 Accuracy : 0.90625\n",
      "Iteration 456 of epoch 1 complete. Loss : 0.5103600025177002 Accuracy : 0.8125\n",
      "Iteration 457 of epoch 1 complete. Loss : 0.24456682801246643 Accuracy : 0.90625\n",
      "Iteration 458 of epoch 1 complete. Loss : 0.2338639795780182 Accuracy : 0.9375\n",
      "Iteration 459 of epoch 1 complete. Loss : 0.24120689928531647 Accuracy : 0.9375\n",
      "Iteration 460 of epoch 1 complete. Loss : 0.3184679448604584 Accuracy : 0.90625\n",
      "Iteration 461 of epoch 1 complete. Loss : 0.23354724049568176 Accuracy : 0.9375\n",
      "Iteration 462 of epoch 1 complete. Loss : 0.2746361196041107 Accuracy : 0.90625\n",
      "Iteration 463 of epoch 1 complete. Loss : 0.15376386046409607 Accuracy : 0.96875\n",
      "Iteration 464 of epoch 1 complete. Loss : 0.34277409315109253 Accuracy : 0.875\n",
      "Iteration 465 of epoch 1 complete. Loss : 0.4204387962818146 Accuracy : 0.84375\n",
      "Iteration 466 of epoch 1 complete. Loss : 0.13801249861717224 Accuracy : 0.96875\n",
      "Iteration 467 of epoch 1 complete. Loss : 0.37398195266723633 Accuracy : 0.875\n",
      "Iteration 468 of epoch 1 complete. Loss : 0.3368282616138458 Accuracy : 0.875\n",
      "Iteration 469 of epoch 1 complete. Loss : 0.21085181832313538 Accuracy : 0.9375\n",
      "Iteration 470 of epoch 1 complete. Loss : 0.3243193030357361 Accuracy : 0.875\n",
      "Iteration 471 of epoch 1 complete. Loss : 0.1563268005847931 Accuracy : 0.96875\n",
      "Iteration 472 of epoch 1 complete. Loss : 0.27892348170280457 Accuracy : 0.90625\n",
      "Iteration 473 of epoch 1 complete. Loss : 0.3012768030166626 Accuracy : 0.90625\n",
      "Iteration 474 of epoch 1 complete. Loss : 0.1486225426197052 Accuracy : 0.96875\n",
      "Iteration 475 of epoch 1 complete. Loss : 0.19472455978393555 Accuracy : 0.9375\n",
      "Iteration 476 of epoch 1 complete. Loss : 0.2547014057636261 Accuracy : 0.90625\n",
      "Iteration 477 of epoch 1 complete. Loss : 0.2691156268119812 Accuracy : 0.90625\n",
      "Iteration 478 of epoch 1 complete. Loss : 0.2138116955757141 Accuracy : 0.9375\n",
      "Iteration 479 of epoch 1 complete. Loss : 0.22386625409126282 Accuracy : 0.9375\n",
      "Iteration 480 of epoch 1 complete. Loss : 0.3136555552482605 Accuracy : 0.90625\n",
      "Iteration 481 of epoch 1 complete. Loss : 0.23252291977405548 Accuracy : 0.9375\n",
      "Iteration 482 of epoch 1 complete. Loss : 0.35349124670028687 Accuracy : 0.875\n",
      "Iteration 483 of epoch 1 complete. Loss : 0.29332223534584045 Accuracy : 0.90625\n",
      "Iteration 484 of epoch 1 complete. Loss : 0.336743026971817 Accuracy : 0.875\n",
      "Iteration 485 of epoch 1 complete. Loss : 0.3370363414287567 Accuracy : 0.875\n",
      "Iteration 486 of epoch 1 complete. Loss : 0.19850189983844757 Accuracy : 0.9375\n",
      "Iteration 487 of epoch 1 complete. Loss : 0.3507644534111023 Accuracy : 0.875\n",
      "Iteration 488 of epoch 1 complete. Loss : 0.3606495261192322 Accuracy : 0.875\n",
      "Iteration 489 of epoch 1 complete. Loss : 0.23463000357151031 Accuracy : 0.9375\n",
      "Iteration 490 of epoch 1 complete. Loss : 0.290086030960083 Accuracy : 0.90625\n",
      "Iteration 491 of epoch 1 complete. Loss : 0.4367032051086426 Accuracy : 0.84375\n",
      "Iteration 492 of epoch 1 complete. Loss : 0.2740451693534851 Accuracy : 0.90625\n",
      "Iteration 493 of epoch 1 complete. Loss : 0.36764299869537354 Accuracy : 0.875\n",
      "Iteration 494 of epoch 1 complete. Loss : 0.37933656573295593 Accuracy : 0.875\n",
      "Iteration 495 of epoch 1 complete. Loss : 0.40315601229667664 Accuracy : 0.84375\n",
      "Iteration 496 of epoch 1 complete. Loss : 0.24551944434642792 Accuracy : 0.9375\n",
      "Iteration 497 of epoch 1 complete. Loss : 0.2654251754283905 Accuracy : 0.90625\n",
      "Iteration 498 of epoch 1 complete. Loss : 0.2384769320487976 Accuracy : 0.9375\n",
      "Iteration 499 of epoch 1 complete. Loss : 0.43152305483818054 Accuracy : 0.84375\n",
      "Iteration 500 of epoch 1 complete. Loss : 0.26920682191848755 Accuracy : 0.90625\n",
      "Iteration 501 of epoch 1 complete. Loss : 0.34771159291267395 Accuracy : 0.875\n",
      "Iteration 502 of epoch 1 complete. Loss : 0.20872986316680908 Accuracy : 0.9375\n",
      "Iteration 503 of epoch 1 complete. Loss : 0.2791595458984375 Accuracy : 0.90625\n",
      "Iteration 504 of epoch 1 complete. Loss : 0.17679457366466522 Accuracy : 0.9375\n",
      "Iteration 505 of epoch 1 complete. Loss : 0.2636895477771759 Accuracy : 0.90625\n",
      "Iteration 506 of epoch 1 complete. Loss : 0.2751530110836029 Accuracy : 0.90625\n",
      "Iteration 507 of epoch 1 complete. Loss : 0.2701919376850128 Accuracy : 0.90625\n",
      "Iteration 508 of epoch 1 complete. Loss : 0.36147117614746094 Accuracy : 0.84375\n",
      "Iteration 509 of epoch 1 complete. Loss : 0.21364517509937286 Accuracy : 0.9375\n",
      "Iteration 510 of epoch 1 complete. Loss : 0.38754695653915405 Accuracy : 0.84375\n",
      "Iteration 511 of epoch 1 complete. Loss : 0.18085519969463348 Accuracy : 0.96875\n",
      "Iteration 512 of epoch 1 complete. Loss : 0.32624128460884094 Accuracy : 0.875\n",
      "Iteration 513 of epoch 1 complete. Loss : 0.3573511838912964 Accuracy : 0.875\n",
      "Iteration 514 of epoch 1 complete. Loss : 0.36948472261428833 Accuracy : 0.875\n",
      "Iteration 515 of epoch 1 complete. Loss : 0.4836515486240387 Accuracy : 0.8125\n",
      "Iteration 516 of epoch 1 complete. Loss : 0.16006946563720703 Accuracy : 0.96875\n",
      "Iteration 517 of epoch 1 complete. Loss : 0.35904693603515625 Accuracy : 0.875\n",
      "Iteration 518 of epoch 1 complete. Loss : 0.21506717801094055 Accuracy : 0.9375\n",
      "Iteration 519 of epoch 1 complete. Loss : 0.3417803645133972 Accuracy : 0.875\n",
      "Iteration 520 of epoch 1 complete. Loss : 0.1678869128227234 Accuracy : 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 521 of epoch 1 complete. Loss : 0.14534655213356018 Accuracy : 0.96875\n",
      "Iteration 522 of epoch 1 complete. Loss : 0.23012186586856842 Accuracy : 0.9375\n",
      "Iteration 523 of epoch 1 complete. Loss : 0.2677767872810364 Accuracy : 0.90625\n",
      "Iteration 524 of epoch 1 complete. Loss : 0.40832266211509705 Accuracy : 0.875\n",
      "Iteration 525 of epoch 1 complete. Loss : 0.35578450560569763 Accuracy : 0.875\n",
      "Iteration 526 of epoch 1 complete. Loss : 0.26295405626296997 Accuracy : 0.90625\n",
      "Iteration 527 of epoch 1 complete. Loss : 0.16405972838401794 Accuracy : 0.96875\n",
      "Iteration 528 of epoch 1 complete. Loss : 0.5062030553817749 Accuracy : 0.78125\n",
      "Iteration 529 of epoch 1 complete. Loss : 0.20678922533988953 Accuracy : 0.9375\n",
      "Iteration 530 of epoch 1 complete. Loss : 0.2982042133808136 Accuracy : 0.90625\n",
      "Iteration 531 of epoch 1 complete. Loss : 0.2509151101112366 Accuracy : 0.90625\n",
      "Iteration 532 of epoch 1 complete. Loss : 0.28772279620170593 Accuracy : 0.90625\n",
      "Iteration 533 of epoch 1 complete. Loss : 0.2477331906557083 Accuracy : 0.9375\n",
      "Iteration 534 of epoch 1 complete. Loss : 0.25975802540779114 Accuracy : 0.90625\n",
      "Iteration 535 of epoch 1 complete. Loss : 0.1805511713027954 Accuracy : 0.96875\n",
      "Iteration 536 of epoch 1 complete. Loss : 0.41341522336006165 Accuracy : 0.84375\n",
      "Iteration 537 of epoch 1 complete. Loss : 0.5029399394989014 Accuracy : 0.8125\n",
      "Iteration 538 of epoch 1 complete. Loss : 0.4415011405944824 Accuracy : 0.8125\n",
      "Iteration 539 of epoch 1 complete. Loss : 0.28913819789886475 Accuracy : 0.90625\n",
      "Iteration 540 of epoch 1 complete. Loss : 0.4051663875579834 Accuracy : 0.84375\n",
      "Iteration 541 of epoch 1 complete. Loss : 0.32501327991485596 Accuracy : 0.875\n",
      "Iteration 542 of epoch 1 complete. Loss : 0.2756969928741455 Accuracy : 0.90625\n",
      "Iteration 543 of epoch 1 complete. Loss : 0.3869500160217285 Accuracy : 0.84375\n",
      "Iteration 544 of epoch 1 complete. Loss : 0.1912209540605545 Accuracy : 0.9375\n",
      "Iteration 545 of epoch 1 complete. Loss : 0.3874991238117218 Accuracy : 0.84375\n",
      "Iteration 546 of epoch 1 complete. Loss : 0.3536114990711212 Accuracy : 0.875\n",
      "Iteration 547 of epoch 1 complete. Loss : 0.26802709698677063 Accuracy : 0.90625\n",
      "Iteration 548 of epoch 1 complete. Loss : 0.44147491455078125 Accuracy : 0.8125\n",
      "Iteration 549 of epoch 1 complete. Loss : 0.2587715685367584 Accuracy : 0.90625\n",
      "Iteration 550 of epoch 1 complete. Loss : 0.1018906682729721 Accuracy : 1.0\n",
      "Iteration 551 of epoch 1 complete. Loss : 0.23420077562332153 Accuracy : 0.9375\n"
     ]
    }
   ],
   "source": [
    "train(net, criterion, opti, train_dataloader, test_dataloader, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models/model_state_dict_512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanisha17116/.local/lib/python3.6/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type ContextEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net, \"models/model_512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContextEmbeddings(freeze_bert=  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"models/model_state_dict_512\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextEmbeddings(\n",
       "  (bert_layer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (embedding_layer): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (cls_layer): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n"
     ]
    }
   ],
   "source": [
    "output_train = []\n",
    "for it, (seq, attn_masks, labels) in enumerate(Train_dataloader):\n",
    "    bert_out, _ = model.bert_layer.forward(seq, attn_masks)\n",
    "    embeds = model.embedding_layer.forward(bert_out[:,0])\n",
    "    output_train.append(embeds)\n",
    "    print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(output_train, open(\"models/output_train_512.pkl\",\"wb\"))\n",
    "print(\"Output_train_pickled...YAYYYYY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = []\n",
    "for it, (seq, attn_masks, labels) in enumerate(Test_dataloader) :\n",
    "    bert_out, _ = model.bert_layer.forward(seq, attn_masks)\n",
    "    embeds = model.embedding_layer.forward(bert_out[:,0])\n",
    "    output_test.append(embeds)\n",
    "    print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(output_test, open(\"models/output_test_512.pkl\",\"wb\"))\n",
    "print(\"Output_test_pickled...YAYYYYY\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
